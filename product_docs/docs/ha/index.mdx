---
title: Failover Manager HA use cases
description: "With the capabilities of Failover Manager, EDB has designed four basic
architectures to run a High availability environment"
indexCards: simple
hideVersion: true
---

EDB Failover Manager provides various high availability options for EDB
Postgres Advanced Server using the Postgres connection poolers and
connection libraries. This documentation discusses the implications of a
high-availability architecture for these options.

Failover Manager is a high-availability module that monitors the health
of a Postgres streaming replication cluster and verifies failures
quickly. When a database failure occurs, Failover Manager can
automatically promote a streaming replication Standby node into a
writable Primary node to ensure continued performance and protect
against data loss with minimal service interruption.

A Failover Manager cluster is comprised of Failover Manager processes
that reside on the following hosts on a network:

-   A Primary node is the Primary database server that is servicing
    database clients.

-   One or more Standby nodes are streaming replication servers
    associated with the Primary node.

-   The Witness node confirms assertions of either the Primary or a
    Standby in a failover scenario. If during a failure situation, the
    Primary finds itself in a partition with half or more of the nodes,
    it will stay Primary. As such, Failover Manager supports running in
    a cluster with an even number of agents.

    For ensuring the high availability of your database, core features of Failover Manager can be combined with the Postgres connection libraries (client connect failover) and/or connection poolers.

With the capabilities of Failover Manager, EDB has designed four basic
architectures to run a High availability environment:

1. [Failover Manager using VIP (virtual IP)](03_efm_vip): Failover Manager has a key
capability to manage VIP addresses out of the box. VIP addresses
allow applications to connect to a single IP address, which is being
routed to the Primary database server. This architecture is the most
basic solution to run when VIP addresses are available in your
environment.

2. [Failover Manager using client connect failover](04_efm_client_connect_failover):
PostgreSQL client libraries like libpq and jdbc allow for Client
Connection Failover. With Client Connection failover, the connection
string will contain multiple servers (host=srv1,srv2), and the
client library will loop over the available hosts to find a
connection that is available and capable of Read/Write operations.
This capability allows clients to follow the master during a
switchover. This solution does not rely on Virtual IP addresses and
can be used in every environment where such client configurations
can be set.

3. [Failover Manager with PgBouncer](05_efm_pgbouncer): PgBouncer adds a lot of capabilities such as connection pooling and the option to halt traffic. It can additionally be used as a proxy between the Client and the Postgres Database Server. By leveraging the integration options in Failover Manager to run reconfiguration of PgBouncer during a
Failover, PgBouncer can be used to route the traffic to the correct
primary database server.

4. [Failover Manager with Pgpool](06_efm_pgpool): Pgpool-II is
another tool that is used as a proxy between the Client and the
Postgres Database Server. Pgpool-II adds a lot of capabilities such
as running in cluster mode with a Watchdog, managing VIP's, and
Read-Only scalability. Failover Manager has native capabilities to
integrate with Pgpool-II to redirect traffic to another primary
during Database failover overations.

The following is a list of features that are supported in each of the
architectures:

**Client Connect Failover**

|Features | EFM with VIP | EFM with client connect failover | EFM with with PgBouncer | EFM with with Pgpool |
|---------------------------------------- | ----------------------- | ----------------------------------------- | ----------------------- | ---------------------|
|Connection pooling | | | Yes | Yes |
|Runs on cloud (no VIP) | | Yes | Yes | Yes |
|Halt traffic option | | | Yes | |
|Read-only scalability | | Yes (using multiple connection factories) | | Yes |
|Clustered proxy | | | | Yes |
|Proxy integration | | | ssh | PCP |
|Minimum servers required | 3 | 3 | 5 | 6 |
|Complexity | Low | Low | Medium | High |
|Network hops | 1 | 1 | | |
|Failover duration | Low | Medium | Low | Low |