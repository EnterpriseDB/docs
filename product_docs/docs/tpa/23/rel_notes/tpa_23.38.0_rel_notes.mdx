---
title: Trusted Postgres Architect 23.38.0 release notes
navTitle: Version 23.38.0
originalFilePath: product_docs/docs/tpa/23/rel_notes/src/tpa_23.38.0_rel_notes.yml
editTarget: originalFilePath
---

Released: 9 June 2025

New features, enhancements, bug fixes, and other changes in Trusted Postgres Architect 23.38.0 include the following:

## Highlights

- Support for deploying EDB Postgres Distributed version 6 in both Expanded and Essential architectures
- Improvements to PgBouncer implementation including a mitigation for CVE-2025-2291
- Support for PEM 10.1

## Enhancements

<table class="table w-100"><thead><tr><th>Description</th><th width="10%">Addresses</th></tr></thead><tbody>
<tr><td><details><summary>Support for PGD6 architectures.</summary><hr/><p>TPA can now configure and deploy clusters using the PGD-X and PGD-S architectures
based on PGD6 The PGD-S architecture implements PGD Essential and the PGD-X architecture
implements PGD Expanded. These architectures have sensible default configurations
and also accept various configure options to customize their behavior. PGD 6
deployments no longer include pgd-proxy; instead, PGD's built-in Connection Manager
is configured. Testing support for the new architectures is added.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Configure PEM to monitor Barman when both are present in a cluster.</summary><hr/><p>When a cluster is configured with PEM enabled (using the <code>--enable-pem</code>
option) and includes a Barman node, the following actions are now performed automatically:</p>
<ul>
<li><code>enable_pg_backup_api</code> is set to <code>true</code> in <code>config.yml</code></li>
<li>The <code>pem-agent</code> role is assigned to the Barman node</li>
<li>The Barman endpoint is registered with the local PEM agent.</li>
</ul>
<p>These changes simplify setup and ensure seamless integration between
PEM and Barman.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Added support for Rocky Linux 9 on AWS.</summary><hr/><p>TPA now supports configuring a cluster using Rocky Linux 9.5 on the <code>aws</code> platform. This is now the default version for Rocky Linux on AWS if a version is not specified.</p></details></td><td></td></tr>
<tr><td><details><summary>Added support for pg_backup_api on SLES 15.</summary><hr/><p>TPA will now configure pg_backup_api is on SUSE Linux Enterprise
Server 15 (SLES 15) when PEM monitoring is enabled and a Barman node is present
in the cluster.</p>
</details></td><td></td></tr>
</tbody></table>


## Changes

<table class="table w-100"><thead><tr><th>Description</th><th width="10%">Addresses</th></tr></thead><tbody>
<tr><td><details><summary>Treat <code>PEM_DB_PORT</code> as a string for PEM 10.1 and above.</summary><hr/><p>PEM 10.1 adds support for multi-host connection strings from the web application
to the backend servers. To support this change, the <code>PEM_DB_PORT</code> parameter in PEM's
<code>config_setup.py</code> file is now a string rather than an integer. While TPA does
not yet support deploying HA PEM configurations, TPA will now correctly set this
parameter as a string when the PEM version is 10.1 or greater.</p>
</details></td><td></td></tr>
<tr><td><details><summary>TPA will now skip repository checks when <code>repo</code> is excluded from tasks.</summary><hr/><p>The <code>repo</code> tag is available for exclusion, but previously would only skip
tasks under the <code>sys/repositories</code> role. Now it also skips over the initialization
tasks which check which repositories to use and the verifies the credentials to
access them are provided.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Improved the behavior of <code>postgres_package_version</code>.</summary><hr/><p>Setting <code>postgres_package_version</code> will now cause TPA to install the selected version of various postgres-related components on Debian or Ubuntu systems installing EDB Postgres Advanced Server or EDB Postgres Extended Server. This avoids dependency resolution problems when newer package versions are visible in repositories.</p></details></td><td></td></tr>
<tr><td><details><summary>Added a new task selectors <code>create_postgres_system_user</code> and <code>create_pgd_proxy_system_user</code>.</summary><hr/><p>Added new task selectors that allow to skip the postgres_user and pgd_proxy_user
operating system user. This allows clusters to use remote users created by a centralized
user management such as NIS. This can be set in config.yml: <code>cluster_vars: excluded_tasks: - create_postgres_system_user - create_pgd_proxy_system_user</code></p>
</details></td><td>48601,44388</td></tr>
<tr><td><details><summary>TPA will now redirect PgBouncer to the new primary in M1 repmgr clusters during switchover.</summary><hr/><p>TPA will now ensure that PgBouncer instances are redirected to the new primary node
after using the <code>switchover</code> command in a repmgr + PgBouncer cluster that has <code>repmgr_redirect_pgbouncer</code>
set to true. The <code>tpaexec switchover</code> command will now ensure that PgBouncer instance
connect to the new primary node. A new <code>revert_redirect</code> variable can also be
set as extra-variable after a first switchover is done to revert back to the initial
primary node.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Made <code>key_id</code>/<code>gpgkey</code> optional in custom repository definitions.</summary><hr/><p>The <code>key_id</code> and <code>gpgkey</code> parameters (for apt and yum custom repositories definition) are not required by the underlying modules, there are use cases where this is not easy to provide ahead of installation. With this change, TPA does not make it mandatory to provide those in custom repository definitions.</p></details></td><td></td></tr>
<tr><td><details><summary>Fixed <code>verify-settings</code> check in <code>tpaexec test</code> for PGD CLI 5.7.0+.</summary><hr/><p>The output for the PGD CLI command <code>pgd verify-settings</code> changed in PGD 5.7.0.
TPA now correctly parses the output when using version above 5.7.0 of PGD CLI.
Note, verify-settings will be deprecated along with
other commands in future PGD releases. those commands are now wrapper calling the
new commands until the deprecation occurs.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Improved logic for granting permissions to <code>barman_role</code>.</summary><hr/><p>TPA now uses the <code>postgresql_privs</code>
module to apply the grant on <code>barman_role</code>, so that changes are only applied when
needed. In addition, in PGD clusters, TPA will use the <code>bdr_database</code> on second deploys so the DDL
is replicated across the cluster by PGD.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Documented <code>cluster_vars</code> variable templating in config.yml</summary><hr/><p>Added documentation to explain correct templating procedure for variables defined
under <code>cluster_vars</code> with a worked example in order to avoid confusion from unexpected
behavior associated with inventory variables not being defined when improperly
templated in <code>config.yml</code>.</p>
</details></td><td>48797</td></tr>
<tr><td><details><summary>TPA will now raise an ArchitectureError for if an invalid CIDR is passed to <code>--network</code> flag during configure.</summary><hr/><p>By default, the Python standard library <code>ipaddress</code> package enforces 'strict'
interpretation of the CIDR, whereby the IP used should be the network address
of the range. Previously, any IP passed to the <code>--network</code> flag that contained
host bits would dump a stacktrace due to the raised ValueError. That exception
is now caught and an ArchitectureError is raised to display a clear message to
the user about the <code>--network</code> parameter.</p>
</details></td><td></td></tr>
<tr><td><details><summary>Added a default value for EFM application.name property.</summary><hr/><p>If the EFM <code>application.name</code> property is not set for a node, TPA will use
the Postgres <code>cluster_name</code> property as a default. EFM uses this value when performing
a switchover or when building a new standby database.</p>
</details></td><td></td></tr>
<tr><td><details><summary>TPA now uses the EDB repository setup script on SUSE.</summary><hr/><p>Previously, TPA did not use the EDB repository setup script on SUSE because it did not work on repeat deploys. Zypper would raise because the repositories that the script attempts to install already exist, and require unique names. Now that the repository setup script task is skipped if the repositories are already installed, this issue is not encountered.</p></details></td><td></td></tr>
</tbody></table>


## Bug Fixes

<table class="table w-100"><thead><tr><th>Description</th><th width="10%">Addresses</th></tr></thead><tbody>
<tr><td><details><summary>TPA will now create the <code>pgbouncer_get_auth()</code> function in dedicated database.</summary><hr/><p>The <code>pgbouncer_get_auth()</code> function was created in the <code>pg_catalog</code> schema
and execute granted to the <code>pgbouncer_auth_user</code>. This function was created in
every database, but this was not necessary for PgBouncer. A failure may be encountered
during the <code>pgd node upgrade</code> process when this function was created in the <code>pg_catalog</code>
schema as it is not included in the dump created by <code>pg_dump</code>. A later task attempts
to run a <code>GRANT</code> on this function and fails, as the function is not restored since
it was not originally dumped. Now this function is only created in a single database,
named under the <code>pgbouncer_auth_database</code> variable in <code>config.yml</code>, which defaults
to <code>pgbouncer_auth_database</code> if not included. It is only created if at least one
instance with <code>pgbouncer</code> role is included in the cluster. A warning is also issued
during deploy and upgrade if any databases define this function under the <code>pg_catalog</code>
schema, as a future TPA release may remove the function from that schema. The
<code>pgbouncer_get_auth()</code> function itself used by PgBouncer <code>auth_query</code> has been
updated to address CVE-2025-2291. This vulnerability allowed for authentication
using expired passwords, potentially granting unauthorized access because the
auth_query mechanism did not consider the <code>VALID UNTIL</code> attribute set in PostgreSQL
for user passwords.</p>
</details></td><td>42911, 45068</td></tr>
<tr><td><details><summary>Fixed an issue whereby some tasks were incorrectly skipped when the <code>--check</code> option was used.</summary><hr/><p>In PGD clusters without HARP, the <code>Read current configuration file if exists</code> task needs to run in check
mode to ensure we have the information available to correctly skip the following
HARP check task. However, by default Ansible skips tasks using the the <code>shell</code> module during check mode, meaning this task did not run, resulting in a spurious failure on subsequent tasks.
We now let Ansible know that this task has to be run.</p>
</details></td><td></td></tr>
<tr><td>Fixed a bug whereby settings added to `ignore_slots` via `cluster_vars['patroni_conf_settings']['bootstrap']['dcs']` were not merged into the eventual config.</td><td></td></tr>
<tr><td><details><summary>Fixed an issue where RAFT checks for BDR nodes with replica role were not skipped during upgrade.</summary><hr/><p>Physical replication of a <code>subscriber-only</code> node can be achieved in a <code>PGD</code> cluster by installing <code>repmgr</code> as a failover-manager and designating the <code>subscriber-only</code> node as the <code>primary</code> and listing another BDR data node as the <code>backup</code>; this backup node is given the <code>replica</code> role. This configuration would result in the PGD upgrade process failing, since TPA expects BDR data nodes to have RAFT enabled, but the physical replica BDR data node (with both <code>replica</code> and <code>bdr</code> roles) by design does not. As a fix, certain BDR-specific tasks in the upgrade process now skip any node that has a <code>replica</code> role, allowing for a successful upgrade.</p></details></td><td>46186</td></tr>
</tbody></table>


