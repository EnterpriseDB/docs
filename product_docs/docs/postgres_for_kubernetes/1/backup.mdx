---
title: 'Backup'
originalFilePath: 'src/backup.md'
---



PostgreSQL natively provides first class backup and recovery capabilities based
on file system level (physical) copy. These have been successfully used for
more than 15 years in mission critical production databases, helping
organizations all over the world achieve their disaster recovery goals with
Postgres.

!!!note
There's another way to backup databases in PostgreSQL, through the
`pg_dump` utility - which relies on logical backups instead of physical ones.
However, logical backups are not suitable for business continuity use cases
and as such are not covered by {{name.ln}} (yet, at least).
If you want to use the `pg_dump` utility, let yourself be inspired by the
["Troubleshooting / Emergency backup" section](troubleshooting.md#emergency-backup).
!!!

In EDB Postgres for Kubernetes, the backup infrastructure for each PostgreSQL cluster is made
up of the following resources:

-   **WAL archive**: a location containing the WAL files (transactional logs)
    that are continuously written by Postgres and archived for data durability
-   **Physical base backups**: a copy of all the files that PostgreSQL uses to
    store the data in the database (primarily the `PGDATA` and any tablespace)

CNP-I provides a generic and extensible interface for managing WAL archiving
(both archive and restore operations), as well as the base backup and
corresponding restore processes.

On the other hand, EDB Postgres for Kubernetes supports two ways to store physical base backups:

-   on [object stores](backup_barmanobjectstore.md), as tarballs - optionally
    compressed
-   on [Kubernetes Volume Snapshots](backup_volumesnapshot.md), if supported by
    the underlying storage class

!!!info Important
Before choosing your backup strategy with {{name.ln}}, it is important that
you take some time to familiarize with some basic concepts, like WAL archive,
hot and cold backups.
!!!

!!!info Important
Please refer to the official Kubernetes documentation for a list of all
the supported [Container Storage Interface (CSI) drivers](https://kubernetes-csi.github.io/docs/drivers.html)
that provide snapshotting capabilities.
!!!

!!!info
Starting with version 1.25, {{name.ln}} includes experimental support for
backup and recovery using plugins, such as the
[Barman Cloud plugin](https://github.com/cloudnative-pg/plugin-barman-cloud).
!!!

## WAL archive

The WAL archive in PostgreSQL is at the heart of **continuous backup**, and it
is fundamental for the following reasons:

-   **Hot backups**: the possibility to take physical base backups from any
    instance in the Postgres cluster (either primary or standby) without shutting
    down the server; they are also known as online backups
-   **Point in Time recovery** (PITR): the possibility to recover at any point in
    time from the first available base backup in your system

!!!warning
WAL archive alone is useless. Without a physical base backup, you cannot
restore a PostgreSQL cluster.
!!!

In general, the presence of a WAL archive enhances the resilience of a
PostgreSQL cluster, allowing each instance to fetch any required WAL file from
the archive if needed (normally the WAL archive has higher retention periods
than any Postgres instance that normally recycles those files).

This use case can also be extended to [replica clusters](replica_cluster.md),
as they can simply rely on the WAL archive to synchronize across long
distances, extending disaster recovery goals across different regions.

When you [configure a WAL archive](wal_archiving.md), {{name.ln}} provides
out-of-the-box an [RPO](before_you_start.md#postgresql-terminology) ≤ 5 minutes for disaster
recovery, even across regions.

!!!info Important
Our recommendation is to always setup the WAL archive in production.
There are known use cases - normally involving staging and development
environments - where none of the above benefits are needed and the WAL
archive is not necessary. RPO in this case can be any value, such as
24 hours (daily backups) or infinite (no backup at all).
!!!

### Cold and Hot backups

Hot backups have already been defined in the previous section. They require the
presence of a WAL archive, and they are the norm in any modern database
management system.

**Cold backups**, also known as offline backups, are instead physical base backups
taken when the PostgreSQL instance (standby or primary) is shut down. They are
consistent per definition, and they represent a snapshot of the database at the
time it was shut down.

As a result, PostgreSQL instances can be restarted from a cold backup without
the need of a WAL archive, even though they can take advantage of it, if
available (with all the benefits on the recovery side highlighted in the
previous section).

In those situations with a higher RPO (for example, 1 hour or 24 hours), and
shorter retention periods, cold backups represent a viable option to be considered
for your disaster recovery plans.

## Comparing Available Backup Options: Object Stores vs Volume Snapshots

{{name.ln}} currently supports two main approaches for physical backups:

-   **Object store–based backups**, via the [**Barman Cloud
    Plugin**](https://cloudnative-pg.io/plugin-barman-cloud/) or the
    [**deprecated native integration**](backup_barmanobjectstore.md)
-   [**Volume Snapshots**](backup_volumesnapshot.md), using the
    Kubernetes CSI interface and supported storage classes

!!!info Important
CNP-I is designed to enable third parties to build and integrate their own
backup plugins. Over time, we expect the ecosystem of supported backup
solutions to grow.
!!!

### Object Store–Based Backups

Backups to an object store (e.g. AWS S3, Azure Blob, GCS):

-   availability of a viable object store solution in your Kubernetes cluster
-   availability of a trusted storage class that supports volume snapshots
-   size of the database: with object stores, the larger your database, the
    longer backup and, most importantly, recovery procedures take (the latter
    impacts [RTO](before_you_start.md#postgresql-terminology)); in presence of Very Large Databases
    (VLDB), the general advice is to rely on Volume Snapshots as, thanks to
    copy-on-write, they provide faster recovery
-   data mobility and possibility to store or relay backup files on a
    secondary location in a different region, or any subsequent one
-   other factors, mostly based on the confidence and familiarity with the
    underlying storage solutions

### Volume Snapshots

Native volume snapshots:

-   Do not require WAL archiving, though its use is still strongly
    recommended in production
-   Support incremental and differential copies, depending on the
    capabilities of the underlying storage class
-   Support both hot and cold backups
-   Do not support retention policies

### Choosing Between the Two

The best approach depends on your environment and operational requirements.
Consider the following factors:

-   **Object store availability**: Ensure your Kubernetes cluster can access a
    reliable object storage solution, including a stable networking layer.
-   **Storage class capabilities**: Confirm that your storage class supports CSI
    volume snapshots with incremental/differential features.
-   **Database size**: For very large databases (VLDBs), **volume snapshots are
    generally preferred** as they enable faster recovery due to copy-on-write
    technology—this significantly improves your
    [Recovery Time Objective (RTO)](before_you_start.md#postgresql-terminology).
-   **Data mobility**: Object store–based backups may offer greater flexibility
    for replicating or storing backups across regions or environments.
-   **Operational familiarity**: Choose the method that aligns best with your
    team's experience and confidence in managing storage.

### Comparison Summary

| Feature                           | Object Store |   Volume Snapshots   |
| --------------------------------- | :----------: | :------------------: |
| **WAL archiving**                 |   Required   |    Recommended^1^    |
| **Cold backup**                   |       ❌      |           ✅          |
| **Hot backup**                    |       ✅      |           ✅          |
| **Incremental copy**              |       ❌      |         ✅^2^         |
| **Differential copy**             |       ❌      |         ✅^2^         |
| **Backup from a standby**         |       ✅      |           ✅          |
| **Snapshot recovery**             |     ❌^3^     |           ✅          |
| **Retention policies**            |       ✅      |           ❌          |
| **Point-in-Time Recovery (PITR)** |       ✅      | Requires WAL archive |
| **Underlying technology**         | Barman Cloud |    Kubernetes API    |

* * *

> **Notes:**
>
> 1.  WAL archiving must currently use an object store through a plugin (or the
>     deprecated native one).
> 2.  Availability of incremental and differential copies depends on the
>     capabilities of the storage class used for PostgreSQL volumes.
> 3.  Snapshot recovery can be emulated by using the
>     `bootstrap.recovery.recoveryTarget.targetImmediate` option.

## Scheduled Backups

Scheduled backups are the recommended way to implement a reliable backup
strategy in {{name.ln}}. They are defined using the `ScheduledBackup` custom
resource.

!!!info
For a complete list of configuration options, refer to the
[`ScheduledBackupSpec`](pg4k.v1.md#scheduledbackupspec)
in the API reference.
!!!

### Cron Schedule

!!!warning
Beware that this format accepts also the `seconds` field, and it is
different from the `crontab` format in Unix/Linux systems.
!!!

!!!warning
This format differs from the traditional Unix/Linux `crontab`—it includes a
**seconds** field as the first entry.
!!!

Example of a daily scheduled backup:

```yaml
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: ScheduledBackup
metadata:
  name: backup-example
spec:
  schedule: "0 0 0 * * *"  # At midnight every day
  backupOwnerReference: self
  cluster:
    name: pg-backup
  # method: plugin, volumeSnapshot, or barmanObjectStore (default)
```

The schedule `"0 0 0 * * *"` triggers a backup every day at midnight
(00:00:00). In Kubernetes CronJobs, the equivalent expression would be `0 0 * * *`,
since seconds are not supported.

### Backup Frequency and RTO

!!!tip Hint
Backup frequency might impact your recovery time objective ([RTO](before_you_start.md#postgresql-terminology)) after a
disaster which requires a full or Point-In-Time recovery operation. Our
advice is that you regularly test your backups by recovering them, and then
measuring the time it takes to recover from scratch so that you can refine
your RTO predictability. Recovery time is influenced by the size of the
base backup and the amount of WAL files that need to be fetched from the archive
and replayed during recovery (remember that WAL archiving is what enables
continuous backup in PostgreSQL!).
Based on our experience, a weekly base backup is more than enough for most
cases - while it is extremely rare to schedule backups more frequently than once
a day.
!!!

To optimize your disaster recovery strategy based on continuous backup:

-   Regularly test restoring from your backups.
-   Measure the time required for a full recovery.
-   Account for the size of base backups and the number of WAL files that must be
    retrieved and replayed.

In most cases, a **weekly base backup** is sufficient. It is rare to schedule
full backups more frequently than once per day.

!!!note
`.spec.backupOwnerReference` indicates which ownerReference should be put inside
the created backup resources.

-   *none:* no owner reference for created backup objects (same behavior as before the field was introduced)
-   *self:* sets the Scheduled backup object as owner of the backup
-   *cluster:* set the cluster as owner of the backup
!!!

```yaml
spec:
  immediate: true
```

!!!info
For a full list of available options, see the
[`BackupSpec`](pg4k.v1.md#backupspec) in the
API reference.
!!!

To temporarily stop scheduled backups from running:

```yaml
spec:
  suspend: true
```

\### Backup Owner Reference (`.spec.backupOwnerReference`)

Controls which Kubernetes object is set as the owner of the backup resource:

-   `none`: No owner reference (legacy behavior)
-   `self`: The `ScheduledBackup` object becomes the owner
-   `cluster`: The PostgreSQL cluster becomes the owner

## On-Demand Backups

On-demand backups allow you to manually trigger a backup operation at any time
by creating a `Backup` resource.

!!!info
For a full list of available options, see the
[`BackupSpec`](pg4k.v1.md#backupspec) in the
API reference.
!!!

### Example: Requesting an On-Demand Backup

To start an on-demand backup, apply a `Backup` request custom resource like the
following:

```yaml
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Backup
metadata:
  name: backup-example
spec:
  method: barmanObjectStore
  cluster:
    name: pg-backup
```

In this example, the operator will orchestrate the backup process using the
`barman-cloud-backup` tool and store the backup in the configured object store.

### Monitoring Backup Progress

You can check the status of the backup using:

```bash
kubectl describe backup backup-example
```

While the backup is in progress, you'll see output similar to:

```text
Name:         backup-example
Namespace:    default
...
Spec:
  Cluster:
    Name:  pg-backup
Status:
  Phase:       running
  Started At:  2020-10-26T13:57:40Z
Events:        <none>
```

Once the backup has successfully completed, the `phase` will be set to
`completed`, and the output will include additional metadata:

```text
Name:         backup-example
Namespace:    default
...
Status:
  Backup Id:         20201026T135740
  Destination Path:  s3://backups/
  Endpoint URL:      http://minio:9000
  Phase:             completed
  S3 Credentials:
    Access Key Id:
      Name:  minio
      Key:   ACCESS_KEY_ID
    Secret Access Key:
      Name:  minio
      Key:   ACCESS_SECRET_KEY
  Server Name:       pg-backup
  Started At:        2020-10-26T13:57:40Z
  Stopped At:        2020-10-26T13:57:44Z
```

!!!info Important
This feature will not backup the secrets for the superuser and the
application user. The secrets are supposed to be backed up as part of
the standard backup procedures for the Kubernetes cluster.
!!!

!!!info Important
On-demand backups do **not** include Kubernetes secrets for the PostgreSQL
superuser or application user. You should ensure these secrets are included in
your broader Kubernetes cluster backup strategy.
!!!

## Backup Methods

{{name.ln}} currently supports the following backup methods for scheduled
and on-demand backups:

-   `plugin` – Uses a CNP-I plugin (requires `.spec.pluginConfiguration`)
-   `volumeSnapshot` – Uses native [Kubernetes volume snapshots](backup_volumesnapshot.md#how-to-configure-volume-snapshot-backups)
-   `barmanObjectStore` – Uses [Barman Cloud for object storage](backup_barmanobjectstore.md)
    *(deprecated starting with v1.26 in favor of the
    [Barman Cloud Plugin](https://cloudnative-pg.io/plugin-barman-cloud/),
    but still the default for backward compatibility)*

Specify the method using the `.spec.method` field (defaults to
`barmanObjectStore`).

If your cluster is configured to support volume snapshots, you can enable
scheduled snapshot backups like this:

!!!info
Although the standby might not always be up to date with the primary,
in the time continuum from the first available backup to the last
archived WAL this is normally irrelevant. The base backup indeed
represents the starting point from which to begin a recovery operation,
including PITR. Similarly to what happens with
[`pg_basebackup`](https://www.postgresql.org/docs/current/app-pgbasebackup.html),
when backing up from an online standby we do not force a switch of the WAL on the
primary. This might produce unexpected results in the short term (before
`archive_timeout` kicks in) in deployments with low write activity.
!!!

To use the Barman Cloud Plugin as the backup method, set `method: plugin` and
configure the plugin accordingly. You can find an example in the
["Performing a Base Backup" section of the plugin documentation](https://cloudnative-pg.io/plugin-barman-cloud/docs/usage/#performing-a-base-backup)

## Backup from a Standby

Taking a base backup involves reading the entire on-disk data set of a
PostgreSQL instance, which can introduce I/O contention and impact the
performance of the active workload.

To reduce this impact, **{{name.ln}} supports taking backups from a standby
instance**, leveraging PostgreSQL’s built-in capability to perform backups from
read-only replicas.

By default, backups are performed on the **most up-to-date replica** in the
cluster. If no replicas are available, the backup will fall back to the
**primary instance**.

!!!note
The examples in this section are focused on backup target selection and do not
take the backup method (`spec.method`) into account, as it is not relevant to
the scope being discussed.
!!!

### How It Works

When `prefer-standby` is the target (the default behavior), {{name.ln}} will
attempt to:

1.  Identify the most synchronized standby node.
2.  Run the backup process on that standby.
3.  Fall back to the primary if no standbys are available.

This strategy minimizes interference with the primary’s workload.

!!!warning
Although the standby might not always be up to date with the primary,
in the time continuum from the first available backup to the last
archived WAL this is normally irrelevant. The base backup indeed
represents the starting point from which to begin a recovery operation,
including PITR. Similarly to what happens with
[`pg_basebackup`](https://www.postgresql.org/docs/current/app-pgbasebackup.html),
when backing up from an online standby we do not force a switch of the WAL on the
primary. This might produce unexpected results in the short term (before
`archive_timeout` kicks in) in deployments with low write activity.
!!!

### Forcing Backup on the Primary

To always run backups on the primary instance, explicitly set the backup target
to `primary` in the cluster configuration:

```yaml
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Cluster
metadata:
  [...]
spec:
  backup:
    target: "primary"
```

!!!warning
Beware of setting the target to primary when performing a cold backup
with volume snapshots, as this will shut down the primary for
the time needed to take the snapshot, impacting write operations.
This also applies to taking a cold backup in a single-instance cluster, even
if you did not explicitly set the primary as the target.
!!!

### Overriding the Cluster-Wide Target

You can override the cluster-level target on a per-backup basis, using either
`Backup` or `ScheduledBackup` resources. Here's an example of an on-demand
backup:

```yaml
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Backup
metadata:
  [...]
spec:
  cluster:
    name: [...]
  target: "primary"
```

In this example, even if the cluster’s default target is `prefer-standby`, the
backup will be taken from the primary instance.

## Retention Policies

{{name.ln}} is evolving toward a **backup-agnostic architecture**, where
backup responsibilities are delegated to external **CNP-I plugins**. These
plugins are expected to offer advanced and customizable data protection
features, including sophisticated retention management, that go beyond the
built-in capabilities and scope of {{name.ln}}.

As part of this transition, the `spec.backup.retentionPolicy` field in the
`Cluster` resource is **deprecated** and will be removed in a future release.

For more details on available retention features, refer to your chosen plugin’s documentation.
For example: ["Retention Policies" with Barman Cloud Plugin](https://cloudnative-pg.io/plugin-barman-cloud/docs/retention/).

!!!info Important
Users are encouraged to rely on the retention mechanisms provided by the
backup plugin they are using. This ensures better flexibility and consistency
with the backup method in use.
!!!
