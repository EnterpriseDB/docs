---
title: "Notifications"
---

<div id="notifications" class="registered_link"></div>

will send e-mail notifications and/or invoke a notification script when a notable event occurs that affects the cluster. If you have configured to send an email notification, you must have an SMTP server running on port 25 on each node of the cluster. Use the following parameters to configure notification behavior for :

```
user.email
script.notification
from.email
```

For more information about editing the configuration properties, see `Specifying Cluster Properties <cluster_properties>`.

The body of the notification contains details about the event that triggered the notification, and about the current state of the cluster. For example:

```
EFM node: 10.0.1.11
Cluster name: acctg
Database name: postgres
VIP: ip_address (Active|Inactive)
Database health is not being monitored.
```

The VIP field displays the IP address and state of the virtual IP if implemented for the node.

assigns a severity level to each notification. The following levels indicate increasing levels of attention required:

-   `INFO` indicates an informational message about the agent and does not require any manual intervention (for example, has started or stopped). See [List of INFO level notifications](#notifications_info)
-   `WARNING` indicates that an event has happened that requires the administrator to check on the system (for example, failover has occurred). See [List of WARNING level notifications](#notifications_warning)
-   `SEVERE` indicates that a serious event has happened and requires the immediate attention of the administrator (for example, failover was attempted, but was unable to complete). See [List of SEVERE level notifications](#notifications_severe)

The severity level designates the urgency of the notification. A notification with a severity level of `SEVERE` requires user attention immediately, while a notification with a severity level of `INFO` will call your attention to operational information about your cluster that does not require user action. Notification severity levels are not related to logging levels; all notifications are sent regardless of the log level detail specified in the configuration file.

You can use the [notification.level](04_configuring_efm/01_cluster_properties/#notification_level) property to specify the minimum severity level that will trigger a notification.

**Please note:** : In addition to sending notices to the administrative email address, all notifications are recorded in the cluster log file (`/var/log/efm-4.0/<cluster_name>.log`).

The conditions listed in the table below will trigger an `INFO` level notification:

<div id="notifications_info" class="registered_link"></div>

<table><thead><tr class="header"><th><strong>Subject</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr class="odd"><td>Executed fencing script</td><td>Executed fencing script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="even"><td>Executed post-promotion script</td><td>Executed post-promotion script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="odd"><td>Executed remote pre-promotion script</td><td>Executed remote pre-promotion script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="even"><td>Executed remote post-promotion script</td><td>Executed remote post-promotion script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="odd"><td>Executed post-database failure script</td><td>Executed post-database failure script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="even"><td>Executed primary isolation script</td><td><blockquote><p>Executed primary isolation script <em>script_name</em> Results: <em>script_results</em></p></blockquote></td></tr><tr class="odd"><td>Witness agent running on <em>node_address</em> for cluster <em>cluster_name</em></td><td>Witness agent is running.</td></tr><tr class="even"><td>Primary agent running on <em>node_address</em> for cluster <em>cluster_name</em></td><td><blockquote><p>Primary agent is running and database health is being monitored.</p></blockquote></td></tr><tr class="odd"><td>Standby agent running on <em>node_address</em> for cluster <em>cluster_name</em></td><td>Standby agent is running and database health is being monitored.</td></tr><tr class="even"><td>Idle agent running on node <em>node_address</em> for cluster <em>cluster_name</em></td><td>Idle agent is running. After starting the local database, the agent can be resumed.</td></tr><tr class="odd"><td>Assigning VIP to node <em>node_address</em></td><td>Assigning VIP <em>VIP_address</em> to node <em>node_address</em> Results: <em>script_results</em></td></tr><tr class="even"><td>Releasing VIP from node <em>node_address</em></td><td>Releasing VIP <em>VIP_address</em> from node <em>node_address</em> Results: <em>script_results</em></td></tr><tr class="odd"><td>Starting auto resume check for cluster <em>cluster_name</em></td><td>The agent on this node will check every <em>auto.resume.period</em> seconds to see if it can resume monitoring the failed database. The cluster should be checked during this time and the agent stopped if the database will not be started again. See the agent log for more details.</td></tr><tr class="even"><td>Executed agent resumed script</td><td>Executed agent resumed script <em>script_name</em> Results: <em>script_results</em></td></tr><tr class="odd"><td>WAL logs backed up during promotion</td><td>When reconfiguring this standby to follow the new primary, the pg_xlog or pg_wal contents were backed up in the <em>pgdata</em> directory. This backup should be removed when convenient to free up disk space.</td></tr></tbody></table>

The conditions listed in the table below will trigger a *WARNING* level notification:

<div id="notifications_warning" class="registered_link"></div>

<table><thead><tr class="header"><th><strong>Subject</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr class="odd"><td>Witness agent exited on <em>node_address</em> for cluster <em>cluster_name</em></td><td>Witness agent has exited.</td></tr><tr class="even"><td>Primary agent exited on <em>node_address</em> for cluster <em>cluster_name</em></td><td><blockquote><p>Database health is not being monitored.</p></blockquote></td></tr><tr class="odd"><td>Cluster <em>cluster_name</em> notified that primary node has left</td><td><blockquote><p>Failover is disabled for the cluster until the primary agent is restarted.</p></blockquote></td></tr><tr class="even"><td>Standby agent exited on <em>node_address</em> for cluster <em>cluster_name</em></td><td>Database health is not being monitored.</td></tr><tr class="odd"><td>Agent exited during promotion on <em>node_address</em> for cluster <em>cluster_name</em></td><td>Database health is not being monitored.</td></tr><tr class="even"><td>Agent exited on <em>node_address</em> for cluster <em>cluster_name</em></td><td>The agent has exited. This is generated by an agent in the Idle state.</td></tr><tr class="odd"><td>Agent exited for cluster <em>cluster_name</em></td><td>The agent has exited. This notification is usually generated during startup when an agent exits before startup has completed.</td></tr><tr class="even"><td>Virtual IP address assigned to non-primary node</td><td><blockquote><p>The virtual IP address appears to be assigned to a non-primary node. To avoid any conflicts, will release the VIP. You should confirm that the VIP is assigned to your primary node and manually reassign the address if it is not.</p></blockquote></td></tr><tr class="odd"><td>Virtual IP address not assigned to primary node.</td><td><blockquote><p>The virtual IP address appears to not be assigned to a primary node. EDB Postgres will attempt to reacquire the VIP.</p></blockquote></td></tr><tr class="even"><td>No standby agent in cluster for cluster <em>cluster_name</em></td><td>The standbys on <em>cluster_name</em> have left the cluster.</td></tr><tr class="odd"><td>Standby agent failed for cluster <em>cluster_name</em></td><td>A standby agent on <em>cluster_name</em> has left the cluster, but the coordinator has detected that the standby database is still running.</td></tr><tr class="even"><td>Standby database failed for cluster <em>cluster_name</em></td><td>A standby agent has signaled that its database has failed. The other nodes also cannot reach the standby database.</td></tr><tr class="odd"><td>Standby agent cannot reach database for cluster <em>cluster_name</em></td><td>A standby agent has signaled database failure, but the other nodes have detected that the standby database is still running.</td></tr><tr class="even"><td>Cluster <em>cluster_name</em> has dropped below three nodes</td><td>At least three nodes are required for full failover protection. Please add witness or agent node to the cluster.</td></tr><tr class="odd"><td>Subset of cluster <em>cluster_name</em> disconnected from primary</td><td><blockquote><p>This node is no longer connected to the majority of the cluster <em>cluster_name</em>. Because this node is part of a subset of the cluster, failover will not be attempted. Current nodes that are visible are: <em>node_address</em></p></blockquote></td></tr><tr class="even"><td>Promotion has started on cluster <em>cluster_name</em>.</td><td>The promotion of a standby has started on cluster <em>cluster_name</em>.</td></tr><tr class="odd"><td>Witness failure for cluster <em>cluster_name</em></td><td>Witness running at <em>node_address</em> has left the cluster.</td></tr><tr class="even"><td>Idle agent failure for cluster <em>cluster_name</em>.</td><td>Idle agent running at <em>node_address</em> has left the cluster.</td></tr><tr class="odd"><td>One or more nodes isolated from network for cluster <em>cluster_name</em></td><td>This node appears to be isolated from the network. Other members seen in the cluster are: <em>node_name</em></td></tr><tr class="even"><td>Node no longer isolated from network for cluster <em>cluster_name</em>.</td><td>This node is no longer isolated from the network.</td></tr><tr class="odd"><td>Standby agent tried to promote, but primary DB is still running</td><td><blockquote><p>The standby EFM agent tried to promote itself, but detected that the primary DB is still running on <em>node_address</em>. This usually indicates that the primary EFM agent has exited. Failover has NOT occurred.</p></blockquote></td></tr><tr class="even"><td>Standby agent started to promote, but primary has rejoined.</td><td><blockquote><p>The standby EFM agent started to promote itself, but found that a primary agent has rejoined the cluster. Failover has NOT occurred.</p></blockquote></td></tr><tr class="odd"><td>Standby agent tried to promote, but could not verify primary DB</td><td><blockquote><p>The standby EFM agent tried to promote itself, but could not detect whether or not the primary DB is still running on <em>node_address</em>. Failover has NOT occurred.</p></blockquote></td></tr><tr class="even"><td>Standby agent tried to promote, but VIP appears to still be assigned</td><td>The standby EFM agent tried to promote itself, but could not because the virtual IP address (<em>VIP_address</em>) appears to still be assigned to another node. Promoting under these circumstances could cause data corruption. Failover has NOT occurred.</td></tr><tr class="odd"><td>Standby agent tried to promote, but appears to be orphaned</td><td>The standby EFM agent tried to promote itself, but could not because the well-known server (<em>server_address</em>) could not be reached. This usually indicates a network issue that has separated the standby agent from the other agents. Failover has NOT occurred.</td></tr><tr class="even"><td>Potential manual failover required on cluster <em>cluster_name</em>.</td><td>A potential failover situation was detected for cluster <em>cluster_name</em>. Automatic failover has been disabled for this cluster, so manual intervention is required.</td></tr><tr class="odd"><td>Failover has completed on cluster <em>cluster_name</em></td><td>Failover has completed on cluster <em>cluster_name</em>.</td></tr><tr class="even"><td>Lock file for cluster <em>cluster_name</em> has been removed</td><td>The lock file for cluster <em>cluster_name</em> has been removed from: <em>path_name</em> on node <em>node_address</em>. This lock prevents multiple agents from monitoring the same cluster on the same node. Please restore this file to prevent accidentally starting another agent for cluster.</td></tr><tr class="odd"><td>A recovery file for cluster <em>cluster_name</em> has been found on primary node</td><td><blockquote><p>A recovery file for cluster <em>cluster_name</em> has been found at: <em>path_name</em> on primary node <em>node_address</em>. This may be problematic should you attempt to restart the DB on this node.</p></blockquote></td></tr><tr class="even"><td>recovery_target_timeline is not set to latest in recovery settings</td><td>The recovery_target_timeline parameter is not set to latest in the recovery settings. The standby server will not be able to follow a timeline change that occurs when a new primary is promoted.</td></tr><tr class="odd"><td>Promotion has not occurred for cluster <em>cluster_name</em></td><td>A promotion was attempted but there is already a node being promoted: <em>ip_address</em>.</td></tr><tr class="even"><td>Standby not reconfigured after failover in cluster <em>cluster_name</em></td><td>The auto.reconfigure property has been set to false for this node. The node has not been reconfigured to follow the new primary node after a failover.</td></tr><tr class="odd"><td><p>Could not resume replay for cluster <em>cluster_name</em></p></td><td><p>Could not resume replay for standby being promoted. Manual intervention may be required. Error: <em>error_decription</em> This error is returned if the server encounters an error when invoking replay during the promotion of a standby.</p></td></tr><tr class="even"><td>Could not resume replay for standby <em>standby_id</em>.</td><td>Could not resume replay for standby. Manual intervention may be required. Error: <em>error_message</em>.</td></tr><tr class="odd"><td>Possible problem with database timeout values</td><td>Your remote.timeout value (<em>value</em>) is higher than your local.timeout value (<em>value</em>). If the local database takes too long to respond, the local agent could assume that the database has failed though other agents can connect. While this will not cause a failover, it could force the local agent to stop monitoring, leaving you without failover protection.</td></tr><tr class="even"><td>No standbys available for promotion in cluster <em>cluster_name</em></td><td>The current number of standby nodes in the cluster has dropped to the minimum number: <em>number</em>. There cannot be a failover unless another standby node(s) is added or made promotable.</td></tr><tr class="odd"><td>No promotable standby for cluster <em>cluster_name</em></td><td>The current failover priority list in the cluster is empty. You have removed the only promotable standby for the cluster <em>cluster_name</em>. There cannot be a failover unless another promotable standby node(s) is added or made promotable by adding to failover priority list.</td></tr><tr class="even"><td>Synchronous replication has been reconfigured for cluster <em>cluster_name</em></td><td>The number of synchronous standby nodes in the cluster has dropped below <em>number</em>. The synchronous standby names on primary has been reconfigured to: <em>new synchronous_standby_names value</em>.</td></tr><tr class="odd"><td>Synchronous replication has been disabled for cluster <em>cluster_name</em>.</td><td>The number of synchronous standby nodes in the cluster has dropped below <em>count</em>. The primary has been taken out of synchronous replication mode.</td></tr><tr class="even"><td>Could not reload database configuration.</td><td>Could not reload database configuration. Manual intervention is required. Error: <em>error_message</em>.</td></tr><tr class="odd"><td>Custom monitor timeout for cluster <em>cluster_name</em></td><td>The following custom monitoring script has timed out: <em>script_name</em></td></tr><tr class="even"><td><p>Custom monitor 'safe mode' failure for cluster <em>cluster_name</em></p></td><td><p>The following custom monitor script has failed, but is being run in "safe mode": <em>script_name</em>. Output: <em>script_results</em></p></td></tr><tr class="odd"><td><em>primary.shutdown.as.failure</em> set to true for primary node</td><td>The <em>primary.shutdown.as.failure</em> property has been set to true for this cluster. Stopping the primary agent without stopping the entire cluster will be treated by the rest of the cluster as an immediate primary agent failure. If maintenance is required on the primary database, shut down the primary agent and wait for a notification from the remaining nodes that failover will not happen.</td></tr><tr class="even"><td>Primary cannot ping local database for cluster <em>cluster_name</em></td><td>The primary agent can no longer reach the local database running at <em>node_address</em>. Other nodes are able to access the database remotely, so the primary will become IDLE and attempt to resume monitoring the database.</td></tr></tbody></table>

<div id="notifications_severe" class="registered_link"></div>

The conditions listed in the table below will trigger a *SEVERE* notification:

<table><thead><tr class="header"><th><strong>Subject</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr class="odd"><td>Standby database restarted but EFM cannot connect</td><td>The start or restart command for the database ran successfully but the database is not accepting connections. EFM will keep trying to connect for up to <em>restart.connection.timeout</em> seconds.</td></tr><tr class="even"><td>Unable to connect to DB on <em>node_address</em></td><td>The maximum connections limit has been reached.</td></tr><tr class="odd"><td>Unable to connect to DB on <em>node_address</em></td><td>Invalid password for db.user=<em>user_name</em>.</td></tr><tr class="even"><td>Unable to connect to DB on <em>node_address</em></td><td>Invalid authorization specification.</td></tr><tr class="odd"><td>Primary cannot resume monitoring local database for cluster <em>cluster_name</em></td><td>The primary agent can no longer reach the local database running at <em>node_address</em>. Other nodes are able to access the database remotely, so the primary will not release the VIP and/or create a recovery.conf file. The primary agent will remain IDLE until the resume command is run to resume monitoring the database.</td></tr><tr class="even"><td><p>Fencing script error</p></td><td><p>Fencing script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em> Failover has NOT occurred.</p></td></tr><tr class="odd"><td><p>Post-promotion script failed</p></td><td><p>Post-promotion script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em></p></td></tr><tr class="even"><td><p>Remote post-promotion script failed</p></td><td><p>Remote post-promotion script <em>script_name</em> failed to execute successfully Exit Value: <em>exit_code</em> Results: <em>script_results</em></p><p>Node: <em>node_address</em></p></td></tr><tr class="odd"><td><p>Remote pre-promotion script failed</p></td><td><p>Remote pre-promotion script <em>script_name</em> failed to execute successfully Exit Value: <em>exit_code</em> Results: <em>script_results</em></p><p>Node: <em>node_address</em></p></td></tr><tr class="even"><td><p>Post-database failure script error</p></td><td><p>Post-database failure script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em></p></td></tr><tr class="odd"><td><p>Agent resumed script error</p></td><td><p>Agent resumed script <em>script_name</em> failed to execute successfully. Results: <em>script_results</em></p></td></tr><tr class="even"><td><p>Primary isolation script failed</p></td><td><p>Primary isolation script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em></p></td></tr><tr class="odd"><td><p>Could not promote standby</p></td><td><p>The promote command failed on node. Could not promote standby. Error details: <em>error_details</em></p></td></tr><tr class="even"><td>Error creating recovery.conf file on <em>node_address</em> for cluster <em>cluster_name</em></td><td>There was an error creating the recovery.conf file on primary node <em>node_address</em> during promotion. Promotion has continued, but requires manual intervention to ensure that the old primary node can not be restarted. Error details: <em>message_details</em></td></tr><tr class="odd"><td>An unexpected error has occurred for cluster <em>cluster_name</em></td><td>An unexpected error has occurred on this node. Please check the agent log for more information. Error: <em>error_details</em></td></tr><tr class="even"><td>Primary database being fenced off for cluster <em>cluster_name</em></td><td><blockquote><p>The primary database has been isolated from the majority of the cluster. The cluster is telling the primary agent at <em>ip_address</em> to fence off the primary database to prevent two primarys when the rest of the failover manager cluster promotes a standby.</p></blockquote></td></tr><tr class="odd"><td>Isolated primary database shutdown.</td><td><blockquote><p>The isolated primary database has been shutdown by failover manager.</p></blockquote></td></tr><tr class="even"><td>Primary database being fenced off for cluster <em>cluster_name</em></td><td><blockquote><p>The primary database has been isolated from the majority of the cluster. Before the primary could finish detecting isolation, a standby was promoted and has rejoined this node in the cluster. This node is isolating itself to avoid more than one primary database.</p></blockquote></td></tr><tr class="odd"><td>Could not assign VIP to node <em>node_address</em></td><td>Failover manager could not assign the VIP address for some reason.</td></tr><tr class="even"><td><em>primary_or_standby</em> database failure for cluster <em>cluster_name</em></td><td><blockquote><p>The database has failed on the specified node.</p></blockquote></td></tr><tr class="odd"><td>Agent is timing out for cluster <em>cluster_name</em></td><td>This agent has timed out trying to reach the local database. After the timeout, the agent could successfully ping the database and has resumed monitoring. However, the node should be checked to make sure it is performing normally to prevent a possible database or agent failure.</td></tr><tr class="even"><td>Resume timed out for cluster <em>cluster_name</em></td><td>This agent could not resume monitoring after reconfiguring and restarting the local database. See agent log for details.</td></tr><tr class="odd"><td>Internal state mismatch for cluster <em>cluster_name</em></td><td>The failover manager cluster's internal state did not match the actual state of the cluster members. This is rare and can be caused by a timing issue of nodes joining the cluster and/or changing their state. The problem should be resolved, but you should check the cluster status as well to verify. Details of the mismatch can be found in the agent log file.</td></tr><tr class="even"><td>Failover has not occurred</td><td>An agent has detected that the primary database is no longer available in cluster <em>cluster_name</em>, but there are no standby nodes available for failover.</td></tr><tr class="odd"><td>Database in wrong state on <em>node_address</em></td><td>The standby agent has detected that the local database is no longer in recovery. The agent will now become idle. Manual intervention is required.</td></tr><tr class="even"><td>Database in wrong state on <em>node_address</em></td><td>The primary agent has detected that the local database is in recovery. The agent will now become idle. Manual intervention is required.</td></tr><tr class="odd"><td><p>Database connection failure for cluster <em>cluster_name</em></p></td><td><p>This node is unable to connect to the database running on: <em>node_address</em></p><p>Until this is fixed, failover may not work properly because this node will not be able to check if the database is running or not.</p></td></tr><tr class="even"><td><p>Standby custom monitor failure for cluster <em>cluster_name</em></p></td><td><p>The following custom monitor script has failed on a standby node. The agent will stop monitoring the local database. Script location: <em>script_name</em> Script output: <em>script_results</em></p></td></tr><tr class="odd"><td><p>Primary custom monitor failure for cluster <em>cluster_name</em></p></td><td><p>The following custom monitor script has failed on a primary node. EFM will attempt to promote a standby. Script location: <em>script_name</em> Script output: <em>script_results</em></p></td></tr><tr class="even"><td>Loopback address set for <em>ping.server.ip</em></td><td>Loopback address is set for <em>ping.server.ip</em> property. This setting can interfere with the network isolation detection and hence it should be changed.</td></tr><tr class="odd"><td><p>Load balancer attach script error</p></td><td><p>Load balancer attach script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em></p></td></tr><tr class="even"><td><p>Load balancer detach script error</p></td><td><p>Load balancer detach script <em>script_name</em> failed to execute successfully. Exit Value: <em>exit_code</em> Results: <em>script_results</em></p></td></tr><tr class="odd"><td>Not enough synchronous standbys available in cluster <em>cluster_name</em>.</td><td>The number of synchronous standby nodes in the cluster has dropped to <em>count</em>. All write queries on the primary will be blocked until enough synchronous standby nodes are added.</td></tr></tbody></table>
