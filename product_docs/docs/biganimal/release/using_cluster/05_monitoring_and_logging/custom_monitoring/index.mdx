---
title: "Other Monitoring and Logging Solutions"
---

 BigAnimal provides a Prometheus-compatible endpoint you can use to connect to your own monitoring infrastructure as well as Postgres logs via blob storage. 
  
 To enable this on your clusters, contact [BigAnimal Support](/biganimal/release/overview/support).
 

## Metrics

You can access metrics in a [Prometheus format](https://prometheus.io/docs/concepts/data_model/) if you request to have this feature enabled via support. You can retrieve the hostname and port for your clusters by using the Prometheus URL available on the Monitoring and logging tab on each cluster's detail page in the BigAnimal portal. 

We have provided some [example metrics](/biganimal/latest/using_cluster/05_monitoring_and_logging/custom_monitoring/example_metrics/) to help get you started.

If private networking is configured for the metrics, you likely need to configure network peering:

* [Configuring AWS VPC peering](/biganimal/latest/using_cluster/02_connecting_your_cluster/02_connecting_from_aws/02_vpc_peering/)

* [Configuring Azure virtual network peering](/biganimal/release/using_cluster/02_connecting_your_cluster/01_connecting_from_azure/02_virtual_network_peering)

### Patterns for accessing metrics

A common pattern for metric shipping is to have the vendor-supplied agent scrape the metrics endpoint and send the metrics to the desired platform.

![metrics pattern](images/metrics_pattern.png)

Here are links for more information on some common monitoring services:

- [Datadog open metrics integration](https://docs.datadoghq.com/integrations/openmetrics/)

- [Dynatrace Prometheus extension](https://www.dynatrace.com/support/help/extend-dynatrace/extensions20/data-sources/prometheus-extensions)

- [New Relic Prometheus integration](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/#OpenMetrics)

- Self-managed Grafana (not Grafana Cloud): [Grafana Prometheus datasource](https://grafana.com/docs/grafana/latest/datasources/prometheus/)

## Logs

You can view your logs in your cloud provider's blob storage solution if you request to have this feature enabled via support. You can retrieve the location of your object storage on the Monitoring and logging tab on your cluster's detail page in the BigAnimal portal. 

The general pattern for getting logs from blob storage in to the cloud provider's solution is to write a custom serverless function that watches the blob storage and uploads to the desired solution. 

![logs pattern](images/logs_pattern.png)

### Watching for logs on AWS

You could leverage some Python code to read the S3 bucket, then use the AWS APIs to upload to your custom monitoring solution. For example, CloudWatch: [aws-load-balancer-logs-to-cloudwatch](https://github.com/rupertbg/aws-load-balancer-logs-to-cloudwatch).

### Watching for logs on Azure

You could leverage [Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview) to read the Azure Blob Storage object, then use the API to upload the data to CloudWatch.

### Uploading logs to common third-party providers

Once your function has observed new log data, you can use your monitoring provider's API to push log data to their platform.

Some platform providers have limitations regarding the ingestion of logs. Read the vendor documentation carefully.

- Datadog

  - AWS: [Collecting logs from S3 buckets](https://docs.datadoghq.com/logs/guide/send-aws-services-logs-with-the-datadog-lambda-function/?tab=awsconsole#collecting-logs-from-s3-buckets)

  - Azure: [Log collection from Blob Storage](https://docs.datadoghq.com/integrations/azure/?tab=blobstorage#create-a-new-azure-blob-storage-function)

- Dynatrace

  - AWS: [S3 log forwarder](https://github.com/dynatrace-oss/dynatrace-aws-s3-log-forwarder)

  - Azure: [Azure log forwarder](https://github.com/dynatrace-oss/dynatrace-azure-log-forwarder)
    !!! Warning
        At the time of writing, the Dynatrace integration only works if you can stream the logs from Azure Storage to Azure event hub.

- New Relic

  - AWS: [Lambda for sending logs from S3](https://docs.newrelic.com/docs/logs/forward-logs/aws-lambda-sending-logs-s3/)

  - Azure: [Send logs from Azure Blob storage](https://docs.newrelic.com/docs/logs/forward-logs/azure-log-forwarding/#azure-blob-storage)
