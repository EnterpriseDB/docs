---
title: "High Availability Patterns for PEM Deployment"
navTitle: "High Availability Patterns"
redirects:
- /pem/latest/pem_ha_setup/
---

If you require your PEM deployment to be highly available, you can deploy multiple instances of the PEM backend database and frontend web application in a High Availbility (HA) topology.
This page details the fundamental requirements of such a topology.

## The PEM backend database

An HA backend for PEM is much like any other HA Postgres cluster. The key requirements for the backend are as follows:

### There can be only one

There must be exactly one instance of the PEM backend database accepting write connections at a given time.
We will call this instance “the primary PEM backend database” or just “the primary”.
Other instances of the backend database must be replicas of the primary and you must have some method of promoting a replica to replace a failed primary.
This can be achieved by using a supported failover manager to manage promotion in the cluster.
We support EDB Failover Manager (EFM) or Patroni for this purpose.
EDB Postgres Distributed is not supported as a PEM backend.

### Connections must go to the primary

Connections from the active PEM web application instance(s) and all running PEM agents must be made to the primary PEM backend database.
There are two broad approaches to routing traffic to the primary:

1. Provide a single endpoint for clients which always routes to the primary
This can be achieved in many ways including a VIP, but also including a network load balancer or proxy.

2. Teach clients to switch the endpoint they use when the primary changes
This is typically achieved using multi-host connection strings. 


## Running multiple web applications

Web application HA is achieved by running multiple instances of the web application, all connected to the primary as noted above.
Unlike the backend, it is fine to have multiple instances of the PEM web application running simultaneously.

These instances are not stateless, you cannot route subsequent requests within the same user session to different instances and expect it to work. One user session should take place entirely on a single instance.

User preferences are saved locally so will not be shared. This can be reconfigured so the user preferences are stored in the backend.

Files generated, for example by performing a dump of a database, are stored locally so will not be available on a different instance. You could potentially mount a shared volume for this, although you would then have to ensure the HA of the volume.

## Upgrading an HA PEM

You must have a viable procedure to upgrade the cluster

The configure_pem_server.sh script expects to be able to stop and start the PEM service to make config changes. If this happens in an HA cluster it will just cause a failover.

## Load balancers, proxies and VIPs

In our reference architectures below, we use the label ‘Proxy/VIP’ for any component that presents a single endpoint for inbound connections and routes that traffic to the primary. 
However, it can be any component that provides this facility. A few techniques that we recommend are detailed below.

### Using a Virtual IP (VIP) set by the failover manager

A virtual IP or VIP is an IP address that is not bound to a physical network interface on a single server, but can instead be freely moved around within the subnet. Because VIP is implemented by the networking stack of the server OS it doesn’t require any additional hardware or software. To use a VIP to route traffic to the primary, you need to configure your failover manager to assign the VIP to the primary during failover. EFM has built-in functionality for working with VIP so we tend to recommend EFM if VIP is your choice for routing.

VIPs cannot be used if your cluster spans multiple subnets as is typically the case in multi-region cloud deployments.

### Using a load balancer

If your environment provides a load balancer (Elastic Load Balancer in AWS or F5 on-prem for example), you can leverage this to route traffic to the primary. There are generally two ways to do this:

1. Have your failover manager connect to the load balancer and reconfigure it to route inbound connections to the new primary on failover

1. Use a failover manager that provides an HTTP endpoint on each node that can be polled by the load balancer to determine whether the node is accepting traffic. Only the primary should return a positive result (for example HTTP code 200) meaning the load balancer will switch all traffic to the primary within one poll interval.

This can be achieved with EFM and the additional lightweight efm-api-node-state service 

Patroni supports this pattern natively via the /primary or /read-write endpoints.

Pattern 2 is generally preferable because it fits with how load balancers are generally designed to work and does not require you to provide any additional administrative access to the load balancer.

!!! Note
The reason we say “if your environment provides a load balancer” is that this solution needs to be properly implemented to avoid being a single point of failure. The load balancer itself needs to be highly available, meaning it will need to leverage environment-level things like DNS or elastic IPs to ensure traffic is routed to an available load balancer.

Simply adding an instance of HAProxy or pgBouncer in front of your database is not sufficient and EDB support cannot help you build a load balancer. It needs to be a production-strength solution approved and supported by your organisation.
!!!
