---
title: "Configuration Expert Recommendations"
---

<div id="pe_configuration_expert_recommendations" class="registered_link"></div>


|                   |                                                                                                                                                                             |
|-------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Rule              | Check shared\_buffers                                                                                                                                                       |
| Recommendation    | Consider adjusting shared\_buffers                                                                                                                                          |
| Trigger           | shared\_buffers &lt; (OS == Windows ? 64MB : MIN(0.20 \* (system\_memory - 256MB), 6GB)) or shared\_buffers &gt; (OS == Windows ? 512MB : MAX(0.35 \* system\_memory, 8GB)) |
| Recommended Value | system\_memory &lt; 1GB ? MAX((system\_memory - 256MB) / (OS == Windows ? 6 : 3), 64MB), OS == Windows ? MAX(system\_memory / 8, 256MB) : MAX(system\_memory / 4, 8GB)      |
| Severity          | Medium                                                                                                                                                                      |

**Description:** The configuration variable shared\_buffers controls the amount of memory reserved by PostgreSQL for its internal buffer cache. Setting this value too low may result in "thrashing" the buffer cache, resulting in excessive disk activity and degraded performance. However, setting it too high may also cause performance problems. PostgreSQL relies on operating system caching to a significant degree , and setting this value too high may result in excessive "double buffering" that can degrade performance. It also increases the internal costs of managing the buffer pool. On UNIX-like systems, a good starting value is approximately 25% of system memory, but not more than 8GB. On Windows systems, values between 64MB and 512MB typically perform best. The optimal value is workload-dependent, so it may be worthwhile to try several different values and benchmark your system to determine which one delivers best performance.

Note: PostgreSQL will fail to start if the necessary amount of shared\_memory cannot be located. This is usually due to an operating system limitation which can be raised by changing a system configuration setting, often called shmall.See the documentation for more details. You must set this limit to a value somewhat higher than the amount of memory required for shared\_buffers,because PostgreSQL's shared memory allocation also includes amounts required for other purposes.

|                   |                                                                                                                                                                                    |
|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Rule              | Check work\_mem                                                                                                                                                                    |
| Recommendation    | Consider adjusting work\_mem                                                                                                                                                       |
| Trigger           | given spare\_mem = system\_memory - (OS == Windows ? 256MB : MAX(0.25 \* system\_memory, 8GB)) then work\_mem &lt; MAX(1MB, spare\_mem / 512) or work\_mem &gt; (spare\_mem / 128) |
| Recommended Value | given spare\_mem defined as on the previous line then MAX (1MB, spare\_mem / 256)                                                                                                  |
| Severity          | Medium                                                                                                                                                                             |

**Description:** The configuration variable work\_mem controls the amount of memory PostgreSQL will use for each individual hash or sort operation. When a sort would use more than this amount of memory, the planner will arrange to perform an external sort using disk files. While this algorithm is memory efficient, it is much slower than an in-memory quick sort. Similarly, when a hash join would use more than this amount of memory, the planner will arrange to perform it in multiple batches, which saves memory but is likewise much slower. In either case, the planner may in the alternative choose some other plan that does not require the sort or hash operation, but this too is often less efficient. Therefore, for good performance it is important to set this parameter high enough to allow the planner to choose good plans. However, each concurrently executing query can potentially involve several sorts or hashes, and the number of queries on the system can vary greatly Therefore, a value for this setting that works well when the system is lightly loaded may result in swapping when the system becomes more heavily loaded. Swapping has very negative effects on database performance and should be avoided, so it is usually wise to set this value somewhat conservatively.

Note: work\_mem can be adjusted for particular databases, users, or user-and -database combinations by using the commands ALTER ROLE and ALTER DATABASE It can also be changed for a single session using the SET command. This can be helpful when particular queries can be shown to run much faster with a value of work\_mem that is too high to be applied to the system as a whole.

|                |                                    |
|----------------|------------------------------------|
| Rule           | Check max\_connections             |
| Recommendation | Consider using a connection pooler |
| Trigger        | max\_connections &gt; 100          |
| Severity       | Medium                             |

**Description:** The configuration variable max\_connection is set to a value greater than 100. PostgreSQL performs best when the number of simultaneous connections is low. Peak throughput is typically achieved when the connection count is limited to is limited to approximately twice the number of system CPU cores plus the number of spindles available for disk I/O (in the case of an SSD or other non-rotating media, some experimentation may be needed to determine the "effective spindle count"). Installing a connection pooler, such as pgpool-II or pgbouncer, can allow many clients to be multiplexed onto a smaller number of server connections ,sometimes resulting in dramatic performance gains.

|                   |                                                                                                                                                                                                                |
|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Rule              | Check maintenance\_work\_mem                                                                                                                                                                                   |
| Recommendation    | Consider adjusting maintenance\_work\_mem                                                                                                                                                                      |
| Trigger           | spare\_mem = system\_memory - (OS == Windows ? 256MB : MAX(0.25 \* system\_memory, 8GB)) then maintenance\_work\_mem &lt; MAX(16MB, spare\_mem / 32) or maintenance\_work\_mem &gt; MIN(spare\_mem / 8, 256MB) |
| Recommended Value | spare\_mem as defined on the previous line then MIN(spare\_mem/16, 256MB)                                                                                                                                      |
| Severity          | Low                                                                                                                                                                                                            |

**Description:** The configuration variable maintenance\_work\_mem controls the amount of memory PostgreSQL will use for maintenance operations such as CREATE INDEX and VACUUM. Increasing this setting from the default of 16MB to 256MB can make these operations run much faster. Higher settings typically do not produce a significant further improvement. On PostgreSQL 8.3 and higher, multiple autovacuum processes may be running at one time (up to autovacuum\_max\_workers, which defaults to 3), and each such process will use the amount of dedicated memory dictated by this parameter. This should be kept in mind when setting this parameter, especially on systems with relatively modest amounts of physical memory, so as to avoid swapping. Swapping has very negative effects on database performance and should be avoided. If the value recommended above is less than 256MB, it is chosen with this consideration in mind. However, the optimal value is workload-dependent, so it may be worthwhile to experiment with higher or lower settings.

|                |                                               |
|----------------|-----------------------------------------------|
| Rule           | Check effective\_io\_concurrency              |
| Recommendation | Consider adjusting effective\_io\_concurrency |
| Trigger        | effective\_io\_concurrency &lt; 2             |
| Severity       | Low                                           |

**Description:** If the PostgreSQL data files are located on a RAID array or SSD, effective\_io\_concurrency should be set to the approximate number of I/O requests that the system can service simultaneously. For RAID arrays, this is typically equal to the number of drives in the array. For SSDs, some experimentation may be needed to determine the most effective value. Setting this parameter to an appropriate value impoves the performance of bitmap index scans. The default value of 1 is appropriate for cases where all PostgreSQL data files are located on a single spinning medium.

|                |                                  |
|----------------|----------------------------------|
| Rule           | Check fsync is enabled           |
| Recommendation | Consider configuring fsync = on. |
| Trigger        | fsync = off                      |
| Severity       | High                             |

**Description:** When fsync is set to off, a system crash can result in unrecoverable data loss or non-obvious corruption. fsync = off is an appropriate setting only if you are prepared to erase and recreate all of your databases in the event of a system crash or unexpected power outage.

Note: Much of the performance benefit obtained by configuring fsync = off can also be obtained by configuring synchronous\_commit = off. However, the latter settings is far safer: in the event of a crash, the last few transactions committed might be lost if they have not yet made it to disk, but the database will not be corrupted.

|                |                                                                                                        |
|----------------|--------------------------------------------------------------------------------------------------------|
| Rule           | Check wal\_sync\_method                                                                                |
| Recommendation | On Windows, consider configuring wal\_sync\_method = fsync or wal\_sync\_method = fsync\_writethrough. |
| Trigger        | OS = Windows and wal\_sync\_method not in ('fsync', 'fsync\_writethrough')                             |
| Severity       | High                                                                                                   |

**Description:** In order to guarantee reliable crash recovery, PostgreSQL must ensure that the operating system flushes the write-ahead log to disk when asked to do so. On Windows, this can be achieved by setting wal\_sync\_method to fsync or fsync\_writethrough, or by disabling the disk cache on the drive where the write-ahead log is written. (It is safe to leave the disk cache enable if a battery-back disk cache is in use.)

Note: In cases where the loss of a very recently committed transaction is acceptable, the performance impact of flushing the write ahead log to disk can be mitigated by setting synchronous\_commit = off. In other situations, the use of a battery-backed RAID controller is recommended.

|                |                                                                            |
|----------------|----------------------------------------------------------------------------|
| Rule           | Check wal\_sync\_method                                                    |
| Recommendation | On Mac OS X, consider configuring wal\_sync\_method = fsync\_writethrough. |
| Trigger        | OS == MacOS X and wal\_sync\_method != fsync\_writethrough                 |
| Severity       | High                                                                       |

**Description:** In order to guarantee reliable crash recovery, PostgreSQL must ensure that the operating system flushes the write-ahead log to disk when asked to do so. On MacOS X, this can be achieved by setting wal\_sync\_method to fsync\_writethrough or by disabling the disk cache on the drive where the write-ahead log is written. It is safe to leave the disk cache enable if a battery-back disk cache is in use.

Note: In cases where the loss of a very recently committed transaction is acceptable, the performance impact of flushing the write ahead log to disk can be mitigated by setting synchronous\_commit = off. In other situations, the use of a battery-backed RAID controller is recommended.

|                |                                                 |
|----------------|-------------------------------------------------|
| Rule           | Check wal\_buffers                              |
| Recommendation | Consider adjusting wal\_buffers                 |
| Trigger        | wal\_buffers &lt; 1MB or wal\_buffers &gt; 16MB |
| Severity       | Medium                                          |

**Description:** Increasing the configuration parameter wal\_buffers from the default value of 64kB to 1MB or more can reduced the number of times the database must flush the write-ahead log, leading to improved performance under some workloads. There is no benefit to setting this parameter to a value greater than the size of a WAL segment (16MB).

|                |                                     |
|----------------|-------------------------------------|
| Rule           | Check commit\_delay                 |
| Recommendation | Consider setting commit\_delay = 0. |
| Trigger        | commit\_delay != 0                  |
| Severity       | Low                                 |

**Description:** Setting the commit\_delay configuration parameter to a non-zero value causes the system to wait for the specified number of microseconds before flushing the write-ahead log to disk at commit time, potentially allowing several concurrent transactions to commit with a single log flush. In most cases, this does not produce a performance benefit, and in some cases, it can produce a performance regression. Unless you have confirmed through benchmarking that a non-default value for this parameter produces a performance benefit, the default value of 0 is recommended.

|                |                                                               |
|----------------|---------------------------------------------------------------|
| Rule           | Check checkpoint\_segments                                    |
| Recommendation | Consider adjusting checkpoint\_segments.                      |
| Trigger        | checkpoint\_segments &lt; 10 or checkpoint\_segments &gt; 300 |
| Severity       | Medium                                                        |

**Description:** In order to ensure reliable and efficient crash recovery, PostgreSQL periodically writes all dirty buffers to disk. This process is called a checkpoint.Checkpoints occur when (1) the number of write-ahead log segments written since the last checkpoint exceeds checkpoint\_segments, (2) the amount of time since the last checkpoint exceeds checkpoint\_timeout, (3) the SQL command CHECKPOINT is issued, or (4) the system completes either shutdown or crash recovery. Increasing the value of checkpoint\_segments will reduce the frequency of checkpoints and will therefore improve performance, especially during bulk loading. The main downside of increasing checkpoint\_segments is that, in the event of a crash, recovery will require a longer period of time to return the database to a consistent state. In addition, increasing checkpoint\_segments will increase disk space consumption during periods of heavy system activity. However, because the theoretical limit on the amount of additional disk space that will be consumed for this reason is less than 32MB per additional checkpoint segment, this is often a small price to pay for improved performance.

Values between 30 and 100 are often suitable for modern systems. However, on smaller systems, a value as low as 10 may be appropriate, and on larger systems, a value as 300 may be useful. Values outside this range are generally not worthwhile.

|                |                                                    |
|----------------|----------------------------------------------------|
| Rule           | Check checkpoint\_completion\_target               |
| Recommendation | Consider adjusting checkpoint\_completion\_target. |
| Trigger        | checkpoint\_completion\_target != 0.9              |
| Severity       | Medium                                             |

**Description:** In order to ensure reliable and efficient crash recovery, PostgreSQL periodically writes all dirty buffers to disk. This process is called a checkpoint. Beginning in PostgreSQL 8.3, checkpoints take place over an extended period of time in order to avoid swamping the I/O system. checkpoint\_completion\_target controls the rate at which the checkpoint is performed, as a function of the time remaining before the next checkpoint is due to start. A value of 0 indicates that the checkpoint should be performed as quickly as possible, whereas a value of 1 indicates that the checkpoint should complete just as the next checkpoint is scheduled to start. It is usually beneficial to spread the checkpoint out as much as possible; however, if checkpoint\_completion\_target is set to a value greater than 0.9, unexpected delays near the end of the checkpoint process can cause the checkpoint to fail to complete before the next one needs to start. Because of this, the recommended setting is 0.9.

|                   |                                                                                                                                   |
|-------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| Rule              | Check effective\_cache\_size                                                                                                      |
| Recommendation    | Consider adjusting effective\_cache\_size.                                                                                        |
| Trigger           | effective\_cache\_size &lt; 0.5 \* system\_memory or effective\_cache\_size &gt; MAX(0.9 \* system\_memory, system\_memory - 1GB) |
| Recommended value | 0.75 \* system\_memory                                                                                                            |
| Severity          | Medium                                                                                                                            |

**Description:** When estimating the cost of a nested loop with an inner index-scan, PostgreSQL uses this parameter to estimate the chances that rows from the inner relation which are fetched multiple times will still be in cache when the second fetch occurs. Changing this parameter does not allocate any memory, but an excessively small value may discourage the planner from using indexes that would in fact speed up the query. The recommended value is 75% of system memory.

|                   |                                                                             |
|-------------------|-----------------------------------------------------------------------------|
| Rule              | Check default\_statistics\_target                                           |
| Recommendation    | Consider adjusting default\_statistics\_target.                             |
| Trigger           | default\_statistics\_target &lt; 25 or default\_statistics\_target &gt; 400 |
| Recommended value | 100                                                                         |
| Severity          | Medium                                                                      |

**Description:** PostgreSQL uses statistics to generate good query plans. These statistics are gathered either by a manual ANALYZE command or by an automatic analyze launched by the autovacuum daemon, and they include the most common values in each column of each database table, the approximate distribution of the remaining values, the fraction of rows which are NULL, and several other pieces of statistical information.

default\_statistics\_target indicates the level of detail that should be used in gathering and recording these statistics. A value of 100, which is the default beginning in PostgreSQL 8.4, is reasonable for most workloads. For very simple queries, a smaller value may be useful, while for complex queries especially against large tables, a higher value may work better. In some case, it can be helpful to override the default statistics target for specific table columns using ALTER TABLE .. ALTER COLUMN .. SET STATISTICS.

|                |                                  |
|----------------|----------------------------------|
| Rule           | Check planner methods is enabled |
| Recommendation | Avoid disabling planner methods. |
| Trigger        | any [enable]()\* GUC is off      |
| Severity       | High                             |

**Description:** The enable\_bitmapscan, enable\_hashagg, enable\_hashjoin, enable\_indexscan, enable\_material, enable\_mergejoin, enable\_nestloop, enable\_seqscan, enable\_sort, and enable\_tidscan parameters are intended primarily for debugging and should not be turned off. It can sometimes be helpful to disable one or more of these parameters for a particular query, when there is no other way to obtain the desired plan. However, none of these parameters should ever be turned off on a system-wide basis.

|                |                                          |
|----------------|------------------------------------------|
| Rule           | Check track\_counts is enabled           |
| Recommendation | Consider configuring track\_counts = on. |
| Trigger        | track\_counts = off                      |
| Severity       | High                                     |

**Description:** Autovacuum will not function properly if track\_counts is disabled. Regular vacuuming is crucial to system stability and performance.

|                |                                       |
|----------------|---------------------------------------|
| Rule           | Check autovacuum is enabled           |
| Recommendation | Consider configuring autovacuum = on. |
| Trigger        | autovacuum = off                      |
| Severity       | High                                  |

**Description:** Enabling autovacuum is an important part of maintaining system stability and performance. Although disabling autovacuum may be useful during bulk loading, it should always be promptly reenabled when bulk loading is completed. Leaving autovacuum disabled for extended periods of time will result in table and index "bloat",where available free space is not reused, resulting in uncontrolled table and index growth. Reversing such bloat requires invasive maintenance using CLUSTER, REINDEX, and/or VACUUM FULL. Allowing autovacuum to work normally is usually sufficient to avoid the need for such maintenance.

|                |                                                                |
|----------------|----------------------------------------------------------------|
| Rule           | Check configuring seq\_page\_cost                              |
| Recommendation | Consider configuring seq\_page\_cost &lt;= random\_page\_cost. |
| Trigger        | seq\_page\_cost &gt; random\_page\_cost                        |
| Severity       | Medium                                                         |

**Description:** seq\_page\_cost and random\_page\_cost are parameters used by the query parameter to determine the optimal plan for each query. seq\_page\_cost represents the cost of a sequential page read, while random\_page\_cost represents the cost of a random page read. While these costs might be equal, if, for example, the database is fully cached in RAM, the sequential cost can never be higher. The PostgreSQL query planner will produce poor plans if seq\_page\_cost is set higher than random\_page\_cost.

|                |                                                                             |
|----------------|-----------------------------------------------------------------------------|
| Rule           | Check reducing random\_page\_cost                                           |
| Recommendation | Consider reducing random\_page\_cost to no more than twice seq\_page\_cost. |
| Trigger        | random\_page\_cost &gt; 2 \* seq\_page\_cost                                |
| Severity       | Low                                                                         |

**Description:** seq\_page\_cost and random\_page\_cost are parameters used by the query parameter to determine the optimal plan for each query. seq\_page\_cost represents the cost of a sequential page read, while random\_page\_cost represents the cost of a random page read. random\_page\_cost should always be greater than or equal to seq\_page\_cost, but it is rarely beneficial to set random\_page\_cost to a value more than twice seq\_page\_cost. However, the correct values for these variables are workload-dependent. If the database's working set is much larger than physical memory and the blocks needed to execute a query will rarely be in cache, setting random\_page\_cost to a value greater than twice seq\_page\_cost may maximize performance.

|                |                                                                                                                                  |
|----------------|----------------------------------------------------------------------------------------------------------------------------------|
| Rule           | Check increasing seq\_page\_cost                                                                                                 |
| Recommendation | Consider increasing seq\_page\_cost.                                                                                             |
| Trigger        | seq\_page\_cost &lt; cpu\_tuple\_cost, seq\_page\_cost &lt; cpu\_index\_tuple\_cost, or seq\_page\_cost &lt; cpu\_operator\_cost |
| Severity       | Medium                                                                                                                           |

**Description:** The cost of reading a page into the buffer cache, even if it is already resident in the operating system buffer cache, is rarely less than the cost of a CPU operation. Thus, the value of the configuration parameter seq\_page\_cost should usually be greater than the values of the configuration parameters cpu\_tuple\_cost ,cpu\_index\_tuple\_cost, and cpu\_operator\_cost.
