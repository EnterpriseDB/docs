---
title: "Upgrading PGD clusters manually"
---

Because EDB Postgres Distributed consists of multiple software components,
the upgrade strategy depends partially on the components that are being upgraded.

In general, you can upgrade the cluster with almost zero downtime by
using an approach called *rolling upgrade*. Using this approach, nodes are upgraded one by one, and
the application connections are switched over to already upgraded nodes.

You can also stop all nodes, perform the upgrade on all nodes, and
only then restart the entire cluster. This approach is the same as with a standard PostgreSQL setup.
This strategy of upgrading all nodes at the same time avoids running with
mixed versions of software and therefore is the simplest. However, it incurs
downtime and we don't recommend it unless you can't perform the rolling upgrade
for some reason.

To upgrade an EDB Postgres Distributed cluster:

1. Plan the upgrade.
2. Prepare for the upgrade.
3. Upgrade the server software.
4. Check and validate the upgrade.

## Upgrade planning

There are broadly two ways to upgrade each node:

* Upgrade nodes in place to the newer software version. See [Rolling server
  software upgrades](#rolling-server-software-upgrades).
* Replace nodes with ones that have the newer version installed. See [Rolling
  upgrade using node join](#rolling-upgrade-using-node-join).

You can use both of these approaches in a rolling manner.

### Rolling upgrade considerations

While the cluster is going through a rolling upgrade, mixed versions of software
are running in the cluster. For example, suppose nodeA has PGD 4.3.6, while
nodeB and nodeC have 5.6.1. In this state, the replication and group
management uses the protocol and features from the oldest version (4.3.6
in this example), so any new features provided by the newer version
that require changes in the protocol are disabled. Once all nodes are
upgraded to the same version, the new features are enabled.

Similarly, when a cluster with WAL-decoder-enabled nodes is going through a
rolling upgrade, WAL decoder on a higher version of PGD node produces
[logical change records (LCRs)](../decoding_worker/#enabling) with a
higher pglogical version. WAL decoder on a lower version of PGD node produces
LCRs with a lower pglogical version. As a result, WAL senders on a higher version
of PGD nodes aren't expected to use LCRs due to a mismatch in protocol
versions. On a lower version of PGD nodes, WAL senders can continue to use LCRs.
Once all the PGD nodes are on the same PGD version, WAL senders use LCRs.

A rolling upgrade starts with a cluster with all nodes at a prior release. It
then proceeds by upgrading one node at a time to the newer release, until all
nodes are at the newer release. There must be no more than two versions of the
software running at the same time. An upgrade must be completed, with all nodes
fully upgraded, before starting another upgrade.

Where additional caution is required to reduce business risk, more time may be required to perform an upgrade.
For maximum caution and to reduce the time required upgrading production systems, we suggest performing the upgrades in a separate test environment first.

Don't run with mixed versions of the software for any longer than is absolutely necessary to complete the upgrade.
You can check on the versions in the cluster using the [`pgd nodes list --versions`](/pgd/5.8/cli/command_ref/nodes/list/) command.

The longer you run with mixed versions, the more likely you are to encounter issues, the more difficult it is to diagnose and resolve them.  
We recommend upgrading in off peak hours for your business, and over a short period of time.

While you can use a rolling upgrade for upgrading a major version of the software, we don't support mixing PostgreSQL, EDB Postgres Extended, and EDB Postgres Advanced Server in one cluster. So you can't use this approach to change the Postgres variant.

!!! Warning
    Downgrades of EDB Postgres Distributed aren't supported. They require
    that you manually rebuild the cluster.

### Rolling server software upgrades

A rolling upgrade is where the [server software
upgrade](#server-software-upgrade) is upgraded sequentially on each node in a
cluster without stopping the cluster. Each node is temporarily stopped from
participating in the cluster and its server software is upgraded. Once updated, it's
returned to the cluster, and it then catches up with the cluster's activity
during its absence.

The actual procedure depends on whether the Postgres component is being
upgraded to a new major version.

During the upgrade process, you can switch the application over to a node
that's currently not being upgraded to provide continuous availability of
the database for applications.

### Rolling upgrade using node join

The other method to upgrade the server software is to join a new node
to the cluster and later drop one of the existing nodes running
the older version of the software.

For this approach, the procedure is always the same. However, because it
includes node join, a potentially large data transfer is required.

Take care not to use features that are available only in
the newer Postgres version until all nodes are upgraded to the
newer and same release of Postgres. This is especially true for any
new DDL syntax that was added to a newer release of Postgres.

!!! Note
    `bdr_init_physical` makes a byte-by-byte copy of the source node
    so you can't use it while upgrading from one major Postgres version
    to another. In fact, currently `bdr_init_physical` requires that even the
    PGD version of the source and the joining node be exactly the same.
    You can't use it for rolling upgrades by way of joining a new node method. Instead, use a logical join.

### Upgrading a CAMO-enabled cluster

Upgrading a CAMO-enabled cluster requires upgrading CAMO groups one by one while
disabling the CAMO protection for the group being upgraded and reconfiguring it
using the new [commit scope](../commit-scopes/commit-scopes)-based settings.

We recommended the following approach for upgrading two BDR nodes that
constitute a CAMO pair to PGD 5.0:

1. Ensure `bdr.enable_camo` remains `off` for transactions on any of
  the two nodes, or redirect clients away from the two nodes. Removing
  the CAMO pairing while attempting to use CAMO leads to errors
  and prevents further transactions.
1. Uncouple the pair by deconfiguring CAMO either by resetting
  `bdr.camo_origin_for` and `bdr.camo_parter_of` (when upgrading from
  BDR 3.7.x) or by using `bdr.remove_camo_pair` (on BDR 4.x).
1. Upgrade the two nodes to PGD 5.0.
1. Create a dedicated node group for the two nodes and move them into
  that node group.
1. Create a [commit scope](../commit-scopes/commit-scopes) for this node
  group and thus the pair of nodes to use CAMO.
1. Reactivate CAMO protection again either by setting a
  `default_commit_scope` or by changing the clients to explicitly set
  `bdr.commit_scope` instead of `bdr.enable_camo` for their sessions
  or transactions.
1. If necessary, allow clients to connect to the CAMO-protected nodes
  again.

## Upgrade preparation

Each major release of the software contains several changes that might affect
compatibility with previous releases. These might affect the Postgres
configuration, deployment scripts, as well as applications using PGD. We
recommend considering these changes and making any needed adjustments in advance of the upgrade.

See individual changes mentioned in the [release notes](../rel_notes/) and any version-specific upgrade notes.

## Server software upgrade

Upgrading EDB Postgres Distributed on individual nodes happens in place.
You don't need to back up and restore when upgrading the BDR extension.

### BDR extension upgrade

The BDR extension upgrade process consists of a few steps.

#### Stop Postgres

During the upgrade of binary packages, it's usually best to stop the running
Postgres server first. Doing so ensures that mixed versions don't get loaded in case
of an unexpected restart during the upgrade.

#### Upgrade packages

The first step in the upgrade is to install the new version of the BDR packages. This installation
installs both the new binary and the extension SQL script. This step is specific to the operating system.

#### Start Postgres

Once packages are upgraded, you can start the Postgres instance. The BDR
extension is upgraded upon start when the new binaries
detect the older version of the extension.

### Postgres upgrade

The process of in-place upgrade of Postgres depends on whether you're
upgrading to a new minor version of Postgres or to a new major version of Postgres.

#### Minor version Postgres upgrade

Upgrading to a new minor version of Postgres is similar to [upgrading
the BDR extension](#bdr-extension-upgrade). Stopping Postgres, upgrading packages,
and starting Postgres again is typically all that's needed.

However, sometimes more steps, like reindexing, might be recommended for
specific minor version upgrades. Refer to the release notes of the 
version of Postgres you're upgrading to.

#### Major version Postgres upgrade

Upgrading to a new major version of Postgres is more complicated than upgrading to a minor version.

EDB Postgres Distributed provides a `pgd node upgrade` command line utility,
which you can use to do [in-place Postgres major version upgrades](inplace_upgrade).

!!! Note
    When upgrading to a new major version of any software, including Postgres, the
    BDR extension, and others, it's always important to ensure
    your application is compatible with the target version of the software you're upgrading.

## Upgrade check and validation

After you upgrade your PGD node, you can verify the current
version of the binary:

```sql
SELECT bdr.bdr_version();
```

Always check your [monitoring](../monitoring) after upgrading a node to confirm
that the upgraded node is working as expected.

## Moving from HARP or PGD Proxy to Connection Manager

PGD 4 uses HARP-proxy based routing while PGD 5 uses PGD Proxy.  In PGD 6, both are replaced with Connection Manager.

When you upgrade from PGD 4 or 5 to PGD 6, you must take steps to move to Connection Manager.

HARP and PGD Proxy can temporarily coexist with the new
[connection management](../routing) configuration. This means you can:

-  Upgrade a whole cluster to a PGD 6.1 cluster. 
-  Set up the connection routing. 
-  Replace HARP Proxy or PGD Proxy with Connection Manager. 
-  Move application connections to Connection Manager instances.
-  Remove the HARP Manager or PGD Proxy from all servers.

We strongly recommend doing this as soon as possible after upgrading nodes to
PGD 6.1.


### PGD 4 - Moving from HARP to connection manager

A PGD 4 cluster using HARP-proxy based routing continues this routing method to all nodes until the entire cluster is upgraded to 6.1.0 or higher.
HARP-proxy based routing functions the same within a mixed version cluster. HARP uses its own mechanism to elect a leader since a 4.x cluster does not have a write leader. 

There are two scenarios:

 - If Proxy runs separately from PGD nodes, connection manager can be enabled, but not used, on the upgraded nodes. Once the entire cluster is upgraded to version 6.1.0 or higher, connection manager is already enabled and ready. Applications then can be pointed to the connection manager port/node and it will start routing. Once confirmed to be working, proxy can be stopped.

 - If Proxy runs on one or more PGD nodes, connection manager can be kept disabled when restarting PGD to 6.1.  Disabling it will prevent port interference. Once the entire cluster is upgraded to version 6.1.0 or higher, use the following steps to change to connection manager routing:

   1. From one of the upgraded 6.1 nodes, run the following query to ensure that SCRAM hashes of all user passwords are the same across all nodes:

```sql
      DO $$
DECLARE
    rec RECORD;
    command TEXT;
BEGIN
    FOR rec IN SELECT rolname,rolpassword FROM pg_authid WHERE rolcanlogin = true AND rolpassword like 'SCRAM-SHA%'
    LOOP
        command := 'ALTER ROLE ' || quote_ident(rec.rolname) || ' WITH ENCRYPTED PASSWORD ' || ''' || rec.rolpassword || ''';
```

   2. Change the write leader to be same node as the leader proxy is using.
   3. Choose a PGD node that is not the write leader and fence it off with `route_fence` to *true* so it does not become the write leader.
   4. Stop the node.
   5. Enable connection manager by setting the GUC `bdr.enable_builtin_connection_manager` to *true*. 
   6. Restart the server.
   7. Stop the HARP proxy on the node.
   8. Stop the node. 
   9. Restart the node. This will ensure the connection manager is running.
    10. Unfence the node HARP as well as PGD routing.
    11. Repeat the steps for all remaining nodes.
    12. Stop proxy and let the application switch to connection manager.


### PGD 5 - Moving from PGD Proxy to Connection Manager

A PGD 5.9 cluster first needs to start using the connection manager, since 6.1 does not support PGD proxy.

1. From one of the PGD 5.9 nodes, run the following query to ensure that SCRAM hashes of all user passwords are the same across all nodes:

 -  ```sql
      DO $$
DECLARE
    rec RECORD;
    command TEXT;
BEGIN
    FOR rec IN SELECT rolname,rolpassword FROM pg_authid WHERE rolcanlogin = true AND rolpassword like 'SCRAM-SHA%'
    LOOP
        command := 'ALTER ROLE ' || quote_ident(rec.rolname) || ' WITH ENCRYPTED PASSWORD ' || ''' || rec.rolpassword || ''';
 ```
  - !!! Note
    No new users should be added to 5.9 after executing this. If they are added, run the query again. The above block does not change the passwords, it just ensure SCRAM hashes are same across the cluster on all nodes.
  !!!

    2. Fence a node in this cluster with route_fence so that it does not become the write leader.
    3. Enable the GUC `bdr.enable_builtin_connection_manager` to *true*. 
    4. Restart the server.
    5.  Stop PGD proxy running on the server.
    6. Restart the server. It will start with the connection manager running on the default port. If the proxy read and write ports were different, the connection manager port read and write ports can be changed to be the same as proxy by `bdr.alter_node_group_option()`.
    7. Unfence the node.  This node can now accept connections from the user and route to the leader via connection manager.
    8. Repeat this for each node in the cluster.  This will ensure all nodes are now routing via connection manager.
    9. For each node in the cluster, upgrade it in a rolling manner as follows:


    - Transfer leadership to another node if this node is the leader.
    - Fence it so it cannot become the leader.
    - Stop it.
    - Restart it with the 6.1 package to begin the node upgrade.

  

