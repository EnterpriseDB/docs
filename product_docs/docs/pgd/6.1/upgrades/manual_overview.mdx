---
title: "Upgrading PGD clusters manually"
---

Because EDB Postgres Distributed consists of multiple software components,
the upgrade strategy depends partially on the components that are being upgraded.

In general, you can upgrade the cluster with almost zero downtime by
using an approach called *rolling upgrade*. Using this approach, nodes are upgraded one by one, and
the application connections are switched over to already upgraded nodes.

You can also stop all nodes, perform the upgrade on all nodes, and
only then restart the entire cluster. This approach is the same as with a standard PostgreSQL setup.
This strategy of upgrading all nodes at the same time avoids running with
mixed versions of software and therefore is the simplest. However, it incurs
downtime and we don't recommend it unless you can't perform the rolling upgrade
for some reason.

To upgrade an EDB Postgres Distributed cluster:

1. Plan the upgrade.
2. Prepare for the upgrade.
3. Upgrade the server software.
4. Check and validate the upgrade.

## Upgrade planning

There are broadly two ways to upgrade each node:

* Upgrade nodes in place to the newer software version. See [Rolling server
  software upgrades](#rolling-server-software-upgrades).
* Replace nodes with ones that have the newer version installed. See [Rolling
  upgrade using node join](#rolling-upgrade-using-node-join).

You can use both of these approaches in a rolling manner.

### Rolling upgrade considerations

While the cluster is going through a rolling upgrade, mixed versions of software
are running in the cluster. For example, suppose nodeA has PGD 4.3.6, while
nodeB and nodeC have 5.6.1. In this state, the replication and group
management uses the protocol and features from the oldest version (4.3.6
in this example), so any new features provided by the newer version
that require changes in the protocol are disabled. Once all nodes are
upgraded to the same version, the new features are enabled.

Similarly, when a cluster with WAL-decoder-enabled nodes is going through a
rolling upgrade, WAL decoder on a higher version of PGD node produces
[logical change records (LCRs)](../decoding_worker/#enabling) with a
higher pglogical version. WAL decoder on a lower version of PGD node produces
LCRs with a lower pglogical version. As a result, WAL senders on a higher version
of PGD nodes aren't expected to use LCRs due to a mismatch in protocol
versions. On a lower version of PGD nodes, WAL senders can continue to use LCRs.
Once all the PGD nodes are on the same PGD version, WAL senders use LCRs.

A rolling upgrade starts with a cluster with all nodes at a prior release. It
then proceeds by upgrading one node at a time to the newer release, until all
nodes are at the newer release. There must be no more than two versions of the
software running at the same time. An upgrade must be completed, with all nodes
fully upgraded, before starting another upgrade.

Where additional caution is required to reduce business risk, more time may be required to perform an upgrade.
For maximum caution and to reduce the time required upgrading production systems, we suggest performing the upgrades in a separate test environment first.

Don't run with mixed versions of the software for any longer than is absolutely necessary to complete the upgrade.
You can check on the versions in the cluster using the [`pgd nodes list --versions`](/pgd/5.8/cli/command_ref/nodes/list/) command.

The longer you run with mixed versions, the more likely you are to encounter issues, the more difficult it is to diagnose and resolve them.  
We recommend upgrading in off peak hours for your business, and over a short period of time.

While you can use a rolling upgrade for upgrading a major version of the software, we don't support mixing PostgreSQL, EDB Postgres Extended, and EDB Postgres Advanced Server in one cluster. So you can't use this approach to change the Postgres variant.

!!! Warning
    Downgrades of EDB Postgres Distributed aren't supported. They require
    that you manually rebuild the cluster.

### Rolling server software upgrades

A rolling upgrade is where the [server software
upgrade](#server-software-upgrade) is upgraded sequentially on each node in a
cluster without stopping the cluster. Each node is temporarily stopped from
participating in the cluster and its server software is upgraded. Once updated, it's
returned to the cluster, and it then catches up with the cluster's activity
during its absence.

The actual procedure depends on whether the Postgres component is being
upgraded to a new major version.

During the upgrade process, you can switch the application over to a node
that's currently not being upgraded to provide continuous availability of
the database for applications.

### Rolling upgrade using node join

The other method to upgrade the server software is to join a new node
to the cluster and later drop one of the existing nodes running
the older version of the software.

For this approach, the procedure is always the same. However, because it
includes node join, a potentially large data transfer is required.

Take care not to use features that are available only in
the newer Postgres version until all nodes are upgraded to the
newer and same release of Postgres. This is especially true for any
new DDL syntax that was added to a newer release of Postgres.

!!! Note
    `bdr_init_physical` makes a byte-by-byte copy of the source node
    so you can't use it while upgrading from one major Postgres version
    to another. In fact, currently `bdr_init_physical` requires that even the
    PGD version of the source and the joining node be exactly the same.
    You can't use it for rolling upgrades by way of joining a new node method. Instead, use a logical join.

### Upgrading a CAMO-enabled cluster

Upgrading a CAMO-enabled cluster requires upgrading CAMO groups one by one while
disabling the CAMO protection for the group being upgraded and reconfiguring it
using the new [commit scope](../commit-scopes/commit-scopes)-based settings.

We recommended the following approach for upgrading two BDR nodes that
constitute a CAMO pair to PGD 5.0:

1. Ensure `bdr.enable_camo` remains `off` for transactions on any of
  the two nodes, or redirect clients away from the two nodes. Removing
  the CAMO pairing while attempting to use CAMO leads to errors
  and prevents further transactions.
1. Uncouple the pair by deconfiguring CAMO either by resetting
  `bdr.camo_origin_for` and `bdr.camo_parter_of` (when upgrading from
  BDR 3.7.x) or by using `bdr.remove_camo_pair` (on BDR 4.x).
1. Upgrade the two nodes to PGD 5.0.
1. Create a dedicated node group for the two nodes and move them into
  that node group.
1. Create a [commit scope](../commit-scopes/commit-scopes) for this node
  group and thus the pair of nodes to use CAMO.
1. Reactivate CAMO protection again either by setting a
  `default_commit_scope` or by changing the clients to explicitly set
  `bdr.commit_scope` instead of `bdr.enable_camo` for their sessions
  or transactions.
1. If necessary, allow clients to connect to the CAMO-protected nodes
  again.

## Upgrade preparation

Each major release of the software contains several changes that might affect
compatibility with previous releases. These might affect the Postgres
configuration, deployment scripts, as well as applications using PGD. We
recommend considering these changes and making any needed adjustments in advance of the upgrade.

See individual changes mentioned in the [release notes](../rel_notes/) and any version-specific upgrade notes.

## Server software upgrade

Upgrading EDB Postgres Distributed on individual nodes happens in place.
You don't need to back up and restore when upgrading the BDR extension.

### BDR extension upgrade

The BDR extension upgrade process consists of a few steps.

#### Stop Postgres

During the upgrade of binary packages, it's usually best to stop the running
Postgres server first. Doing so ensures that mixed versions don't get loaded in case
of an unexpected restart during the upgrade.

#### Upgrade packages

The first step in the upgrade is to install the new version of the BDR packages. This installation
installs both the new binary and the extension SQL script. This step is specific to the operating system.

#### Start Postgres

Once packages are upgraded, you can start the Postgres instance. The BDR
extension is upgraded upon start when the new binaries
detect the older version of the extension.

### Postgres upgrade

The process of in-place upgrade of Postgres depends on whether you're
upgrading to a new minor version of Postgres or to a new major version of Postgres.

#### Minor version Postgres upgrade

Upgrading to a new minor version of Postgres is similar to [upgrading
the BDR extension](#bdr-extension-upgrade). Stopping Postgres, upgrading packages,
and starting Postgres again is typically all that's needed.

However, sometimes more steps, like reindexing, might be recommended for
specific minor version upgrades. Refer to the release notes of the 
version of Postgres you're upgrading to.

#### Major version Postgres upgrade

Upgrading to a new major version of Postgres is more complicated than upgrading to a minor version.

EDB Postgres Distributed provides a `pgd node upgrade` command line utility,
which you can use to do [in-place Postgres major version upgrades](inplace_upgrade).

!!! Note
    When upgrading to a new major version of any software, including Postgres, the
    BDR extension, and others, it's always important to ensure
    your application is compatible with the target version of the software you're upgrading.

## Upgrade check and validation

After you upgrade your PGD node, you can verify the current
version of the binary:

```sql
SELECT bdr.bdr_version();
```

Always check your [monitoring](../monitoring) after upgrading a node to confirm
that the upgraded node is working as expected.



## PGD 4 - Moving from HARP Proxy to Connection Manager

A PGD 4 cluster using HARP Proxy based routing continues this routing method to all nodes until the entire cluster is upgraded to 6.1.0 or higher.
HARP-proxy based routing functions the same within a mixed version cluster. HARP uses its own mechanism to elect a leader since a 4.x cluster does not have a write leader. 

There are two scenarios:

 - If Proxy runs separately from PGD nodes, Connection Manager can be enabled, but not used, on the upgraded nodes. Once the entire cluster is upgraded to version 6.1.0 or higher, Connection Manager is already enabled and ready. Applications then can be pointed to the Connection Manager port/node and it will start routing. Once confirmed to be working, proxy can be stopped.

 - If Proxy runs on one or more PGD nodes, Connection Manager can be kept disabled when restarting PGD to 6.1.  Disabling it will prevent port interference. Once the entire cluster is upgraded to version 6.1.0 or higher, use the following steps to change to Connection Manager routing:

1. Before upgrade from PGD 4.4+ to PGD 6.1+, ensure the Connection Manager guc is set to *false* in postgresql.conf:

```bash
test-pgd6major-d1:~ $ psql bdrdb
```
```sql
bdrdb=# select * from bdr.group_versions_details ;
```
Output:
```
  node_id   |        node_name         | postgres_version |    bdr_version     
------------+--------------------------+------------------+--------------------
 2387061892 | test-pgd6major-d2 | 14.19.0          | PGD 6.1.0 Expanded
 799389892  | test-pgd6major-d1 | 14.19.0          | PGD 6.1.0 Expanded
(2 rows)
```

```bash
bdrdb=# show bdr.enable_builtin_connection_manager;
```
Output:
```
 bdr.enable_builtin_connection_manager 
---------------------------------------
 off
(1 row)
```

2. Confirm the version details of all nodes in the cluster:

```bash
enterprisedb@test-pgd6major-d1:~ $ pgd nodes list --versions
```
Output:
```
 Node Name                | BDR Version        | Postgres Version 
--------------------------+--------------------+------------------
 test-pgd6major-d1 | PGD 6.1.0 Expanded | 14.19.0          
 test-pgd6major-d2 | PGD 6.1.0 Expanded | 14.19.0     
```

3. Enable node group routing:

```sql
bdrdb=# SELECT bdr.alter_node_group_option(node_group_name := 'bdrgroup',config_key := 'enable_routing', config_value := true::TEXT);
```
Output:
```
 alter_node_group_option 
-------------------------
 
(1 row)
```

4. Check the harp-proxy leader and PGD write leader and ensure they are the same:

```bash
test-pgd6major-d1:~ $ harpctl get leader a
```
Output:
```
Cluster  Name                     Location Ready Fenced Allow Routing Routing Status Role    Type Lock Duration 
-------  ----                     -------- ----- ------ ------------- -------------- ----    ---- ------------- 
bdrgroup test-pgd6major-d2 a        true  false  true                         primary bdr  6             
```

```bash
test-pgd6major-d1:~ $pgd group bdrgroup show --summary -o json | jq -r '. | .[3] | .value'
```
Output:
```
test-pgd6major-d1
```

```sql
bdrdb=# select bdr.routing_leadership_transfer('bdrgroup','test-pgd6major-d2');
```
Output:
```
 routing_leadership_transfer 
-----------------------------
 
(1 row)
```

```bash
test-pgd6major-d1:~ $ pgd group bdrgroup show --summary -o json | jq -r '. | .[3] | .value'
```
Output:
```
test-pgd6major-d2
```

5. Fence off the non write leader node from HARP and verify it was fenced, so it does not become the leader during the middle of the upgrade: 

```bash
test-pgd6major-d1:~ $ harpctl fence test-pgd6major-d1 
```
Output:
```
2025-08-19T07:20:20.204Z	INFO	cmd/fence.go:42	fence node test-pgd6major-d1
```

```bash
test-pgd6major-d1:~ $ harpctl get nodes
```
Output:
```
Cluster  Name                     Location Ready Fenced Allow Routing Routing Status Role    Type Lock Duration 
-------  ----                     -------- ----- ------ ------------- -------------- ----    ---- ------------- 
bdrgroup test-pgd6major-d1 a        false true   true          N/A            primary bdr  6             
bdrgroup test-pgd6major-d2 a        true  false  true          ok             primary bdr  6             
```

6. Fence from PGD’s routing (`route_fence: true`) and verify, so it does not end up becoming the leader:

```bash
test-pgd6major-d1:~ $ psql bdrdb
```
```sql
bdrdb=# SELECT bdr.alter_node_option(node_name := 'test-pgd6major-d1',
          config_key := 'route_fence', config_value := true::TEXT);
```
Output:
```
 alter_node_option 
-------------------
 
(1 row)
```
```sql
bdrdb=# select node_name,route_fence from bdr.node_routing_config_summary ;
```
Output:
```
        node_name         | route_fence 
--------------------------+-------------
 test-pgd6major-d2 | f
 test-pgd6major-d1 | t
(2 rows)
```
7. Perform following steps on *test-pgd6major-d1* to upgrade it and enable connection manager when it comes up since it's disabled by default:

 - Stop the Postgres service.
 - Install PGD 6.1 packages and remove the PGD 4.4 package.
 - Stop HARP manager.
 - Enable the Connection Manager by changing the `guc bdr.enable_builtin_connection_manager` to *true* in postgresql.conf file 
 - Start the Postgres service. This performs an in-place upgrade of the PGD local node to 6.1.
 - Start the Postgres service.
 - Start harp manager.
 - Unfence d1 from harp:

```bash
test-pgd6major-d1:~ $ harpctl unfence test-pgd6major-d1
```

8. Unfence d1 from PGD’s routing:

```sql
bdrdb=# SELECT bdr.alter_node_option(node_name := 'test-pgd6major-d1', config_key := 'route_fence', config_value := false::TEXT);
```
Output:
```
 alter_node_option 
-------------------
 
(1 row)
```

9. Repeat the same steps on all nodes.  Use `bdr.group_raft_details` to confirm the cluster version is now at *6001*.
10.  From one of the upgraded 6.1 nodes, run the following query to ensure that SCRAM hashes of all user passwords are the same across all nodes:

```sql
      DO $$
DECLARE
    rec RECORD;
    command TEXT;
BEGIN
    FOR rec IN SELECT rolname,rolpassword FROM pg_authid WHERE rolcanlogin = true AND rolpassword like 'SCRAM-SHA%'
    LOOP
        command := 'ALTER ROLE ' || quote_ident(rec.rolname) || ' WITH ENCRYPTED PASSWORD ' || ''' || rec.rolpassword || ''';
```
11. Stop proxy and let the application switch to Connection Manager.  For new users added after this point, PGD 6.1 will replicate the same hashes to all nodes.
   

## PGD 5 - Moving from PGD Proxy to Connection Manager

Use the following steps to move from PGD Proxy to Connection Manager:

1. From one of the PGD 5.9 nodes, run the following query to ensure that SCRAM hashes of all user passwords are the same across all nodes:

 -  ```sql
      DO $$
DECLARE
    rec RECORD;
    command TEXT;
BEGIN
    FOR rec IN SELECT rolname,rolpassword FROM pg_authid WHERE rolcanlogin = true AND rolpassword like 'SCRAM-SHA%'
    LOOP
        command := 'ALTER ROLE ' || quote_ident(rec.rolname) || ' WITH ENCRYPTED PASSWORD ' || ''' || rec.rolpassword || ''';

        EXECUTE command;
    END LOOP;
END;
$$;
SELECT wait_slot_confirm_lsn(NULL,NULL); 
 ```
  - !!! Note
    No new users should be added to 5.9 after executing this. If they are added, run the query again. The above block does not change the passwords, it just ensure SCRAM hashes are same across the cluster on all nodes.
  !!!

    2. Fence a node in this cluster with `pgd node <node-name> set-option route_fence true` so that it does not become the write leader.
    3. Enable the GUC `bdr.enable_builtin_connection_manager` to *true*. 
    4. Restart the server.
    5. Stop PGD Proxy running on the server.
    6. Restart the server. It will start with the Connection Manager running on the default port. If the proxy read and write ports were different, the Connection Manager port read and write ports can be changed to be the same as proxy by `bdr.alter_node_group_option()`.
    7. Unfence the node.  This node can now accept connections from the user and route to the write leader via Connection Manager.
    8. Repeat this for each node in the cluster.  This will ensure all nodes are now routing via Connection Manager.
    9. For each node in the cluster, proceed with the [rolling upgrade steps](./upgrading_major_rolling/#worked-example).


  

