---
title: "EDB Postgres Distributed Proxy"
navTitle: "PGD Proxy"
indexCards: none
navigation:
- installing_proxy

directoryDefaults:
  description: "The PGD Proxy is a service that acts as proxy layer between the client application and Postgres for your EDB Postgres Distributed cluster"
---

EDB Postgres Distributed Proxy is a daemon that acts as an abstraction layer between the client application and Postgres. It interfaces with the PGD consensus mechanism to get the identity of the current write leader node and redirects traffic to that node.

The PGD cluster always has at least one global group and one data group. PGD elects the write leader for each data group that has the `enable_proxy_routing` and `enable_raft` options set to true. You can attach Proxy to a global group or data group. You can attach multiple proxies to each group.

PGD Proxy is a TCP layer 4 proxy.

## How it works

Upon starting, PGD Proxy connects to one of the endpoints given in the local config file. It fetches:

-  DB connection information for all nodes
-  Proxy options like listen address, listen port
-  Routing details like current write leader

Endpoints given in the config file are used only at startup. After that, actual endpoints are taken from the PGD catalog's `route_dsn` field in `bdr.node_routing_config_summary`.


PGD manages write leader election. PGD Proxy interacts with PGD to get write leader change events notifications on Postgres notify/listen channels and routes client traffic to the current write leader. PGD Proxy disconnects all existing client connections on write leader change or when write leader is unavailable. Write leader election is a Raft-backed activity and is subject to Raft leader availability. Proxy will close the new client connections if write leader is unavailable.

PGD Proxy responds to write leader change events that can be categorized into two modes of operation: *failover* and *switchover*.

Automatic transfer of write leadership from the current write leader node to a new node in the event of Postgres or operating system crash is called failover. PGD elects a new write leader when the current write leader goes down or becomes unresponsive. Once the new write leader is elected by PGD, proxy closes existing client connections to the old write leader and redirects new client connections to the newly elected write leader.

User-controlled, manual transfer of write leadership from the current write leader to a new target leader is called switchover. Switchover is triggered through the [PGD CLI switchover](../cli/command_ref/pgd_switchover) command. The command is submitted to PGD, which attempts to elect the given target node as the new write leader. Similar to failover, PGD Proxy closes existing client connections and redirects new client connections to the newly elected write leader. This is useful during server maintenance, for example, if the current write leader node needs to be stopped for maintenance like a server update or OS patch update.

### Consensus Grace Period

PGD Proxy provides the [consensus_grace_period](index.mdx) proxy option that can be used to configure the routing behaviour upon loss of a Raft leader. Proxy will continue to route to the current write leader(if it is available) for this duration. If the new Raft leader is not elected during this period, the proxy will stop routing. If set to `0s` proxy will stop routing immediately.

The main purpose of this option is to allow users to configure the write behaviour when Raft leader is lost. When Raft leader is not present in the cluster it is not always guaranteed that the current write leader seen by the proxy is the correct one. In some cases, like network partition, example given below, it is possible that two write leaders may be seen by two different proxies attached to the same group increasing the chances of write conflicts. If this is not the expected behavior then as mentioned above grace period can be set to 0s which configures the proxy to stop routing and close existing open connections immediately when it detects Raft leader is lost.

#### Network partition example
Consider a 3 data node group with a proxy on each data node. In this case, if the current write leader gets network partitioned or isolated then the data nodes present in the majority partition will elect a new write leader. If `consensus_grace_period` is set to a non-zero value say `10s` then proxy present on the previous write leader will continue to route writes for this duration.

Note: In this case, if the grace period is kept too high then writes will continue to happen on two write leaders increasing the chances of write conflicts.

Having said that, most of the time, upon loss of the current Raft leader the new Raft leader gets elected by PGD within a few seconds provided more than half of the nodes (quorum) are still up. Hence, if Raft leader is down but write leader is still up then proxy can be configured to allow routing by keeping `consesus_grace_period` to non-zero, positive value. Proxy will wait for the Raft leader to get elected during this period before stopping the routing. This may be helpful in some cases where availability is more important.

## Managing PGD Proxy

PGD CLI provides a few commands to manage proxies in a PGD cluster, such as `create-proxy`, `delete-proxy`, `set-proxy-options`, and `show-proxies`. See [PGD CLI](../cli/command_ref) for more information.

See [Connection management](../routing) for more information on the PGD side of configuration and management of PGD Proxy.

## Proxy log location

### syslog

- Debian based - `/var/log/syslog`
- Red Hat based - `/var/log/messages`

Use the `journalctl` command to filter and view logs for troubleshooting Proxy. The following are few sample commands for quick reference:

```sh
journalctl -u pgd-proxy -n100 -f
journalctl -u pgd-proxy --since today
journalctl -u pgd-proxy --since "10 min ago"
journalctl -u pgd-proxy --since "2022-10-20 16:21:50" --until "2022-10-20 16:21:55"
```
