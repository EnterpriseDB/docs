---
title: "Choosing your architecture"
---

Always On architectures reflect EDB’s Trusted Postgres architectures that encapsulate practices and help you to achieve the highest possible service availability in multiple configurations. These configurations range from single-location architectures to complex distributed systems that protect from hardware failures and data center failures. The architectures leverage EDB Postgres Distributed’s multi-master capability and its ability to achieve 99.999% availability, even during maintenance operations.

You can use EDB Postgres Distributed for architectures beyond the examples described here. Use-case-specific variations have been successfully deployed in production. However, these variations must undergo rigorous architecture review first. Also, EDB’s standard deployment tool for Always On architectures, TPA, must be enabled to support the variations before they can be supported in production environments.

## Standard EDB Always On architectures

EDB has identified a set of standardized architectures to support single or
multi-location deployments with varying levels of redundancy depending on your
RPO and RTO requirements.

The Always ON architecture uses 3 database node group as a basic building block
(it's possible to use 5 node group for extra redundancy as well).

EDB Postgres Distributed consists of the following major building blocks:

- Bi-Directional Replication (BDR) - a Postgres extension that creates the
  multi-master mesh network
- PGD-proxy - a connection router that makes sure the application is connected
 to the right data nodes.

All Always On architectures protect an increasing range of failure situations.
For example, a single active location with 2 data nodes protects against local
hardware failure, but does not provide protection from location failure (data
center or availability zone) failure. Extending that architecture with a backup
at a different location, ensures some protection in case of the catastrophic
loss of a location, but the database still has to be restored from backup first
which may violate recovery time objective (RTO) requirements. By adding a second
active location connected in a multi-master mesh network, ensuring that service
remains available even in case a location goes offline. Finally adding 3rd
location (this can be a witness only location) allows global Raft functionality
to work even in case of one location going offline. The global Raft is primarily
needed to run administrative commands and also some features like DDL or
sequence allocation may not work without it, while DML replication will
continue to work even in the absence of global Raft.

Each architecture can provide zero recovery point objective (RPO), as data can
be streamed synchronously to at least one local master, thus guaranteeing zero
data loss in case of local hardware failure.

Increasing the availability guarantee always drives additional cost for hardware
and licenses, networking requirements, and operational complexity. Thus it is
important to carefully consider the availability and compliance requirements
before choosing an architecture.


## Architecture details

EDB Postgres Distributed uses a [Raft](https://raft.github.io)-based consensus
architecture. While regular database operations (insert, select, delete) don’t
require cluster-wide consensus, EDB Postgres Distributed benefits from an odd
number of nodes to make decisions that require consensus, such as generating
new global sequences, or distributed DDL operations. Even the simpler
architectures always have three nodes within a location, even if not all of
them are storing data.

Applications connect to the standard Always On architectures by way of
multi-host connection strings, where each PGD-Proxy server is a distinct entry
in the multi-host connection string. Other connection mechanisms have been
successfully deployed in production, but they're not part of the standard
Always On architectures.

There should be always at least 2 PGD-Proxy nodes in each location to ensure
highly available setup for them. The PGD-Proxy can be collocated with the
database instance, in which case it's recommended to put PGD-Proxy on every
data node.

### Always On Single Location

<!-- insert the image here -->

- Redundant hardware to quickly restore when local failures
    - 3 PGD nodes could be 3 data nodes (depicted), or 2 data nodes and 1 witness
      which does not hold data
    - 2 or 3 PGD-Proxy nodes
- Barman node for backup and recovery (not depicted)
- Offsite is optional, but recommended
- Postgres Enterprise Manager (PEM) for monitoring (not depicted)

### Always On Multi-Location

<!-- insert the image here -->

- Additional replication between all nodes in Region A and Region B is not shown but occurs as part of the replication mesh
- An optional witness node could be placed in a third region to improve tolerance for location failure
- Redundant hardware to quickly restore when local failures
    - 6 PGD nodes total, 3 in each location
        - could be 3 data nodes (depicted), or 2 data nodes and 1 witness which does not hold data
    - 4 or 6 PGD-Proxy nodes total, 2 or 3 in each location
- Barman node for backup and recovery (not depicted)
- Postgres Enterprise Manager (PEM) for monitoring (not depicted)
- Application can be Active/Active in each location, or Active/Passive or Active DR with only one location taking writes

## Choosing your architecture

All architectures provide the following:
* Hardware failure protection
* Zero downtime upgrades
* Support for availability zones in public/private cloud

Use these criteria to help you to select the appropriate Always On architecture.

|                                                                  | Single active location                                                | Two active locations                                                  | Two active locations + a witness only location                        |
|------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|
| Minimum Locations Needed                                         | 1                                                                     | 2                                                                     | 3                                                                     |
| Location failure protection for data                             | No - unless offsite backup                                            | Yes                                                                   | Yes                                                                   |
| Global consensus in case of location failure                     | No                                                                    | No                                                                    | Yes                                                                   |
| Failover to DR or full DC                                        | DR only if offsite backup                                             | Full DC                                                               | Full DC                                                               |
| Fast local restoration of high availability after device failure | Yes; three local data nodes allow to maintain HA after device failure | Yes; three local data nodes allow to maintain HA after device failure | Yes; three local data nodes allow to maintain HA after device failure |
| Cross location network traffic                                   | only if offsite backup                                                | Full replication traffic                                              | Full replication traffic                                              |
| License cost                                                     | 3 data nodes                                                          | 6 data nodes                                                          | 6 data nodes                                                          |

This general progression can continue to 3 or 5 data locations by adding more
and more 3 node groups to the cluster. The addition of witness only location
is only recommended with 2 data locations.
