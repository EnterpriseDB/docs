---
title: "Deploying an EDB Postgres Distributed example cluster on Bare metal"
navTitle: "Deploying on Bare metal"
description: >
  A quick demonstration of deploying a PGD architecture using TPA on Bare metal
redirects:
  - /pgd/latest/quick_start_bare/
---

## Introducing TPA and PGD

We created TPA to make the installation and management of various Postgres configurations easily repeatable. TPA orchestrates the creation and deployment of Postgres. In this quick start, we install TPA first. If you already have TPA installed, you can skip those steps. TPA can be used, and re-used, to deploy various configurations of Postgres clusters.

PGD is a multi-master replicating implementation of Postgres designed for high performance and availability. The installation of PGD is orchestrated by TPA. We will get TPA to generate a configuration file for a PGD demonstration cluster. 

This cluster uses "bare metal" instanced to host the cluster's nodes: three replicating database nodes, three co-hosted connection proxies and one backup node. "Bare metal" for TPA is any machine which can be SSH'd into with an account with sufficient privileges. We can then use TPA to provision and deploy the required configuration and software to each node.

!!!!Note
This set of steps is specifically for Ubuntu 22.04 LTS on Intel/AMD processors.

## Prerequisites

### Configure your bare metal hosts

You will need to provision four hosts for this quick start. If you are unable to do this you may wish to consider the Docker or AWS quick starts which are easier to set up and quicker to tear down.

Each host should have an operating system installed and be SSH accessible. 

In this quick start, we are going to install PGD nodes onto 4 hosts configured in the cloud. Each one of these hosts, in our example, is installed with Rocky Linux and each has a public IP address to go with its private IP address.

| Host name   | Public IP                | Private IP     |
| ----------- | ------------------------ | -------------- |
| baremetal-1 | 172.19.16.27 | 192.168.2.247 |
| barematal-2 | 172.19.16.26 | 192.168.2.41   |
| baremetal-3 | 172.19.16.25 | 192.168.2.254 |
| baremetal-4 |172.19.16.15 | 192.168.2.30 |

These are example IP addresses and you should substitute them with your own public and private IP addresses as you progress through the quick start.

###  Set up a host admin user

Each machine requires a user account which will be used for installation. For simplicity, you should use the same named user on all the hosts. On each host, you should also configure th user so that you can ssh into the host without being prompted for a password. Ensure you have given that user sudo privileges on the host. On our four hosts, a user called "rocky" is already configured with sudo privileges.

## Preparation

### EDB account

You'll need an EDB account in order to install both TPA _and_ PGD.

[Sign up for a free EDB account](https://www.enterprisedb.com/accounts/register) if you don't already have one - this gives you a trial subscription to EDB's software repositories.

Once you have registered, or if you are already registered, go to the [EDB Repos 2.0](https://www.enterprisedb.com/repos-downloads) page where you can obtain your "repo token".

On your first visit to this page you have to select **Request Access** to generate your "repo token". Copy the token using the Copy Token icon and store it safely.

### Setting environment variables

First, set the `EDB_SUBSCRIPTION_TOKEN` environment variable to the value of your EDB repo token, obtained in the previous [EDB Account](https://www.enterprisedb.com/docs/pgd/latest/quickstart/quick_start_docker/#edb-account) step.

```
export EDB_SUBSCRIPTION_TOKEN=<your-repo-token>
```

This can be added to your `.bashrc` script (or similar shell profile) to ensure it is always set.

### Configure the repository

All the software needed for this example is available from the Postgres Distributed package repository. We download and run a script to configure the Postgres Distributed repository. This repository also contains the TPA packages.
```
curl -1sLf "https://downloads.enterprisedb.com/$EDB_SUBSCRIPTION_TOKEN/postgres_distributed/setup.deb.sh" | sudo -E bash
```
## Installing Trusted Postgres Architect (TPA)

You'll use TPA to provision and deploy PGD. If you previously installed TPA, you can move on to the [next step](#installing-pgd-using-tpa). You'll find full instructions for installing TPA in the [Trusted Postgres Architect documentation](/tpa/latest/INSTALL/), which we've also included here.

### Linux environment

[TPA supports several distributions of Linux](/tpa/latest/INSTALL/) as a host platform. These examples are written for Ubuntu 22.04, but steps are similar for other supported platforms.

### Install the TPA package

```shell
sudo apt install tpaexec
```

### Configuring TPA

You now need to configure TPA, which configures TPA's Python environment. Call `tpaexec` with the command `setup`:

```shell
sudo /opt/EDB/TPA/bin/tpaexec setup
export PATH=$PATH:/opt/EDB/TPA/bin
```

You can add the `export` command to your shell's profile.

### Testing the TPA installation

You can verify TPA is correctly installed by running `selftest`:

```shell
tpaexec selftest
```
TPA is now installed.

## Installing PGD using TPA

### Generating a configuration file

Run the [`tpaexec configure`](/tpa/latest/tpaexec-configure/) command to generate a configuration folder: 

```
tpaexec configure democluster --architecture PGD-Always-ON --platform bare --edb-postgres-advanced 15 --redwood --no-git --location-names dc1 --active-location dc1 --hostnames-unsorted
```

You specify the PGD-Always-ON architecture (`--architecture PGD-Always-ON`), which sets up the configuration for [PGD 5's Always On architectures](https://www.enterprisedb.com/docs/pgd/latest/architectures/). As part of the default architecture, it configures your cluster with three data nodes, co-hosting three [PGD Proxy](https://www.enterprisedb.com/docs/pgd/latest/routing/proxy/) servers, along with a [Barman](https://www.enterprisedb.com/docs/pgd/latest/backup/#physical-backup) node for backup.

Specify that you're using a "bare metal" platform (`--platform bare`). TPA will determine which operating system is running on each host during deployment.  See [the EDB Postgres Distributed compatibility table](https://www.enterprisedb.com/resources/platform-compatibility) for details of which operating systems are supported.

Specify that the data nodes will be running [EDB Postgres Advanced Server v15](https://www.enterprisedb.com/docs/epas/latest/) (`--edb-postgres-advanced 15`) with Oracle compatibility (`--redwood`).

You set the notional location of the nodes to `dc1` using `--location-names`. You then activate the PGD proxies in that location using `--active-locations dc1` set to the same location.

By default, TPA commits configuration changes to a Git repository. For this example, you don't need to do that, so pass the `--no-git` flag.

Finally, you ask TPA to generate repeatable hostnames for the nodes by passing `--hostnames-unsorted`. Otherwise, it selects hostnames at random from a predefined list of suitable words.

This command creates a subdirectory in the current working directory called `democluster`. It contains the `config.yml` configuration file TPA uses to create the cluster. You can view it using:

```shell
less democluster/config.yml
```

You will now need to edit the configuration file to add details related to your bare metal hosts, such as admin user names and public and private IP addresses.

## Editing your configuration

Using your preferred editor, open `democluster/config.yml`.

Search for the line containing `ansible_user: root`. Change `root` to the name of the user name you configured  with ssh access and sudo privileges. Follow that with the line:

```yaml
    manage_ssh_hostkeys: yes
```

Your `instance_defaults` section should now look like this:

```yaml
instance_defaults:
  platform: bare
  vars:
    ansible_user: rocky
    manage_ssh_hostkeys: yes
```
Next, you should search for `node: 1`. You will find this within the configuration settings of the first node, named "kaboom".  

After the `node: 1` line we want to add the public and private IP addresses of our node. We'll use **baremetal-1** as the host for this node. You should add, substituting your IP addresses, add

```yaml
  public_ip: 172.19.16.27
  private_ip: 192.168.2.247
```

To the file, aligning the start of each line with the start of the `node:` line. When you are done, the whole entry for "kaboom" should look like this - with your IP addresses in it:

```yaml
- Name: kaboom
  backup: kapok
  location: dc1
  node: 1
  public_ip: 172.19.16.27
  private_ip: 192.168.2.247 
  role:
  - bdr
  - pgd-proxy
  vars:
    bdr_child_group: dc1_subgroup
    bdr_node_options:
      route_priority: 100
```
You then need to repeat this process for the three other nodes.

Search for `node: 2`

This is the configuration settings for a node named "kaftan". We will use `baremetal-2` for this node and, substituting your IP addresses, add:

```yaml
  public_ip: 172.19.16.26
  private_ip: 192.168.2.41
```

Search for `node: 3`

This is the configuration settings for a node named "kaolin". We will use `baremetal-3` for this node and, substituting your IP addresses, add:

```yaml
  public_ip: 172.19.16.25
  private_ip: 192.168.2.254
```

Finally, search for `node: 4`

This is the configuration settings for a node named "kapok". We will use `baremetal-4` for this node and, substituting your IP addresses, add:

```yaml
  public_ip: 172.19.16.15
  private_ip: 192.168.2.30
```

## Provisioning the cluster

You can now run:

```
tpaexec provision democluster
```

This command performs preparatory work for deploying the cluster. On other platforms such as Docker and AWS, this would also create the required hosts, but as you are on a bare metal platform, you should already have the hosts configured.

!!! SeeAlso "Further reading"
    - [`tpaexec provision`](/tpa/latest/tpaexec-provision/) in the Trusted Postgres Architect documentation


One part of this process for bare hosts is creating key-pairs for the hosts for SSH operations later on. With those key-pairs created, you will need to copy the public part of the key-pair to the hosts. You can do this with `ssh-copy-id`, giving the democluster identity (`-i`) and the login to each host. For our example, this would be these commands:

```shell
ssh-copy-id -i democluster/id_democluster rocky@172.19.16.27
ssh-copy-id -i democluster/id_democluster rocky@172.19.16.26
ssh-copy-id -i democluster/id_democluster rocky@172.19.16.25
ssh-copy-id -i democluster/id_democluster rocky@172.19.16.15
```


You can now create `tpa_known_hosts` file which will allow the hosts to be verified. Use `ssh-keyscan` on each host (`-H`) and append its output to the `tpa_known_hosts`

```shell
ssh-keyscan -H 172.19.16.27 >> democluster/tpa_known_hosts
ssh-keyscan -H 172.19.16.26 >> democluster/tpa_known_hosts
ssh-keyscan -H 172.19.16.25 >> democluster/tpa_known_hosts
ssh-keyscan -H 172.19.16.15 >> democluster/tpa_known_hosts
```

## Deploy your cluster

You now have everything ready to deploy your cluster. To deploy run:

```shell
tpaexec deploy democluster
```

TPA applies the configuration, installing the needed packages and setting up the actual EDB Postgres Distributed cluster.

!!! SeeAlso "Further reading"
    - [`tpaexec deploy`](/tpa/latest/tpaexec-deploy/) in the Trusted Postgres Architect documentation

## Connecting to the cluster

You're now ready to log into one of the nodes of the cluster with SSH and then connect to the database. Part of the configuration process set up SSH logins for all the nodes, complete with keys. To use the SSH configuration, you need to be in the `democluster` directory created by the `tpaexec configure` command earlier:

```shell
cd democluster
```

From there, you can run `ssh -F ssh_config <hostname>` to establish an SSH connection. You will connect to kaboom, the first database node in the cluster:

```shell
ssh -F ssh_config kaboom
__OUTPUT__
[rocky@kaboom ~]# 
```

Notice that you're logged in as `rocky` ,the admin user and ansible user you configured earlier, on `kaboom`.

You now need to adopt the identity of the enterprisedb user. This user is preconfigured and authorized to connect to the cluster's nodes.

```shell
sudo -iu enterprisedb
__OUTPUT__
[root@kaboom ~]# sudo -iu enterprisedb
enterprisedb@kaboom:~ $
```

You can now run the `psql` command to access the `bdrdb` database:

```shell
psql bdrdb
__OUTPUT__
enterprisedb@kaboom:~ $ psql bdrdb
psql (15.2.0, server 15.2.0)
Type "help" for help.

bdrdb=#
```

You're directly connected to the Postgres database running on the `kaboom` node and can start issuing SQL commands.

To leave the SQL client, enter `exit`.

### Using PGD CLI

The pgd utility, also known as the PGD CLI, lets you control and manage your Postgres Distributed cluster. It's already installed on the node. 

You can use it to check the cluster's health by running `pgd check-health`:

```console
enterprisedb@kaboom:~ $ pgd check-health
Check      Status Message
-----      ------ -------
ClockSkew  Ok     All BDR node pairs have clockskew within permissible limit
Connection Ok     All BDR nodes are accessible
Raft       Ok     Raft Consensus is working correctly
Replslots  Ok     All BDR replication slots are working correctly
Version    Ok     All nodes are running same BDR versions
enterprisedb@kaboom:~ $
```

Or, you can use `pgd show-nodes` to ask PGD to show you the data-bearing nodes in the cluster:

```console
enterprisedb@kaboom:~ $ pgd show-nodes
Node   Node ID    Group        Type Current State Target State Status Seq ID
----   -------    -----        ---- ------------- ------------ ------ ------
kaboom 2710197610 dc1_subgroup data ACTIVE        ACTIVE       Up     1
kaftan 3490219809 dc1_subgroup data ACTIVE        ACTIVE       Up     3
kaolin 2111777360 dc1_subgroup data ACTIVE        ACTIVE       Up     2
enterprisedb@kaboom:~ $
```

Similarly, use `pgd show-proxies` to display the proxy connection nodes:

```console
enterprisedb@kaboom:~ $ pgd show-proxies
Proxy  Group        Listen Addresses Listen Port
-----  -----        ---------------- -----------
kaboom dc1_subgroup [0.0.0.0]        6432
kaftan dc1_subgroup [0.0.0.0]        6432
kaolin dc1_subgroup [0.0.0.0]        6432
```

The proxies provide high-availability connections to the cluster of data nodes for applications. You can connect to the proxies and, in turn, to the database with the command `psql -h kaboom,kaftan,kaolin -p 6432 bdrdb`:

```console
enterprisedb@kaboom:~ $ psql -h kaboom,kaftan,kaolin -p 6432 bdrdb
psql (15.2.0, server 15.2.0)
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)
Type "help" for help.

bdrdb=#
```


## Explore your cluster

* [Explore failover](further_explore_failover) with hands-on exercises
* [Understand conflicts](further_explore_conflicts) by creating and monitoring them
* [Next steps](next_steps) in working with your cluster