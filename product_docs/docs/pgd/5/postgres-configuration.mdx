---
navTitle: Postgres configuration
title: Postgres configuration
redirects:
  - ../bdr/configuration
---

Several Postgres configuration parameters affect PGD nodes. You can set these
parameters differently on each node, although that isn't generally recommended.

For PGD's own settings see the [PGD Settings reference](reference/pgd-settings).

## Postgres settings

PGD requires these Postgres settings to run correctly:

-   `wal_level` &mdash; Must be set to `logical`, since PGD relies on logical decoding.
-   `shared_preload_libraries` &mdash; Must contain `bdr`, although it can contain
     other entries before or after, as needed. However, don't include `pglogical`.
-   `track_commit_timestamp` &mdash; Must be set to `on` for conflict resolution to
    retrieve the timestamp for each conflicting row.

PGD requires these PostgreSQL settings to be set to appropriate values,
which vary according to the size and scale of the cluster.

-   `logical_decoding_work_mem` &mdash; Memory buffer size used by logical decoding.
    Transactions larger than this overflow the buffer and are stored
    temporarily on local disk. Default is 64 MB, but you can set it much higher.
-   `max_worker_processes` &mdash; PGD uses background workers for replication
    and maintenance tasks, so you need enough worker slots for it to
    work correctly. The formula for the correct minimal number of workers, for each database, is:
    -  One per PostgreSQL instance plus
    -  One per database on that instance plus
    -  Four per PGD-enabled database plus
    -  One per peer node in the PGD group plus
    -  One for each writer-enabled per peer node in the PGD group
    You might need more worker processes temporarily when a node is being
    removed from a PGD group.
-   `max_wal_senders` &mdash; Two needed per every peer node.
-   `max_replication_slots` &mdash; Same as `max_wal_senders`.
-   `wal_sender_timeout` and `wal_receiver_timeout` &mdash; Determines how
    quickly a node considers its CAMO partner as disconnected or
    reconnected. See [CAMO failure scenarios](durability/camo/#failure-scenarios) for
    details.

In normal running for a group with N peer nodes, PGD requires
N slots and WAL senders. During synchronization, PGD temporarily uses another
N - 1 slots and WAL senders, so be careful to set the parameters high enough
for this occasional peak demand.

With parallel apply turned on, the number of slots must be increased to
N slots from the formula \* writers. This is because the `max_replication_slots`
also sets the maximum number of replication origins, and some of the functionality
of parallel apply uses extra origin per writer.

When the [decoding worker](nodes#decoding-worker) is enabled, this
process requires one extra replication slot per PGD group.

Changing these parameters requires restarting the local node:
`max_worker_processes`, `max_wal_senders`, `max_replication_slots`.

A legacy synchronous replication mode is supported via the use of the following
parameters. See [Durability and performance options](durability) for details and
limitations.

-   `synchronous_commit` &mdash; Affects the durability and performance of PGD replication.
    in a similar way to [physical replication](https://www.postgresql.org/docs/11/runtime-config-wal.html#GUC-SYNCHRONOUS-COMMIT).
-   `synchronous_standby_names` &mdash; Same as above.

## Time-based snapshots

### `snapshot_timestamp`

Turns on the use of [timestamp-based snapshots](tssnapshots) and sets the
timestamp to use.

## Max prepared transactions

### `max_prepared_transactions`

Needs to be set high enough to cope with the maximum number of concurrent prepared transactions across the cluster due to explicit two-phase commits, CAMO, or Eager transactions. Exceeding the limit prevents a node from running a local two-phase commit or CAMO transaction and prevents all Eager transactions on the cluster. You can set this only at Postgres server start.
