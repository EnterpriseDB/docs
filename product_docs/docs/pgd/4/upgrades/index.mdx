---
title: "Upgrading"
---

Because EDB Distributed Postgres consist in multiple software components,
the upgrade strategy depends somewhat on which components are being upgraded.

In general it's possible to upgrade the cluster with almost zero upgrade, by
using approach called Rolling Upgrade where nodes are upgraded one by one, and
the application connections are switched over to already upgraded nodes.

Ii's also possible to stop all nodes, perform the upgrade on all nodes and
only then restart the entire cluster, just like with standard PostgreSQL setup.
This strategy of upgrading all nodes at the same time avoids running with
mixed versions of software and therefore is the simplest, but obviously incurs
some downtime and is not recommended unless the Rolling Upgrade is not possible
for some reason.

To upgrade EDB Distributed Postgres cluster, the following steps need to be
performed:

* plan the upgrade
* prepare for the upgrade
* upgrade the server software
* check and validate the upgrade

## Upgrade Planning

There are broadly two ways to upgrade each node.

* Upgrading nodes in-place to the newer software version, this is referred to
  as Rolling Server Software Upgrades bellow.

* Replacing nodes with ones that have newer version installed, this is referred
  to as Rolling Upgrade Using Node Join bellow.

Both of these approaches can be done in rolling manner.

### Rolling Upgrade considerations

While the cluster is going through a rolling upgrade, mixed versions of software
are running in the cluster. For example, nodeA will have BDR 3.7.16, while
nodeB and nodeC will have 4.1.0. In this state, the replication and group
management will use the protocol and features from the oldest version (3.7.16
in case of this example), so any new features provided by the newer version
which require changes in the protocol will be disabled. Once all nodes are
upgraded to the same version, the new features are automatically enabled.

A rolling upgrade starts with a cluster with all nodes at a prior release,
then proceeds by upgrading one node at a time to the newer release, until
all nodes are at the newer release. There should never be more than 2 versions
of any component running at the same time which means the new upgrade must not
be initiated until the previous upgrade process has fully finished on all nodes.

An upgrade process may take an extended period of time when the user decides
caution is required to reduce business risk, though it's not recommended
to run the mixed versions of the software indefinitely.

While Rolling Upgrade can be used for upgrading major version of the software
it is not supported to mix PostgreSQL, EDB Postgres Extended and
EDB Postgres Advanced in one cluster at the moment, so this approach cannot
be used to change the Postgres variant.

!!! Warning
    Downgrades of the EDB Distributed Postgres are *not* supported and require
    manual rebuild of the cluster.

### Rolling Server Software Upgrades

A rolling upgrade is the process where the below [Server
Software Upgrade](#Server-Software-Upgrade) is performed on each node in the
cluster one after another, while keeping the remainder of the cluster
operational.

The actual procedure here depends on whether the Postgres component is being
upgraded to new major version or not.

During the upgrade process, the application can be switched over to a node
which is currently not being upgraded to provide continuous availability of
the database for applications.

### Rolling Upgrade Using Node Join

The other method of upgrade of the Server Software, is to join a new node
to the cluster and later drop one of the existing nodes running
the older version of the software.

For this approach, the procedure is always the same, however because it
includes node join, the potentially large data transfer is required.

Care must be taken to not use features that are available only in
the newer Postgres versions, until all nodes are upgraded to the
newer and same release of Postgres. This is especially true for any
new DDL syntax that may have been added to newer release of Postgres.

!!! Note
    Note that `bdr_init_physical` makes a byte-by-byte of the source node.
    So it cannot be used while upgrading from one major Postgres version
    to another. In fact, currently `bdr_init_physical` requires that even
    BDR version of the source and the joining node is exactly the same. So
    it cannot be used for rolling upgrades via joining a new node method. In
    all such cases, a logical join must be used.

### Upgrading a CAMO-Enabled Cluster

CAMO protection requires at least one of the nodes of a CAMO pair to
be operational. For upgrades, we recommend to ensure that no CAMO
protected transactions are running concurrent to the upgrade, or to
use a rolling upgrade strategy, giving the nodes enough time to
reconcile in between the upgrades and the corresponding node downtime
due to the upgrade.

## Upgrade Preparation

Each major release of the software contains several changes that may affect
compatibility with previous releases. These may affect the Postgres
configuration, deployment scripts as well as applications using BDR. We
recommend to consider and possibly adjust in advance of the upgrade.

Please see individual changes mentioned in release notes and the version
specific upgrade notes at the end of this chapter.

## Server Software Upgrade

The upgrade of EDB Distributed Postgres on individual nodes happens in-place.
There is no need for backup and restore when upgrading the BDR extension.

### BDR Extension Upgrade

BDR extension upgrade process consists of few simple steps.

#### Stop Postgres

During the upgrade of binary packages, it's usually best to stop the running
Postgres server first to ensure that mixed versions don't get loaded in case
of unexpected restart during the upgrade.

#### Upgrade Packages

The first step in the upgrade is to install the new version of the BDR packages, which
will install both the new binary and the extension SQL script. This step depends
on the operating system used

#### Start Postgres

Once packages are upgrade the Postgres instance can be started, the BDR
extension will automatically be upgraded upon start when the new binaries
detect older version of the extension.

### Postgres Upgrade

The process of in-place upgrade of Postgres highly depends on whether you are
upgrading to new minor version of Postgres of to new major version of Postgres.

#### Minor Version Postgres Upgrade

Upgrading to a new minor version of Postgres is pretty much same as upgrading
the BDR extension as described above. Stopping Postgres, upgrading packages
and starting Postgres again is normally all that's needed.

However, sometimes additional steps like reindexing may be recommended for
specific minor version upgrade. Please refer to the Release Notes of the
specific version of Postgres you are upgrading to.

##### Major Version Postgres Upgrade

Upgrading to new major version of Postgres is more complicated process.

EDB Postgres Distributed provides command-line utility called `bdr_pg_upgrade`
which can be used to do [In-place Postgres Major Version Upgrades](bdr_pg_upgrade).

!!! Note
    When upgrading to new major version of any software, including Postgres, the
    BDR extension and others, it's always important to ensure the compatibility
    of your application with the target version of a given software.

## Upgrade Check and Validation

After this procedure, your BDR node is upgraded. You can verify the current
version of BDR4 binary like this:

```sql
SELECT bdr.bdr_version();
```

Always check the [monitoring](monitoring) after upgrade of a node to confirm
that the upgraded node is working as expected.

## Application Schema Upgrades

Similar to the upgrade of BDR itself, there are two approaches to
upgrading the application schema.  The simpler option is to stop all
applications affected, preform the schema upgrade and restart the
application upgraded to use the new schema variant.  Again, this
imposes some downtime.

To eliminate this downtime, BDR offers ways to perform a rolling
application schema upgrade as documented in the following section.

### Rolling Application Schema Upgrades

By default, DDL will automatically be sent to all nodes. This can be
controlled manually, as described in
[DDL Replication](https://www.enterprisedb.com/docs/bdr/ddl), which
could be used to create differences between database schemas across nodes.
BDR is designed to allow replication to continue even while minor
differences exist between nodes. These features are designed to allow
application schema migration without downtime, or to allow logical
standby nodes for reporting or testing.

!!! Warning
    Rolling Application Schema Upgrades have to be managed, not by BDR.
    Careful scripting will be required to make this work correctly
    on production clusters. Extensive testing is advised.

Details of this are covered here
[Replicating between nodes with differences](https://www.enterprisedb.com/docs/bdr/latest/appusage).

When one node runs DDL that adds a new table, nodes that have not
yet received the latest DDL will need to cope with the extra table.
In view of this, the appropriate setting for rolling schema upgrades
is to configure all nodes to apply the `skip` resolver in case of a
`target_table_missing` conflict. This must be performed before any
node has additional tables added, and is intended to be a permanent
setting.

This is done with the following query, that must be **executed
separately on each node**, after replacing `node1` with the actual
node name:

```sql
SELECT bdr.alter_node_set_conflict_resolver('node1',
		'target_table_missing', 'skip');
```

When one node runs DDL that adds a column to a table, nodes that have not
yet received the latest DDL will need to cope with the extra columns.
In view of this, the appropriate setting for rolling schema
upgrades is to configure all nodes to apply the `ignore` resolver in
case of a `target_column_missing` conflict. This must be performed
before one node has additional columns added and is intended to be a
permanent setting.

This is done with the following query, that must be **executed
separately on each node**, after replacing `node1` with the actual
node name:

```sql
SELECT bdr.alter_node_set_conflict_resolver('node1',
		'target_column_missing', 'ignore');
```

When one node runs DDL that removes a column from a table, nodes that
have not yet received the latest DDL will need to cope with the missing column.
This situation will cause a `source_column_missing` conflict, which uses
the `use_default_value` resolver. Thus, columns that neither
accept NULLs nor have a DEFAULT value will require a two step process:

1. Remove NOT NULL constraint or add a DEFAULT value for a column
   on all nodes.
2. Remove the column.

Constraints can be removed in a rolling manner.
There is currently no supported way for coping with adding table
constraints in a rolling manner, one node at a time.

When one node runs a DDL that changes the type of an existing column,
depending on the existence of binary coercibility between the current
type and the target type, the operation may not rewrite the underlying
table data. In that case, it will be only a metadata update of the
underlying column type. Rewrite of a table is normally restricted.
However, in controlled DBA environments, it is possible to change
the type of a column to an automatically castable one by adopting
a rolling upgrade for the type of this column in a non-replicated
environment on all the nodes, one by one. More details are provided in the
[ALTER TABLE](https://www.enterprisedb.com/docs/bdr/latest/ddl) section.
