---
title: EDB Postgres Distributed 4.3.6 release notes
navTitle: Version 4.3.6
---


Released: 15 October 2024


EDB Postgres Distributed 4.3.6 is a minor release that includes several bug fixes and enhancements.


## Highlights
- HARP proxy settings patches to reduced bloat on consensus request and response journals.
- Fix for accidental drop of autopartition rule when a column of the autopartitioned table is dropped.


## Enhancements

<table class="table"><thead><tr><th>Component</th><th>Version</th><th>Release Note</th><th>Addresses</th></tr></thead><tbody>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Add <code>bdr.bdr_show_all_file_settings()</code> and <code>bdr.bdr_file_settings</code> view</summary><hr/><p>Fix: Correct privileges for bdr_superuser.
Creating wrapper SECURITY DEFINER functions in the bdr schema
and granting access to bdr_superuser to use those: </p>
<ul>
<li><code>bdr.bdr_show_all_file_settings</code></li>
<li><code>bdr.bdr_file_settings</code></li>
</ul></details></td><td></td></tr>
</tbody></table>


## Bug Fixes

<table class="table"><thead><tr><th>Component</th><th>Version</th><th>Release Note</th><th>Addresses</th></tr></thead><tbody>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>bdr&nbsp;consensus: Add GUCs to control automatic Raft vacuum</summary><hr/><p>Aggressive HARP proxy settings may face issues due to
increased bloat on consensus request and response journals.
This patch adds GUCs to control frequency of automatic vacuum on said
catalogs. The <code>bdr.raft_vacuum_interval</code> GUC controls how frequently the
tables are checked for VACUUM and ANALYZE. The Autovacuum GUCs and table
reloptions are used to determine whether VACUUM/ANALYZE is needed or
not.
The <code>bdr.raft_vacuum_full_interval</code> triggers VACUUM FULL on the tables.
The user can disable VACUUM FULL if normal VACUUM is sufficient to keep
the bloat in control.</p></details></td><td>40412</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Do not accidentally drop the autopartition rule when a column of the autopartitioned table is dropped.</summary><hr/><p>When ALTER TABLE .. DROP COLUMN is used, the object_access_hook is fired with <code>classId</code> set to RelationRelationId, but the <code>subId</code> is set to the attribute number to differentiate it from the DROP TABLE command. Therefore, we need to check the subId field to make sure that we are not performing actions that should only be triggered when a table is dropped.</p></details></td><td>40258</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Cleanup unwanted data with drop_node</summary><hr/><p>When we drop the local node we cleanup all the catalog tables internally.</p></details></td><td></td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Delete older logs if we get a snapshot that's already applied</summary><hr/><p>The follower doesn't need the older logs once a snapshot is applied. If
we keep those around, the duplicate request id issue may keep happening.
So clear the logs a bit more aggressively once we know they are not
needed.</p></details></td><td></td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Handling duplicate requests in RAFT preventing protocol breakage</summary><hr/><p>When processing RAFT entries, it's crucial to handle duplicate requests properly to prevent Raft protocol issues. Duplicate requests can occur when a client retries a request that has already been accepted and applied by the Raft leader. The problem arose when the leader failed to detect the duplicate request due to historical evidence being pruned.</p></details></td><td>37725</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Handling Raft Snapshots: Consensus Log</summary><hr/><p>When installing or importing a Raft snapshot, discard the consensus log unless it contains an entry matching the snapshot's last included entry and term.</p></details></td><td>37725</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Allow ANALYZE on temp tables</summary><hr/><p>The ANALYZE statement is already not replicated. We dont need to filter
if it's analyzing temp tables or not, we allow the user to analyze anything.</p></details></td><td>38826</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Re-add function <code>bdr.column_timestamps_enable_internal()</code></summary><hr/><p>This function was removed as the code can be merged with
<code>bdr.column_timestamps_disable()</code>. However, although that function is
deprecated, in a mixed-version cluster it could theoretically be called on an
older node, resulting in <code>bdr.column_timestamps_enable_internal()</code> being
replicated and causing an error on the nodes where it is no longer present.</p></details></td><td></td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Fixed buffer overrun in the writer</summary><hr/><p>Include an extra zero byte at the end of a column value allocation in shared memory queue insert/update/delete messages.</p></details></td><td>98966</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Fix duplicate ids generation for snowflakeid</summary><hr/><p>Disallow multiple backends which concurrently increment the sequences 
to simultaneously initialize the sequence for same millisecond timestamp value,
resulting in duplicate ids both with local seq id component as 0.</p></details></td><td></td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Adjust <code>bdr.alter_table_conflict_detection()</code> to propagate correctly to all nodes</summary><hr/><p>Ensure that the propagation of <code>bdr.alter_table_conflict_detection()</code> (as well as the related, deprecated <code>bdr.column_timestamps_(en|dis)able()</code> functions) is carried out correctly to all logical standbys. Previously, this propagation did not occur if the logical standby was not directly attached to the node on which the functions were executed.</p></details></td><td>40258</td></tr>
<tr><td>BDR</td><td>4.3.6</td><td><details><summary>Raft-related catalog items</summary><hr/><p>This adds compatibility objects for:</p>
<ul>
<li><code>bdr.raft_instances</code></li>
<li><code>bdr.get_raft_status(node_group_name text)</code></li>
</ul>
<p>These additions ensure that Raft status queries operate correctly
in clusters running multiple major versions.</p></details></td><td></td></tr>
</tbody></table>


