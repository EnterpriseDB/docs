---
title: "Components and architectures"
---

EDB Postgres Distributed provides loosely-coupled multi-master logical replication
using a mesh topology. This means that you can write to any server and the
changes will be sent directly, row-by-row to all the
other servers that are part of the same mesh.

![node diagram](../images/nodes.png)

By default EDB Postgres Distributed uses asynchronous replication, applying changes on
the peer nodes only after the local commit. Additional levels of synchronicity can
be configured between different nodes, groups of nodes or all nodes by configuring
[CAMO](/bdr/latest/camo), [group commit](/bdr/latest/group-commit) or
[eager all node replication](/bdr/latest/eager) features.

EDB Postgres Distributed consists of several components that make the whole
cluster work.

## Postgres server

Three different Postgres distributions can be used:

- [PostgreSQL](https://www.postgresql.org/) - open source
- [EDB Postgres Extended](https://techsupport.enterprisedb.com/customer_portal/sw/2ndqpostgres/) - PostgreSQL compatible and optimized for replication
- [EDB Postgres Advanced Server](/epas/latest) - Oracle compatible, optimized for replication, and additional enterprise features

What Postgres distribution and version is right for you depends on the features you need
please refer to the feature matrix for detailed comparison.

## Postgres Extensions

### BDR

A Postgres server with the BDR extension installed will be referred to as a BDR
node. BDR nodes be either data nodes or witness nodes.

Witness nodes don't participate in data replication, and are only used as a
tie-breaker for consensus.

### pglogical3 (3.6/3.7)

pglogical is a Postgres extension that provides logical replication using
logical decoding since Postges 9.4.

Older versions of BDR (3.6/3.7) depend on pglogical3 to provide the replication
channel upon which BDR builds.

## HARP

HARP is connection management tool for BDR cluster.

It leverages consensus-driven quorum to determine the correct connection end-point
in a semi-exclusive manner to prevent unintended multi-node writes from an
application. This reduces the potential for data conflicts.

## Basic Architecture

### Multiple Groups

A BDR node is a member of at least one **Node Group**, and in the most
basic architecture there is a single node group for the whole BDR
cluster.

### Multiple Masters

Each node (database) participating in a BDR group both receives
changes from other members and can be written to directly by the user.

This is distinct from Hot or Warm Standby, where only one primary
server accepts writes, and all the other nodes are standbys that
replicate either from the primary or from another standby.

You don't have to write to all the masters, all of the time; it's
a frequent configuration to direct writes mostly to just one master.

### Asynchronous, by default

Changes made on one BDR node are not replicated to other nodes until
they are committed locally. As a result the data is not exactly the
same on all nodes at any given time; some nodes will have data that
has not yet arrived at other nodes. PostgreSQL's block-based replication
solutions default to asynchronous replication as well. In BDR,
because there are multiple masters and as a result multiple data streams,
data on different nodes might differ even when
`synchronous_commit` and `synchronous_standby_names` are used.

Additional levels of synchronicity can
be configured between different nodes, groups of nodes or all nodes by configuring
[CAMO](/bdr/latest/camo), [group commit](/bdr/latest/group-commit) or
[eager all node replication](/bdr/latest/eager) features.

### Mesh Topology

BDR is structured around a mesh network where every node connects to every
other node and all nodes exchange data directly with each other. There is no
forwarding of data within BDR except in special circumstances such as node
addition and node removal. Data may arrive from outside the BDR cluster or
be sent onwards using native PostgreSQL logical replication.

### Logical Replication

Logical replication is a method of replicating data rows and their changes,
based upon their replication identity (usually a primary key).
We use the term *logical* in contrast to *physical* replication, which uses
exact block addresses and byte-by-byte replication. Index changes are not
replicated, thereby avoiding write amplification and reducing bandwidth.

Logical replication starts by copying a snapshot of the data from the
source node. Once that is done, later commits are sent to other nodes as
they occur in real time. Changes are replicated without re-executing SQL,
so the exact data written is replicated quickly and accurately.

Nodes apply data in the order in which commits were made on the source node,
ensuring transactional consistency is guaranteed for the changes from
any single node. Changes from different nodes are applied independently of
other nodes to ensure the rapid replication of changes.

Replicated data is sent in binary form, when it is safe to do so.

### High Availability

Each master node can be protected by one or more standby nodes, so any node
that goes down can be quickly replaced and continue. Each standby node can
be either a logical or a physical standby node.

Replication continues between currently connected nodes even if one or more
nodes are currently unavailable. When the node recovers, replication
can restart from where it left off without missing any changes.

Nodes can run different release levels, negotiating the required protocols
to communicate. As a result, BDR clusters can use rolling upgrades, even
for major versions of database software.

DDL is automatically replicated across nodes by default. DDL execution can
be user controlled to allow rolling application upgrades, if desired.

### Limits

BDR can run hundreds of nodes on good enough hardware and network, however
for mesh based deployments it's generally not recommended to run more than
32 nodes in one cluster.
Each master node can be protected by multiple physical or logical standby nodes;
there is no specific limit on the number of standby nodes,
but typical usage would be to have 2-3 standbys per master. Standby nodes don't
add additional connections to the mesh network so they are not included in the
32 node recommendation.

BDR currently has hard limit of no more than 1000 active nodes. This is both
the current limit for Raft connections allowed and a limitation of nodes
that the distributed sequence algorithm can support.

BDR places a limit that at most 10 databases in any one PostgreSQL instance
can be BDR nodes across different BDR node groups. However BDR works best if
only one BDR database per PostgreSQL instance is used.

The minimum recommended number of nodes in BDR cluster is 3, because with
2 nodes the consensus stops working if one of the node stops working. One
of the three nodes can however be a witness node.

## Architectural Options & Performance

### Characterising BDR performance

BDR can be configured in a number of different architectures, each of which has
different performance and scalability characteristics.

The Group is the basic building block of a BDR Group consisting of 2+ nodes
(servers). Within a Group, each node is in a different AZ, with dedicated router
and backup, giving Immediate Switchover and High Availability. Each Group has a
dedicated Replication Set defined on it. If the Group loses a node it is easily
repaired/replaced by copying an existing node from the Group.

Adding more master nodes to a BDR Group does not result in significant write
throughput increase when most tables are replicated because BDR has to replay
all the writes on all nodes. Because BDR writes are in general more effective
than writes coming from Postgres clients via SQL, some performance increase
can be achieved. Read throughput generally scales linearly with the number of
nodes.

The following architectures are available:

-   Multimaster/Single Group
-   BDR AlwaysOn

The simplest architecture is just to have one Group, so let's examine that first:

### BDR MultiMaster within one Group

By default, BDR will keep one copy of each table on each node in the Group and any
changes will be propagated to all nodes in the Group.

Since copies of data are everywhere, SELECTs need only ever access the local node.
On a read-only cluster, performance on any one node will not be affected by the
number of nodes. Thus adding nodes will increase linearly the total possible SELECT
throughput.

INSERTs, UPDATEs and DELETEs (DML) are performed locally, then the changes will
be propagated to all nodes in the Group. The overhead of DML apply is less than the
original execution, so if you run a pure write workload on multiple nodes
concurrently, a multi-node cluster will be able to handle more TPS than a single node.

Conflict handling has a cost that will act to reduce the throughput. The throughput
is then dependent upon how much contention the application displays in practice.
Applications with very low contention will perform better than a single node;
applications with high contention could perform worse than a single node.
These results are consistent with any multi-master technology, they are not a facet
or peculiarity of BDR.

Eager replication can avoid conflicts, but is inherently more expensive.

Changes are sent concurrently to all nodes so that the replication lag is minimised.
Adding more nodes means using more CPU for replication, so peak TPS will reduce
slightly as each new node is added.

If the workload tries to uses all CPU resources then this will resource constrain
replication, which could then affect the replication lag.

### BDR AlwaysOn

The AlwaysOn architecture is built from 2 Groups, in 2 separate regions. Each Group
provides HA and IS, but together they also provide Disaster Recovery (DR), so we refer
to this architecture as AlwaysOn with Very High Availability.

Tables are created across both Groups, so any change goes to all nodes, not just to
nodes in the local Group.

One node is the target for the main application. All other nodes are described as
shadow nodes (or "read-write replica"), waiting to take over when needed. If a node
loses contact we switch immediately to a shadow node to continue processing. If a
Group fails, we can switch to the other Group. Scalability is not the goal of this
architecture.

Since we write mainly to only one node, the possibility of contention between is
reduced to almost zero and as a result performance impact is much reduced.

CAMO is eager replication within the local Group, lazy with regard to other Groups.

Secondary applications may execute against the shadow nodes, though these should be
reduced or interrupted if the main application begins using that node.

Future feature: One node is elected as main replicator to other Groups, limiting CPU
overhead of replication as the cluster grows and minimizing the bandwidth to other Groups.

## Deployment

BDR is intended to be deployed in one of a small number of known-good configurations,
using either TPAexec or a configuration management approach
and deployment architecture approved by Technical Support.

Manual deployment is not recommended and may not be supported.

Please refer to the `TPAexec Architecture User Manual` for your architecture.

Log messages and documentation are currently available only in English.

## The Importance of Quorum

The central purpose of HARP is to enforce full Quorum on any Postgres cluster
it manages. Quorum is merely a term generally applied to a voting body that
mandates a certain minimum of attendees are available to make a decision. Or
perhaps even more simply: Majority Rules.

In order for any vote to end in a result other than a tie, an odd number of
nodes must constitute the full cluster membership. Quorum however does not
strictly demand this restriction; a simple majority will suffice. This means
that in a cluster of N nodes, Quorum requires a minimum of N/2+1 nodes to hold
a meaningful vote.

All of this ensures the cluster is always in agreement regarding which node
should be "in charge". For a BDR cluster consisting of multiple nodes, this
determines which node is the primary write target. HARP designates this node
as the Lead Master.

## Reducing Write Targets

The consequence of ignoring the concept of Quorum, or applying it
insufficiently, may lead to a Split Brain scenario where the "correct" write
target is ambiguous or unknowable. In a standard Postgres cluster, it is
important that only a single node is ever writable and sending replication
traffic to the remaining nodes.

Even in Multi-Master capable approaches such as BDR, it can be beneficial to
reduce the amount of necessary conflict management to derive identical data
across the cluster. In clusters that consist of multiple BDR nodes per physical
location or region, this usually means a single BDR node acts as a "Leader" and
remaining nodes are "Shadows". These Shadow nodes are still writable, but doing
so is discouraged unless absolutely necessary.

By leveraging Quorum, it's possible for all nodes to agree exactly which
Postgres node should represent the entire cluster, or a local BDR region. Any
nodes that lose contact with the remainder of the Quorum, or are overruled by
it, by definition cannot become the cluster Leader.

This prevents Split Brain situations where writes unintentionally reach two
Postgres nodes. Unlike technologies such as VPNs, Proxies, load balancers, or
DNS, a Quorum-derived consensus cannot be circumvented by mis-configuration or
network partitions. So long as it's possible to contact the Consensus layer to
determine the state of the Quorum maintained by HARP, only one target is ever
valid.

## Basic Architecture

The design of HARP comes in essentially two parts consisting of a Manager and
a Proxy. The following diagram describes how these interact with a single
Postgres instance:

![HARP Unit](images/ha-unit.png)

The Consensus Layer is an external entity where Harp Manager maintains
information it learns about its assigned Postgres node, and HARP Proxy
translates this information to a valid Postgres node target. Because Proxy
obtains the node target from the Consensus Layer, several such instances may
exist independently.

While using BDR itself as the Consensus Layer, each server node resembles this
variant instead.

![HARP Unit w/BDR Consensus](images/ha-unit-bdr.png)

In either case, each unit consists of the following elements:

* A Postgres or EDB instance
* A Consensus Layer resource, meant to track various attributes of the Postgres
  instance
* A HARP Manager process to convey the state of the Postgres node to the
  Consensus Layer
* A HARP Proxy service that directs traffic to the proper Lead Master node,
  as derived from the Consensus Layer

Not every application stack has access to additional node resources
specifically for the Proxy component, so it can be combined with the
application server to simplify the stack itself.

This is a typical design using two BDR nodes in a single Data Center organized in a Lead Master / Shadow Master configuration:

![HARP Cluster](images/ha-ao.png)

Note that when using BDR itself as the HARP Consensus Layer, at least three
fully qualified BDR nodes must be present to ensure a quorum majority.

![HARP Cluster w/BDR Consensus](images/ha-ao-bdr.png)

(Not shown in the above diagram are connections between BDR nodes.)

## How it Works

When managing a BDR cluster, HARP maintains at most one "Leader" node per
defined Location. Canonically this is referred to as the Lead Master. Other BDR
nodes which are eligible to take this position are Shadow Master state until
such a time they take the Leader role.

Applications may contact the current Leader only through the Proxy service.
Since the Consensus Layer requires Quorum agreement before conveying Leader
state, any and all Proxy services will direct traffic to that node.

At a high level, this is ultimately what prevents application interaction with
multiple nodes simultaneously.

### Determining a Leader

As an example, consider the role of Lead Master within a locally subdivided
BDR Always-On group as may exist within a single data center. When any
Postgres or Manager resource is started, and after a configurable refresh
interval, the following must occur:

1. The Manager checks the status of its assigned Postgres resource.
    - If Postgres is not running, try again after configurable timeout.
    - If Postgres is running, continue.
2. The Manager checks the status of the Leader lease in the Consensus Layer.
    - If the lease is unclaimed, acquire it and assign the identity of
      the Postgres instance assigned to this Manager. This lease duration is
      configurable, but setting it too low may result in unexpected leadership
      transitions.
    - If the lease is already claimed by us, renew the lease TTL.
    - Otherwise do nothing.

Obviously a lot more happens here, but this simplified version should explain
what's happening. The Leader lease can only be held by one node, and if it's
held elsewhere, HARP Manager gives up and tries again later.

!!! Note
    Depending on the chosen Consensus Layer, rather than repeatedly looping to
    check the status of the Leader lease, HARP will subscribe to notifications
    instead. In this case, it can respond immediately any time the state of the
    lease changes, rather than polling. Currently this functionality is
    restricted to the etcd Consensus Layer.

This means HARP itself does not hold elections or manage Quorum; this is
delegated to the Consensus Layer. The act of obtaining the lease must be
acknowledged by a Quorum of the Consensus Layer, so if the request succeeds,
that node leads the cluster in that Location.

### Connection Routing

Once the role of the Lead Master is established, connections are handled
with a similar deterministic result as reflected by HARP Proxy. Consider a case
where HARP Proxy needs to determine the connection target for a particular backend
resource:

1. HARP Proxy interrogates the Consensus layer for the current Lead Master in
   its configured location.
2. If this is unset or in transition;
    - New client connections to Postgres are barred, but clients will
      accumulate and be in a paused state until a Lead Master appears.
    - Existing client connections are allowed to complete current transaction,
      and are then reverted to a similar pending state as new connections.
3. Client connections are forwarded to the Lead Master.

Note that the interplay demonstrated in this case does not require any
interaction with either HARP Manager or Postgres. The Consensus Layer itself
is the source of all truth from the Proxy's perspective.

### Colocation

The arrangement of the work units is such that their organization is required
to follow these principles:

1. The Manager and Postgres units must exist concomitantly within the same
   node.
2. The contents of the Consensus Layer dictate the prescriptive role of all
   operational work units.

This delegates cluster Quorum responsibilities to the Consensus Layer itself,
while HARP leverages it for critical role assignments and key/value storage.
Neither storage or retrieval will succeed if the Consensus Layer is inoperable
or unreachable, thus preventing rogue Postgres nodes from accepting
connections.

As a result, the Consensus Layer should generally exist outside of HARP or HARP
managed nodes for maximum safety. Our reference diagrams reflect this in order
to encourage such separation, though it is not required.

!!! Note
    In order to operate and manage cluster state, BDR contains its own
    implementation of the Raft Consensus model. HARP may be configured to
    leverage this same layer to reduce reliance on external dependencies and
    to preserve server resources. However, there are certain drawbacks to this
    approach that are discussed in further depth in the section on the
    [Consensus Layer](09_consensus-layer).

## Recommended Architecture and Use

HARP was primarily designed to represent a BDR Always-On architecture which
resides within two (or more) Data Centers and consists of at least five BDR
nodes. This does not count any Logical Standby nodes.

The current and standard representation of this can be seen in the following
diagram:

![BDR Always-On Reference Architecture](images/bdr-ao-spec.png)

In this diagram, HARP Manager would exist on BDR Nodes 1-4. The initial state
of the cluster would be that BDR Node 1 is the Lead master of DC A, and BDR
Node 3 is the Lead Master of DC B.

This would result in any HARP Proxy resource in DC A connecting to BDR Node 1,
and likewise the HARP Proxy resource in DC B connecting to BDR Node 3.

!!! Note
    While this diagram only shows a single HARP Proxy per DC, this is merely
    illustrative and should not be considered a Single Point of Failure. Any
    number of HARP Proxy nodes may exist, and they will all direct application
    traffic to the same node.

### Location Configuration

In order for multiple BDR nodes to be eligible to take the Lead Master lock in
a location, a Location must be defined within the `config.yml` configuration
file.

To reproduce the diagram above, we would have these lines in the `config.yml`
configuration for BDR Nodes 1 and 2:

```yaml
location: dca
```

And for BDR Nodes 3 and 4:

```yaml
location: dcb
```

This applies to any HARP Proxy nodes which are designated in those respective
data centers as well.

### BDR 3.7 Compatibility

BDR 3.7 and above offers more direct Location definition by assigning a
Location to the BDR node itself. This is done by calling the following SQL
API function while connected to the BDR node. So for BDR Nodes 1 and 2, we
might do this:

```sql
SELECT bdr.set_node_location('dca');
```

And for BDR Nodes 3 and 4:

```sql
SELECT bdr.set_node_location('dcb');
```

Afterwards, future versions of HARP Manager would derive the `location` field
directly from BDR itself. This HARP functionality is not available yet, so we
recommend using this and the setting in `config.yml` until HARP reports
compatibility with this BDR API method.


