---
title: Distributed high availability
navTitle: Distributed high availability
description: Distributed high-availability clusters are powered by EDB Postgres Distributed and are designed to provide high availability across multiple data groups.
---

Distributed high-availability clusters are powered by [EDB Postgres Distributed](https://enterprisedb.com/docs/pgd/latest/). They use multi-master logical replication to deliver more advanced cluster management than a physical replication-based system. Distributed high-availability clusters allow you to deploy a cluster across multiple regions or a single region. For use cases where high availability across regions is a requirement, a cluster deployment with distributed high availability can provide up to two data groups in (2) distinct regions, along with a witness group in a third separate region.

This configuration provides a true active-active solution as each data group is configured to accept write operations. 

Distributed high-availability clusters support both EDB Postgres Advanced Server and EDB Postgres Extended Server database distributions.

Distributed high-availability clusters contain one or two data groups. Your data groups can contain either three data nodes or two data nodes and one witness node. At any given time, one of these data nodes in each group is the leader and accepts writes, while the rest are referred to as [shadow nodes](https://enterprisedb.com/docs/pgd/latest/terminology/#write-leader). We recommend that you don't use two data nodes and one witness node in production unless you use asynchronous [commit scopes](https://enterprisedb.com/docs/pgd/latest/reference/commit-scopes/commit-scopes/). 

[PGD Connection Manager](https://enterprisedb.com/docs/pgd/latest/reference/connection-manager/) can route all application traffic to the write leader node. Each group has its own write leader, reducing distributed data conflicts. PGD leverages a distributed consensus model to determine availability of the data nodes in the cluster. On failure or unavailability of the leader, PGD elects a new write leader and redirects application traffic. Together with the core capabilities of EDB Postgres Distributed, this mechanism of routing application traffic to the leader node enables fast failover and switchover.

The witness node/witness group doesn't host data but exists for management purposes. It supports operations that require a consensus, for example, in case of a region failure.

!!!note
    Operations against a distributed high-availability cluster leverage the [EDB Postgres Distributed set-leader](https://enterprisedb.com/docs/pgd/latest/cli/command_ref/group/set-leader) feature, which provides subsecond interruptions during planned lifecycle operations.

## Single data location

A configuration with a single data location has one data group and either:

-   Two data nodes with one lead, one shadow, and a witness node, each in separate availability zones

    <div style="width: 75%; margin: auto;">

    ![region(2 data + 1 witness)](../../images/architectures/dhaaha-1x2-with-witness.svg)

    </div>
    
-   Three data nodes with one lead and two shadow nodes, each in separate availability zones

    <div style="width: 75%; margin: auto;">

    ![region(3 data)](../../images/architectures/ahadha-1x3-cluster.svg)

    </div>

## Multiple groups with a witness node

A configuration with multiple data locations has two data groups that contain either three data nodes in each group:

- A data node and two shadow nodes in one group

- The same configuration in another group

- A witness node in a third group

![groups(2 x 3 data + 1 witness)](../../images/architectures/dha-3x3-cluster.svg)

Groups will currently be in the same region (the region the cluster resides in), but this may change in the future.

### Advisory locks

Advisory locks aren't replicated between Postgres nodes, so advisory locks taken on a shadow node don't conflict with advisory locks taken on the leader. We recommend that applications that rely on advisory locking avoid using read-only workloads for those transactions.

## For more information

For instructions on creating, retrieving information from, and managing a distributed high-availability cluster using the HM CLI, see [Using the HM CLI](../../using_hybrid_manager/edbctl/).
