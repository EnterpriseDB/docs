---
title: Controlling schema ingestion
navTitle: Schema ingestion
deepToC: true
---

The [`beacon-agent.yaml` monitoring template](/edb-postgres-ai/current/hybrid-manager/using_hybrid_manager/monitoring/mon_ext_dbs/mon_with_agent/configure_agent/) allows the Agent to ingest all schemas that the migration user has permissions to and contain at least one table.

You can add further configurations to control schema ingestion.

## Defining a subset of schemas for ingestion

 If you don't want the Agent to ingest all schemas, you can define a subset with the `databases.schema.filter` option.

### Using the schema filter

To control which schemas the Agent ingests from your database, add the `filter` parameter within the `databases.schema` configuration block. This allows you to specify a set of schemas to either include or exclude.

The filter parameter requires two settings:

- `mode`: Defines the filtering behavior. It must be set to either `include` or `exclude`. When set to `include`, the Agent only ingests schemas listed in the names list. When set to `exclude`, the Agent ingests all schemas except those listed in the names list.

- `names`: A list of schema names to apply the filter to. Because the names are case-sensitive, ensure you use the same casing for the names as how the schemas are stored in the source database.

!!!note 
    The Agent has a default [list of system schemas](#excluded-system-schemas) for both Oracle and Postgres that are automatically excluded from ingestion because they are irrelevant for the database migration. When an `exclude` filter is used, this default list is combined with any additional schemas you specify. For the `include` filter, the Agent will ingest any schemas you explicitly name, including system schemas that would have otherwise been excluded by default.

### Include example

```yaml
[...]
provider:
  onprem:
    clusters:
      - resource_id: "<cluster_resource_id>"
        [...]
        nodes:
          - resource_id: "<node_resource_id>"
            dsn: $DSN
            schema:
              enabled: true
              poll_interval: 15s 
              filter:
                mode: "include"
                names: 
                  - "<schema_name1>"
                  - "<schema_name2>"
            tags:
              - "<tag_names>"
```

### Exclude example

```yaml
[...]
provider:
  onprem:
    clusters:
      - resource_id: "<cluster_resource_id>"
       [..]
       nodes:
         - resource_id: "<node_resource_id>"
           dsn: $DSN
           schema:
             enabled: true
             poll_interval: 15s 
             filter:
               mode: "exclude"
               names: 
                 - "<schema_name1>"
                 - "<schema_name2>"
           tags:
             - "<tag_names>"
```

### Excluded system schemas

The Agent excludes the following system schemas by default.

**Postgres and EDB Postgres Extended Server**

- information_schema
- pg_catalog
- pg_toast

**EDB Postgres Advanced Server**

- sys

**Oracle** 

- ANONYMOUS
- APEX_PUBLIC_USER
- APEX_030200
- APEX_040000
- APEX_040200
- APPQOSSYS
- AUDSYS 
- CTXSYS 
- DMSYS
- DBSNMP
- DBSFWUSER
- DEMO
- DIP
- DVF
- DVSYS
- EXFSYS
- FLOWS_FILES
- FLOWS_020100 
- FRANCK
- GGSYS
- GSMADMIN_INTERNAL
- GSMCATUSER
- GSMROOTUSER
- GSMUSER
- LBACSYS
- MDDATA
- MDSYS 
- MGMT_VIEW
- OJVMSYS
- OLAPSYS
- ORDPLUGINS
- ORDSYS
- ORDDATA
- OUTLN
- ORACLE_OCM
- OWBSYS
- OWBYSS_AUDIT 
- PDBADMIN
- RMAN
- REMOTE_SCHEDULER_AGENT
- SI_INFORMTN_SCHEMA
- SPATIAL_CSW_ADMIN_USR
- SPATIAL_WFS_ADMIN_USR 
- SQLTXADMIN
- SQLTXPLAIN
- SYS
- SYSBACKUP
- SYSDG
- SYSKM
- SYSRAC
- SYSTEM
- SYSMAN
- TSMSYS
- WKPROXY 
- WKSYS
- WK_TEST
- WMSYS
- XDB
- XS$NULL

## Configuring the agent to support schema ingestion for several databases

When you configure the HM agent to ingest schemas from several Oracle databases, you can use the `schema_export_max_workers` parameter of the `beacon-agent.yaml` configuration file to control how many schema export jobs the HM agent runs in parallel.

The `schema_export_max_workers` sets the maximum number of concurrent schema exports the HM agent runs across all configured Oracle databases. Each active worker consumes CPU, memory, and some disk space on the auxiliary machine that runs HM agent, and generates load on the corresponding source database.

```yml
... # other beacon-agent.yaml configurations
provider:
      onprem:
        runner:
          enabled: true
        schema_export_max_workers: 10
...
```

Start with the default `schema_export_max_workers: 10` when configuring initial schema ingestion. Then, monitor the HM agent host or auxiliary machine (CPU, memory, disk I/O, network) using your standard system tools or the Grafana dashboards included with the HM console.

If the host is underutilized and you need faster ingestion for multiple databases, increase the value in small steps and observe the impact.

If resource usage is consistently high or causing contention, decrease the value and confirm that the system stabilizes.

### Increasing the number of parallel workers

The default `beacon-agent.yaml` configuration can handle schema ingestion for multiple databases, but if you are performing a bulk migration and want the HM agent to ingest the schemas of more than 10 source databases, we recommend increasing the number of parallel workers. You may have also already started the agent and notice that the databases are taking too long to appear in the **Estate > Migrations** tab.

If the auxiliary machine where you are running the agent has available resources (CPU, memory, and network), you can increase the number of parallel workers used during ingestion to improve performance. You can use Grafana monitoring to look at the current CPU and memory utilization on the HM agent host during schema ingestion.

```yaml
provider:
  onprem:
    schema_export_max_workers: 20
    # ...
```

After each change, restart the Agent if it is not running as a service with automatic reload.

### Decreasing the number of parallel workers

If the auxiliary machine where you are running the agent is using up too many resources or you notice spikes in CPU, memory, and disk usage, you can decrease the number of parallel workers used during ingestion to improve performance. This slows down total ingestion time but reduces the peak resource usage on both the HM agent host and the source databases. You can use Grafana monitoring to look at the current CPU and memory utilization on the HM agent host during schema ingestion.

```yaml
provider:
  onprem:
    schema_export_max_workers: 4
    # ...
```
