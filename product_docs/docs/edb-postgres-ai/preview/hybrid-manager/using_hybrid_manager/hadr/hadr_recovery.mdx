---
title: Hybrid Manager recovery
navTitle: HA/DR recovery
description: Learn about high availability and disaster recovery (HA/DR) with Hybrid Manager.
---


The DR procedure is defined as the series of manual steps that you need to take from the deployment of a new appliance to the moment that it’s possible to restore your Postgres clusters using the normal restore procedure.

!!! Warning
    The procedure is based on the 1.0 release of the appliance and is subject to constant change as the feature set changes. You must constantly test and update it for it to remain valid.

### 1. Confirm availability of backups

The first step ensures the backups of the unavailable appliance (aka “old backups”) are reachable from the new appliance.

You can achieve this in multiple ways:

-   Using a replicated bucket as the s3-compatible linked bucket for the new appliance, so the old backups are directly available to the new appliance.
-   Copying the backups of the damaged appliance to the linked storage of the new appliance. You must copy the following items:  
    -   Internal EDB backups folder, with the format `edb-internal-backups/\<random-string\>`  
    -   The Postgres clusters backups folder `customer-pg-backups`  
    -   Any folder corresponding to a defined custom storage location

!!! Note
    The internal backups folder defined for the new appliance will be different from the older one, as it will have a different `\<random-string\>`.

### 2. Preparation steps

#### Define a recovery backup storage location for Velero

Once you have backups available, you can define a new storage location for Velero so you can restore resources from the damaged appliance backups. This is a read-only location to prevent overwriting or removing those backups.

To define a new storage location, use the following Kubernetes manifest:

```yaml
apiVersion: velero.io/v1  
kind: BackupStorageLocation  
metadata:  
  annotations:  
    appliance.enterprisedb.com/s3-prefixes: edb-internal-backups/\<old-backups-ramdom-string\>/velero  
  labels:  
    appliance.enterprisedb.com/s3-credentials: bound  
  name: recovery  
  namespace: velero  
spec:  
  accessMode: ReadOnly  
  config:  
    insecureSkipTLSVerify: "false"  
    region: \<region-of-attached-bucket\>  
    s3ForcePathStyle: "true"  
  default: false  
  objectStorage:  
    bucket: \<linked-bucket-name\>  
    prefix: edb-internal-backups/\<old-backups-random-string\>/velero  
  provider: aws
```

Confirm it using the `velero get backup-locations` command. It must show as `Available`.  If the status is not `Available`, check the Velero pod logs for permission errors on the s3 bucket.

#### Choosing a Velero backup for recovery

Once the old internal Velero backups are available in the recovery storage location, you can list them with the following command:

```bash
velero get backups \--selector velero.io/storage-location=recovery
```

Typically, you choose the latest available completed backup to recover from. Note the Velero backup name, as well as the date and time (UTC), as both are required for a restore.

Example:

```bash
NAME                                      STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR  
velero-backup-kube-state-**20241216154403**   Completed   0        0          2024-12-16 16:44:03 \+0100 CET   5d        recovery           \<none\>
```

!!! Note
    The timestamp value is referred to as the *recovery date* in the instructions that follow.

#### Additional requirements

The following requirements apply to the recovery procedure:

-   The new appliance must be running the same version of the Postgres AI software deployment as the old one.
-   The same locations (`locations.beacon.enterprisedb.com` custom resource) used in the old appliance are available in the new one. `Locations` is currently an internal resource created during install and isn't available in the console. `managed-devspatcher` is the default value.
-   Container images used to build the clusters in the old appliance are available to the new one.

### 3. Recovery steps

#### Restore EDB internal databases (`app-db` and `beacon-db`)

Once the old backups are available, you can restore the EDB internal databases. For each internal database:

1.  Save the cluster manifest to a yaml file: `kubectl get \<cluster-name\> \-o yaml \>\<cluster-name\>.yaml`.  
2.  Edit the cluster spec in the yaml file so the cluster is created from the backups:  

-   Replace the **init** section in bootstrap with a **recovery** section:  
    ```yaml
    recovery:  
      database: \<database name as in the init section\>  
      owner: \<owner name as in the init section\>  
      source: \<pg-cluster-name\>  
      secret:  
        name: \<secret name as in the init section\>  
      recoveryTarget:  
        targetTime: "\<recovery date in YYYY-MM-DD HH:MM:SS+00 format\>"  
    ```
-   Add the following section:  
    ```yaml
    externalClusters:  
    \- barmanObjectStore:  
        destinationPath:  S3://\<linked-bucket-name\>/edb-internal-backups/\<old-backups-random-string\>/databases  
        s3Credentials:  
          inheritFromIAMRole: true  
        wal:  
          maxParallel: 8  
      name: \<pg-cluster-name\>  
    ```
-   Add the following prefix to the `appliance.enterprisedb.com/s3-prefixes` annotation of the `inheritedMetdata` section (the list is comma separated): 
    ```yaml
     edb-internal-backups/\<old-backups-random-string\>/databases/\<db-name\>  
    ```
3.  Delete the cluster:

    ```bash
    kubectl delete cluster \<cluster-name\>)  
    ```

4.  Clean the backup area for the cluster:

    ```bash
    aws s3 rm s3://\<linked-bucket-name\>/edb-internal-backups/\<new-backups-random-string\>/databases/\<pg-cluster-name\>  \--recursive
    ```

5.  Apply the yaml file for the cluster to be re-created: `kubectl apply \-f \<cluster-name\>.yaml`
6.  After the cluster is successfully restored and in a healthy state, restart the `accm-server`  in the namespace `upm-beaco-ff-base`. 

At this point, the portal on the new cluster is available again.

#### Configure the Velero plugin

The plugin helps restore the Kubernetes resources in a correct state, so only the custom managed storage locations are restored. The Postgres clusters resources are restored as deleted, so you can later restore data as desired.

The plugin configuration is made through a `ConfigMap`, so you must apply this manifest:

```yaml
apiVersion: v1  
kind: ConfigMap  
metadata:  
  name: velero-plugin-for-edbpgai  
  namespace: velero  
  labels:  
    velero.io/plugin-config: ""  
    enterprisedb.io/edbpgai-plugin: RestoreItemAction  
data:  
  \# configure disaster recovery mode, so restored items are transformed as needed  
  drMode: "true"  
  \# configure a date corresponding to the velero backup date. Note the format\!  
  drDate: "\<recovery date in YYYY–MM-DDTHH:MM:SSZ format\>”  
  \# old and new buckets for internal custom storage locations  
  oldBucket: \<old-appliance-bucket-name\>  
  newBucket: \<new-appliance-bucket-name\>
```

#### Restore the custom managed storage locations

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-1-storagelocations  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- storagelocations.biganimal.enterprisedb.com  
  includeClusterResources: true  
  labelSelector:  
    matchLabels:  
      biganimal.enterprisedb.io/reserved-by-biganimal: "false"
```

#### Restore the cluster wrappers

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-2-clusterwrappers  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- clusterwrappers.beacon.enterprisedb.com  
  restoreStatus:  
    includedResources:  
    \- clusterwrappers.beacon.enterprisedb.com
```

#### Restore the backup wrappers

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-3-backupwrappers  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- backupwrappers.beacon.enterprisedb.com  
  restoreStatus:  
    includedResources:  
    \- backupwrappers.beacon.enterprisedb.com
```
