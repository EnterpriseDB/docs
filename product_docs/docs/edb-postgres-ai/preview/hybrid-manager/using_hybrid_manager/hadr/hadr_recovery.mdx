---
title: Hybrid Manager disaster recovery
navTitle: Disaster recovery
description: Learn about high availability and disaster recovery (HA/DR) with Hybrid Manager.
---

The DR procedure is defined as the series of manual steps that you need to take from the deployment of a new HM instance to the manual recovery of your Postgres clusters using the restore procedure.

!!! Warning
    You must constantly test and update your DR procedure for it to remain valid.

## Before you start

Before starting the DR procedure, ensure you have made yourself familiar with:

- [Planning and best practices](./hadr)
- [Considerations and backup requirements](./considerations)

## Prerequisites 

- A new HM instance deployed and running. It must be running the same version as the old one.

  - If you have configured [multi-DC](/edb-postgres-ai/preview/hybrid-manager/using_hybrid_manager/multi-dc/) in your old HM instance, ensure the same locations (`locations.beacon.enterprisedb.com` custom resource) are available in the new one. `locations` is currently an internal resource created during install and isn't available in the HM console. `managed-devspatcher` is the default value.

  - The container images used to build the clusters in the old HM instance are available to the new one.

- [Install the Velero CLI](https://velero.io/docs/v1.3.0/velero-install/).

## 1. Make backups available in the new HM instance

The first step ensures the backups of the unavailable HM instance (“old backups”) are reachable from the new HM instance by copying the backups of the damaged HM instance to the linked storage of the new HM instance. 

1.  Obtain the name of the old and new buckets, the ID of the old backup, and the region of the new attached bucket and store these parameters in environment variables:

    ```bash
    export OLD_BUCKET=<old_bucket>
    export OLD_BACKUP_ID=<old_environment_internal_backup_id>
    export NEW_BUCKET=<new_bucket>
    export NEW_REGION=<region_of_the_new_bucket>
    ```

    <details><summary>How do I obtain the old bucket values?</summary> 

    To obtain the old bucket values: 
    
    1. Go to the console/dashboard of your CSP > buckets. 
    1. Find and select the bucket linked to the backups of your old HM instance.
    1. Browse through to the `edb-internal-backups` folder. Inside that folder you will find a subfolder with the backup ID, e.g. 4be7a1c8c9f0.

    </details> 

    <br/>

    <details><summary>EKS Example</summary> 

    This is an example for setting the environment variables for an HM instance deployed on EKS:
    
    ```bash
    export OLD_BUCKET=eks-1105143903-2511-edb-postgres
    export OLD_BACKUP_ID=a7462dbc7106
    export NEW_BUCKET=eks-1105155418-2511-edb-postgres
    export NEW_REGION=eu-west-3
    ```

    </details> 

    <br/>

1.  To copy the data from the old bucket to the new bucket, you first need to locate and note the names of the source and target folders. You need to copy the following folders and their content:

    - **Internal EDB backups folder** &mdash; The internal backups folder in the old bucket `edb-internal-backups/\<random-string\>` is different in the new HM instance, as it will have a different `\<random-string\>`.

    - **Postgres clusters backups folder** &mdash; `customer-pg-backups`.

    - **Folder corresponding to any defined custom storage locations** &mdash; if you have custom storage locations defined in the old HM instance, copy the corresponding folders as well.

1.  Copy the old backups to the new bucket using your preferred tools. Here are some examples using cloud service provider CLIs to move data between buckets:

    <TabContainer syncKey="CSP">
    <Tab title="AWS">

    ```shell
    aws s3 cp --recursive s3://${OLD_BUCKET}/edb-internal-backups/${OLD_BACKUP_ID} s3://${NEW_BUCKET}/edb-internal-backups
    aws s3 cp --recursive s3://${OLD_BUCKET}/customer-pg-backups s3://${NEW_BUCKET}/customer-pg-backups
    ```

    </Tab>

    <Tab title="GCP">

    ```shell
    gsutil -m cp -r gs://${OLD_BUCKET}/edb-internal-backups/${OLD_BACKUP_ID} gs://${NEW_BUCKET}/edb-internal-backups
    gsutil -m cp -r gs://${OLD_BUCKET}/customer-pg-backups gs://${NEW_BUCKET}/customer-pg-backups
    ```

    </Tab>
    </TabContainer>

    [I read that gsutil is being deprecated, can we have the gcloud commands for this?]: # 
    
    [Also, in these examples we don't specify how to move the custom storage locations, should we add that as an optional step?]: #

1.  Load the backups you just copied to your new HM instance by creating a new custom resource definition and applying it to the new HM instance:

    <TabContainer syncKey="CSP">
    <Tab title="AWS">

    ```yaml
    kubectl apply -f - <<EOF
    apiVersion: velero.io/v1
    kind: BackupStorageLocation
    metadata:
      annotations:
        appliance.enterprisedb.com/s3-prefixes: edb-internal-backups/velero
      labels:
        appliance.enterprisedb.com/storage-credentials: bound
      name: recovery
      namespace: velero
    spec:
      accessMode: ReadOnly
      config:
        insecureSkipTLSVerify: "false"
        region: ${NEW_REGION}
        s3ForcePathStyle: "true"
      default: false
      objectStorage:
        bucket: ${NEW_BUCKET}
        prefix: edb-internal-backups/velero
      provider: aws
    EOF
    ```

    </Tab>

    <Tab title="GCP">

    ```yaml
    kubectl apply -f - <<EOF
    apiVersion: velero.io/v1
    kind: BackupStorageLocation
    metadata:
      annotations:
        appliance.enterprisedb.com/s3-prefixes: edb-internal-backups/velero
      labels:
        appliance.enterprisedb.com/storage-credentials: bound
      name: recovery
      namespace: velero
    spec:
      accessMode: ReadOnly
      credential:
        key: gcp
        name: gcs-credentials
      default: false
      objectStorage:
        bucket: ${NEW_BUCKET}
        prefix: edb-internal-backups/velero
      provider: gcp
    EOF
    kubectl apply -f - <<EOF
    apiVersion: velero.io/v1
    kind: BackupStorageLocation
    metadata:
      annotations:
        appliance.enterprisedb.com/s3-prefixes: edb-internal-backups/velero
      labels:
        appliance.enterprisedb.com/storage-credentials: bound
      name: recovery
      namespace: velero
    spec:
      accessMode: ReadOnly
      credential:
        key: gcp
        name: gcs-credentials
      default: false
      objectStorage:
        bucket: ${NEW_BUCKET}
        prefix: edb-internal-backups/velero
      provider: gcp
    EOF
    ```

    </Tab>
    </TabContainer>

1.  Confirm that the new storage location is available:

    ```shell
    velero get backup-locations
    ```
    If the status is not `Available`, check the Velero pod logs for permission errors on the s3 bucket.

1.  Confirm that the backups are available as well: 

    ```shell
    velero get backups --selector velero.io/storage-location=recovery
    ```

1.  Choose the backup you want to restore from. You can have multiple backups available, so choose the one that best suits your needs, e.g. the most recent backup before the disaster happened. Note the Velero backup name, as well as the date and time (UTC), as both are required for a restore, for example: 

    ```bash
    NAME                                      STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR  
    velero-backup-kube-state-**20241216154403**   Completed   0        0          2024-12-16 16:44:03 \+0100 CET   5d        recovery           \<none\>
    ```

    !!! Note
        The timestamp value is referred to as the *recovery date* in the instructions that follow.

1.  (Optional) If you were using HM to manage AI workloads, e.g. with the GenAI Builder, also copy the object store files and CORS configuration from the old bucket to the new one:

    ```shell
    export OLD_BUCKET_DATALAKE=<old bucket>
    export NEW_BUCKET_DATALAKE=<new bucket>
    ```

    <TabContainer syncKey="CSP">
    <Tab title="AWS">

    ```shell
    # Copy data lake objects from old bucket to new bucket
    aws s3 cp --recursive s3://${OLD_BUCKET_DATALAKE}/ s3://${NEW_BUCKET_DATALAKE}/
    # Copy CORS configuration from old bucket to new bucket
    aws s3api get-bucket-cors --bucket ${OLD_BUCKET_DATALAKE} --output json > cors-config.json
    aws s3api put-bucket-cors --bucket ${NEW_BUCKET_DATALAKE} --cors-configuration file://cors-config.json
    ```

    </Tab>

    <Tab title="GCP">

    ```shell
    # Copy data lake objects from old bucket to new bucket
    gsutil -m cp -r gs://${OLD_BUCKET_DATALAKE}/ gs://${NEW_BUCKET_DATALAKE}/
    # Copy CORS configuration from old bucket to new bucket
    gsutil cors get gs://${OLD_BUCKET_DATALAKE} > cors-config.json
    gsutil cors set cors-config.json gs://${NEW_BUCKET_DATALAKE}
    ```

    [same here with gsutil being deprecated, can we have the gcloud commands for this?]: #

    </Tab>
    </TabContainer>

## 2. Recovery steps



### Restore EDB internal databases (`app-db` and `beacon-db`)

Once the old backups are available, you can restore the EDB internal databases. For each internal database:

1.  Save the cluster manifest to a yaml file: `kubectl get \<cluster-name\> \-o yaml \>\<cluster-name\>.yaml`.  
2.  Edit the cluster spec in the yaml file so the cluster is created from the backups:  

-   Replace the **init** section in bootstrap with a **recovery** section:  
    ```yaml
    recovery:  
      database: \<database name as in the init section\>  
      owner: \<owner name as in the init section\>  
      source: \<pg-cluster-name\>  
      secret:  
        name: \<secret name as in the init section\>  
      recoveryTarget:  
        targetTime: "\<recovery date in YYYY-MM-DD HH:MM:SS+00 format\>"  
    ```
-   Add the following section:  
    ```yaml
    externalClusters:  
    \- barmanObjectStore:  
        destinationPath:  S3://\<linked-bucket-name\>/edb-internal-backups/\<old-backups-random-string\>/databases  
        s3Credentials:  
          inheritFromIAMRole: true  
        wal:  
          maxParallel: 8  
      name: \<pg-cluster-name\>  
    ```
-   Add the following prefix to the `appliance.enterprisedb.com/s3-prefixes` annotation of the `inheritedMetdata` section (the list is comma separated): 
    ```yaml
     edb-internal-backups/\<old-backups-random-string\>/databases/\<db-name\>  
    ```
3.  Delete the cluster:

    ```bash
    kubectl delete cluster \<cluster-name\>)  
    ```

4.  Clean the backup area for the cluster:

    ```bash
    aws s3 rm s3://\<linked-bucket-name\>/edb-internal-backups/\<new-backups-random-string\>/databases/\<pg-cluster-name\>  \--recursive
    ```

5.  Apply the yaml file for the cluster to be re-created: `kubectl apply \-f \<cluster-name\>.yaml`
6.  After the cluster is successfully restored and in a healthy state, restart the `accm-server`  in the namespace `upm-beaco-ff-base`. 

At this point, the portal on the new cluster is available again.

### Configure the Velero plugin

The plugin helps restore the Kubernetes resources in a correct state, so only the custom managed storage locations are restored. The Postgres clusters resources are restored as deleted, so you can later restore data as desired.

The plugin configuration is made through a `ConfigMap`, so you must apply this manifest:

```yaml
apiVersion: v1  
kind: ConfigMap  
metadata:  
  name: velero-plugin-for-edbpgai  
  namespace: velero  
  labels:  
    velero.io/plugin-config: ""  
    enterprisedb.io/edbpgai-plugin: RestoreItemAction  
data:  
  \# configure disaster recovery mode, so restored items are transformed as needed  
  drMode: "true"  
  \# configure a date corresponding to the velero backup date. Note the format\!  
  drDate: "\<recovery date in YYYY–MM-DDTHH:MM:SSZ format\>”  
  \# old and new buckets for internal custom storage locations  
  oldBucket: \<old-HM instance-bucket-name\>  
  newBucket: \<new-HM instance-bucket-name\>
```

### Restore the custom managed storage locations

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-1-storagelocations  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- storagelocations.biganimal.enterprisedb.com  
  includeClusterResources: true  
  labelSelector:  
    matchLabels:  
      biganimal.enterprisedb.io/reserved-by-biganimal: "false"
```

### Restore the cluster wrappers

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-2-clusterwrappers  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- clusterwrappers.beacon.enterprisedb.com  
  restoreStatus:  
    includedResources:  
    \- clusterwrappers.beacon.enterprisedb.com
```

### Restore the backup wrappers

Configure and apply the following Velero restore resource manifest:

```yaml
apiVersion: velero.io/v1  
kind: Restore  
metadata:  
  name: restore-3-backupwrappers  
  namespace: velero  
spec:  
  \# Change the backup name to a custom backup name as required  
  backupName: \<velero-backup-name\>  
  includedResources:  
  \- backupwrappers.beacon.enterprisedb.com  
  restoreStatus:  
    includedResources:  
    \- backupwrappers.beacon.enterprisedb.com
```
