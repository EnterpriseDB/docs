---
title: How-to Verify catalog with PyIceberg and PySpark
navTitle: Verify catalog (PyIceberg/PySpark)
description: Use PyIceberg to list and query Lakekeeper catalog tables, and PySpark to enumerate and query views and tables.
---

# Verify catalog with PyIceberg and PySpark

Use third-party tools to validate that catalog-managed tables and views created by PGD offload and data loaders are accessible.

## Verify with PyIceberg

```python
from pyiceberg.catalog import load_catalog
from pyiceberg.catalog.rest import RestCatalog
import os

URI = os.getenv("PARAMS_URI")
TOKEN = os.getenv("API_WRITE")
WAREHOUSE = os.getenv("PARAMS_WAREHOUSE")

catalog = RestCatalog(
    name="PGAA Test",
    warehouse=WAREHOUSE,
    uri=URI,
    token=TOKEN,
    ssl={"cabundle": "ca.crt"}
)
print("table list:")
for namespace in catalog.list_namespaces():
    print(f"{namespace[0]}: ")
    for _, table in catalog.list_tables(namespace):
        print(f"    {table}")
print()

print("tpch_sf_1.lineitem read:")
table = catalog.load_table(("tpch_sf_1", "lineitem"))
print(f"row count: {table.scan().count()}")
print(table.scan(limit=1).to_arrow())
print()

print("public.transactional_table read:")
table = catalog.load_table(("public", "transactional_table"))
print(f"row count: {table.scan().count()}")
print(table.scan(limit=1).to_arrow())
print()

sample_partitioned_table = [t for _, t in catalog.list_tables("public") if t.startswith("partitioned_table")][0]

print(f"public.{sample_partitioned_table} read:")
table = catalog.load_table(("public", sample_partitioned_table))
print(f"row count: {table.scan().count()}")
print(table.scan(limit=1).to_arrow())
print()
```

## Query the cold data view with PySpark

```python
from pyspark.sql import SparkSession

rest_catalog_uri = "$PARAMS_URI"
warehouse = "$PARAMS_WAREHOUSE"
catalog_name = "lakekeeper"
access_token = "$API_WRITE"

spark = SparkSession.builder \
    .appName("IcebergRESTCatalogWithToken") \
    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0,org.apache.iceberg:iceberg-gcp-bundle:1.9.0') \
    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \
    .config(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog") \
    .config(f"spark.sql.catalog.{catalog_name}.type", "rest") \
    .config(f"spark.sql.catalog.{catalog_name}.uri", rest_catalog_uri) \
    .config(f"spark.sql.catalog.{catalog_name}.warehouse", warehouse) \
    .config(f"spark.sql.catalog.{catalog_name}.token", access_token)

spark = spark.getOrCreate()

namespace = "public"
print(f"\nTables in the namespace '{namespace}' of catalog '{catalog_name}':")
tables_in_namespace = spark.sql(f"SHOW TABLES IN {catalog_name}.{namespace}").collect()
for row in tables_in_namespace:
    print(f"- {row.tableName}")

print(f"\nViews in the namespace '{namespace}' of catalog '{catalog_name}':")
views_in_namespace = spark.sql(f"SHOW VIEWS IN {catalog_name}.{namespace}").collect()
for row in views_in_namespace:
    print(f"- {row.viewName}")

test_transactional_table = spark.sql(f"SELECT * FROM {catalog_name}.{namespace}.transactional_table LIMIT 10").collect()
offloaded_count = spark.sql(f"SELECT COUNT(*) FROM {catalog_name}.{namespace}.partitioned_table_offloaded").collect()

print(test_transactional_table)
print(offloaded_count)

spark.stop()
```
