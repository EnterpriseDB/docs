---
title: Analytics Accelerator architecture
navTitle: Architecture
description: High-level architecture of the Analytics Accelerator, including Lakehouse, data flow, catalogs, and integration with EDB Postgres Distributed and Hybrid Manager.
deepToC: true
---

# Analytics Accelerator architecture

This page summarizes the high-level architecture for the Analytics Accelerator built on EDB Postgres AI®. It connects transactional Postgres with open table formats and cloud/object storage to enable scalable, cost‑effective analytics.

## Core building blocks

- Lakehouse: See [Lakehouse overview](./lakehouse) for components and concepts.
- Open table formats: [Apache Iceberg](./iceberg) and [Delta Lake](./delta_lake).
- Tiered data: [Tiered Tables](./tiered_tables) with offload to object storage for cold data and long‑term retention.
- Orchestration and governance: Hybrid Manager (HM) provisions/operates clusters and surfaces analytics capabilities.

## Logical layers

1. Ingest and transactional layer (Postgres / PGD)
   - OLTP applications write to Postgres and optionally EDB Postgres Distributed (PGD) for HA and scale‑out.
   - Tiered Tables can move cold partitions out to object storage.

2. Storage and metadata layer (object storage + table format)
   - Data lands in object storage (for example, S3, GCS, or compatible) using Iceberg/Delta layout and metadata.
   - Optionally backed by a catalog for centralized table metadata.

3. Query and transformation layer
   - Lakehouse nodes query data in place using table format readers and pushdown.
   - External engines and libraries (for example, PyIceberg, Spark) can read/write the same tables when a catalog is configured.

4. Access and integration
   - SQL access from Postgres clients for unified workflows.
   - APIs and drivers for third‑party tools.

## Data flow (common patterns)

- Offload from PGD
  - Use Tiered Tables or offload jobs to move older partitions to object storage while keeping recent data hot.
  - See: [Query tiered tables](./query-tiered-tables) and [Monitor tiered tables](./monitor-tiered-tables).

- Read‑only without a catalog
  - Point Lakehouse at existing Delta/Iceberg tables for ad‑hoc or exploratory reads.
  - See: [Read Delta without a catalog](./how-to-read-delta-without-catalog) and [Read Iceberg without a catalog](./how-to-read-iceberg-without-catalog).

- With a catalog (recommended for multi‑engine)
  - Create or attach a catalog to unify table metadata across engines.
  - See: [Configure Iceberg catalog](./configure-iceberg-catalog), [Attach catalog and read](./how-to-attach-catalog-and-read), and [Verify with PyIceberg/PySpark](./how-to-verify-catalog-with-pyiceberg-pyspark).

## Catalogs and interoperability

- Catalog‑backed tables enable consistent table discovery and schema evolution across engines.
- Lakehouse queries continue to work with or without a catalog; using a catalog is recommended for teams using Spark or Python libraries alongside Postgres.

## Performance considerations

- Partitioning and clustering
  - Design partitions for common predicates. Keep hot ranges small and selective.
- File sizing
  - Balance write throughput and read parallelism; avoid too many tiny files.
- Pushdown and vectorization
  - Prefer operators and predicates supported by the table reader for better pushdown.
- Sampling and statistics
  - Use [lakehouse_table_stats and PGAA functions](./reference/pgaa_functions.mdx) to inspect table health.

## Security and governance

- Object storage
  - Use IAM‑scoped buckets/paths per environment and workload.
- Network and encryption
  - Encrypt at rest (bucket policies) and in transit (TLS). Restrict access to Lakehouse/PGD nodes.
- Auditing and access control
  - Use Postgres roles/policies; align with HM‑provided guardrails.

## High availability

- PGD provides HA for transactional workloads.
- Lakehouse scale‑out is achieved by adding query nodes; storage remains durable in object storage.

## Getting started

- Provision a Lakehouse cluster: [Create a cluster](./create-lakehouse-cluster).
- Prepare environments and variables: [Getting setup](./getting-setup).
- Explore concepts: [Generic analytics concepts](./generic-concepts) and [EDB analytics concepts](./analytics-concepts).

## Related topics

- Reference: [Functions](./reference/functions.mdx), [DirectScan](./reference/directscan.mdx), [Datasets](./reference/datasets.mdx).
- How‑to guides: [Index](./how-to).
