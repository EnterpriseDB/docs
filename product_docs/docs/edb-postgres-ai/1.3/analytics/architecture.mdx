---
title: Analytics Accelerator Architecture
navTitle: Architecture
description: Understanding the Analytics Accelerator architecture for unified transactional and analytical processing
---

## Architecture Overview

Analytics Accelerator extends PostgreSQL's capabilities to create a unified platform for transactional and analytical workloads. The architecture combines operational PostgreSQL databases with lakehouse storage patterns, enabling organizations to query data across multiple storage tiers without complex ETL pipelines or data duplication.

The system operates on three fundamental principles: separation of compute and storage for elastic scaling, transparent query access across hot and cold data tiers, and compatibility with open table formats for ecosystem interoperability. These principles enable cost-effective analytics at petabyte scale while maintaining PostgreSQL's familiar SQL interface and operational characteristics.

## Core Components

### EDB Postgres Distributed Foundation

EDB Postgres Distributed (PGD) provides the transactional foundation with built-in high availability, horizontal scaling, and automated data lifecycle management. PGD nodes handle operational workloads with full ACID compliance while AutoPartition manages time-based partitioning strategies essential for data tiering.

The distributed architecture ensures zero downtime during maintenance operations and provides geographic distribution for disaster recovery. Write operations maintain consistency through multi-master replication while read operations scale horizontally across available nodes.

### Lakehouse Query Engine

Dedicated Lakehouse nodes execute analytical queries against data in object storage. These stateless compute nodes leverage Apache Datafusion®'s vectorized execution engine to process columnar data formats efficiently. The separation from transactional nodes ensures analytical workloads don't impact operational performance.

The query engine implements sophisticated optimizations including partition pruning, predicate pushdown, and adaptive query execution. These optimizations reduce data scanning by orders of magnitude, enabling interactive query performance on massive datasets.

### Storage Abstraction Layer

PostgreSQL File System (PGFS) provides unified access to diverse storage backends including AWS S3, Azure Blob Storage, Google Cloud Storage, and on-premises object stores. This abstraction handles authentication, network resilience, and performance optimization transparently.

The storage layer supports multiple simultaneous storage locations, enabling queries that span cloud providers or combine cloud and on-premises data. Intelligent caching reduces metadata operations and improves query latency for frequently accessed data.

### Table Format Integration

Native support for Apache Iceberg® and Delta Lake enables interoperability with the broader data ecosystem. Analytics Accelerator (PGAA) reads table metadata to understand schema, partitioning, and file locations without manual configuration.

For Iceberg, the system supports complete functionality including schema evolution, hidden partitioning, and time travel queries. Delta Lake support focuses on read operations, providing access to existing Spark-based data lakes. Both formats maintain compatibility with external tools ensuring data remains accessible across multiple compute engines.

## Architectural Layers

### Ingestion and Transaction Layer

Applications write data to PostgreSQL or PGD clusters using standard database connections. This layer handles real-time transactions, maintains referential integrity, and ensures data consistency through ACID guarantees.

Streaming ingestion systems deliver continuous data flows while batch loads handle periodic updates. The transaction layer maintains operational performance regardless of historical data volume through intelligent partitioning and tiering strategies.

### Storage and Metadata Layer

Data resides in object storage using open table formats that provide structure and governance. Parquet files store actual data while metadata tracks schemas, partitions, and file locations. This separation enables schema evolution and time travel without data rewriting.

Optional catalog services provide centralized metadata management for multi-engine environments. Catalogs enable consistent table discovery, coordinate concurrent modifications, and maintain audit trails across different compute engines.

### Query and Transformation Layer

Multiple query engines access the same underlying data based on workload requirements. Analytics Accelerator handles interactive SQL queries, Apache Spark processes complex transformations, and machine learning platforms read training data directly.

Query coordination ensures consistency when multiple engines operate concurrently. Transaction isolation prevents conflicts while snapshot semantics provide repeatable read guarantees essential for analytical accuracy.

### Access and Integration Layer

Standard PostgreSQL wire protocol ensures compatibility with existing tools and applications. Business intelligence platforms, data science notebooks, and custom applications connect without specialized drivers or modifications.

REST APIs enable programmatic access for automation and integration scenarios. These APIs support catalog management, monitoring, and administrative operations essential for production deployments.

## Data Flow Patterns

### Operational to Analytical Pipeline

Data begins its lifecycle in PostgreSQL supporting transactional operations. As data ages and access patterns shift from operational to analytical, Tiered Tables automatically migrate partitions to object storage. This migration occurs transparently with no application changes required.

Recent data remains in PostgreSQL for sub-millisecond query latency while historical data moves to cost-effective object storage. Queries automatically span both tiers, providing complete visibility across all data regardless of storage location.

### Direct Lakehouse Access

Organizations with existing data lakes connect Analytics Accelerator directly to Iceberg or Delta Lake tables. This pattern eliminates data movement, enabling immediate analytical capabilities on existing investments.

The system discovers table schemas automatically, translates PostgreSQL SQL to appropriate operations, and returns results through standard database protocols. This approach proves particularly valuable during migrations or when integrating with established data platforms.

### Multi-Engine Collaboration

Different engines process data based on their strengths while sharing common storage. Spark handles complex ETL transformations writing results as Iceberg tables. Analytics Accelerator queries these tables for interactive analysis. Machine learning platforms read the same data for model training.

This collaborative approach eliminates data duplication and ensures consistency across analytical workflows. Changes made by one engine become immediately visible to others through shared metadata and snapshot isolation.

## Performance Architecture

### Query Optimization Pipeline

The optimizer analyzes queries to identify optimization opportunities specific to lakehouse workloads. Partition pruning eliminates irrelevant data before query execution begins. Predicate pushdown moves filters to the storage layer minimizing data transfer.

Cost-based optimization considers network latency, data locality, and format characteristics when generating execution plans. The optimizer adapts strategies based on actual data distribution discovered during execution.

### Caching Hierarchy

Multiple caching layers optimize repeated access patterns. Metadata caching stores table schemas, partition information, and file locations. Data caching retains frequently accessed Parquet files in local storage. Query result caching serves identical queries without recomputation.

Cache invalidation respects table format versioning ensuring consistency despite underlying changes. Adaptive cache management prioritizes retention based on access frequency, query cost, and available resources.

### Parallel Execution

Queries parallelize across multiple dimensions for maximum throughput. Partition-level parallelism processes independent partitions simultaneously. File-level parallelism reads multiple files within partitions concurrently. Column-level parallelism leverages vectorized execution for columnar operations.

The execution framework dynamically adjusts parallelism based on available resources and query characteristics. This adaptation prevents resource exhaustion while maximizing hardware utilization.

## High Availability and Resilience

### Transactional Tier Availability

PGD provides continuous availability through multi-master replication. Node failures trigger automatic failover with zero data loss. Geographic distribution enables disaster recovery with configurable recovery objectives.

Maintenance operations including upgrades and schema changes occur without downtime through rolling deployment strategies. The system maintains full functionality during partial failures ensuring business continuity.

### Analytical Tier Availability

Stateless Lakehouse nodes provide availability through redundancy rather than replication. Load balancers distribute queries across available nodes with automatic failure detection and rerouting.

Object storage durability guarantees ensure data availability independent of compute failures. Multiple availability zones and cross-region replication provide additional resilience for critical datasets.

### Catalog Resilience

Catalog services implement their own availability strategies based on chosen implementation. REST catalogs leverage HTTP load balancing and backend redundancy. AWS S3 Tables provides serverless availability through managed infrastructure.

Catalog unavailability affects table discovery but not query execution for known tables. The architecture gracefully degrades maintaining core functionality during catalog maintenance or failures.

## Security Architecture

### Access Control Framework

PostgreSQL role-based access control extends to lakehouse resources. Table and column-level permissions apply transparently regardless of storage location. Row-level security policies filter data dynamically based on user context.

Integration with enterprise authentication systems provides single sign-on capabilities. LDAP, Active Directory, and OAuth implementations ensure consistent identity management across the platform.

### Encryption Strategy

Comprehensive encryption protects data throughout its lifecycle. Transport Layer Security encrypts data in motion between clients, compute nodes, and storage. Server-side encryption protects data at rest using cloud provider or customer-managed keys.

Column-level encryption provides additional protection for sensitive fields. Encrypted columns remain queryable through specialized indexes while maintaining security compliance.

### Audit and Compliance

Detailed audit logging tracks all data access for compliance requirements. Query logs capture user, timestamp, and accessed objects. Storage access logs record all object operations. Authentication logs track login attempts and privilege changes.

Compliance frameworks including GDPR, HIPAA, and PCI DSS guide security implementations. Data residency controls ensure geographic compliance while retention policies automate data lifecycle management.

## Deployment Patterns

### Managed Service Deployment

Hybrid Manager provides fully managed Analytics Accelerator deployment with automated provisioning, scaling, and maintenance. The platform handles infrastructure management, security updates, and performance optimization.

### Self-Managed Deployment

Self-managed deployments provide complete control over infrastructure and configuration. Organizations deploy on-premises, in private clouds, or using infrastructure-as-code approaches.

Kubernetes operators simplify deployment and lifecycle management. Helm charts provide templated configurations for common scenarios. Terraform modules enable infrastructure automation across cloud providers.

### Hybrid Deployment

Hybrid patterns combine managed control planes with self-managed data planes. This approach balances operational simplicity with data sovereignty requirements common in regulated industries.

The managed control plane handles orchestration, monitoring, and metadata management. Self-managed data planes maintain data within organizational boundaries while leveraging platform capabilities.
