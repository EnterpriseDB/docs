---
title: Loading data
navTitle: Loading Data

---

<!-- ---
title: Loading data (sync or bring your own)
navTitle: Loading data
description: How to load data into Lakehouse.
--- -->

## Bringing your own data

It's possible to point your Lakehouse node at an arbitrary S3 bucket with Delta Tables inside of it.
However, there are some major caveats to doing so. (These will be resolved in future releases.)

### Caveats

-   The tables must be stored as [Delta Lake Tables](http://github.com/delta-io/delta/blob/master/PROTOCOL.md) within the location.
-   A *Delta Lake Table* (or *Delta Table*) is a folder of Parquet files along with some JSON metadata.

### Loading data into your bucket

You can use the lakehouse-loader utility to export data from an arbitrary Postgres instance to Delta Tables in a storage bucket.
See [Delta Lake Table tools](/edb-postgres-ai/1.3/analytics/reference/delta_tables.mdx) for more information on how to get and use that utility.

For more details, see [querying Delta Lake tables](/edb-postgres-ai/1.3/analytics/how-to-lakehouse-read-with-without-catalog).
