---
title: How-To Guides for Analytics in Hybrid Manager
navTitle: How-To Guides
description: Step-by-step guides for configuring and using analytics features in Hybrid Manager, including Lakehouse Clusters, Apache Iceberg, Delta Lake, and Tiered Tables.
---

This section provides step-by-step guides for configuring and using core analytics features within your Hybrid Manager (HM) environment:

- Deploying and managing **Lakehouse Clusters** for advanced analytical queries
- Configuring and querying **Apache Iceberg** tables
- Configuring and querying **Delta Lake** tables
- Implementing and managing **Tiered Tables** with EDB Postgres Distributed (PGD)

For conceptual background, see [Analytics Concepts in Hybrid Manager](../../../hybrid-manager/analytics/learn/analytics_concepts).

---

## Use Cases

- Read-only Delta without a catalog: Use [Read-only analytics without a catalog: Delta Lake](./read-only-without-catalog) to point directly at Delta datasets in object storage and run SQL.
- Read-only Iceberg without a catalog: Use [Read-only analytics without a catalog: Iceberg](./read-only-analytics-iceberg) to query Iceberg tables by path without a catalog.
- Lakehouse Delta without a catalog: Use [Lakehouse Delta (no catalog)](./lakehouse-delta-no-catalog) for a full TPCH example and 1B-row schema.
- Lakehouse Iceberg without a catalog: Use [Lakehouse Iceberg (no catalog)](./lakehouse-iceberg-no-catalog) to query Iceberg by path from Lakehouse.
- Lakehouse with a catalog: Use [Lakehouse attach catalog](./lakehouse-attach-catalog-and-read) to read catalog-managed tables and views.
- Offload PGD without a catalog: Use [PGD offloads without a catalog](./pgd-offloads-without-catalog) to write offloaded Iceberg tables to filesystem paths and validate reads/restores.
- Tiered Tables storage savings: Use [BDR AutoPartition test without a catalog](./autopartition-test-without-catalog) to offload old partitions and observe before/after storage.
- Catalog-managed Iceberg: Use [Setup Lakekeeper catalog](./setup-lakekeeper-catalog-and-import) + [Lakehouse attach catalog](./lakehouse-attach-catalog-and-read) for interoperable analytics.
- Catalog setup and sync: Use [Setup Lakekeeper catalog](./setup-lakekeeper-catalog-and-import) to load sample tables, attach, and import.
- Offload to catalog end-to-end: Use [Offload to catalog tests](./offload-to-catalog-tests) to validate unified views and counts.
- Validate catalog from non-leader: Use [Catalog reads on non-leader](./read-catalog-from-non-leader) to confirm HA reads.
- Verify with third-party tools: Use [Verify catalog (PyIceberg/PySpark)](./verify-catalog-with-pyiceberg-pyspark) to list and query.
- Third-party catalogs: Use [Configure Snowflake Open Catalog (Polaris)](./configure-polaris-catalog) or [Use AWS S3Tables catalog](./lakehouse-s3tables-catalog).
- Tiered Tables with PGD: Use [PGD offloads without a catalog](./pgd-offloads-without-catalog) and [BDR AutoPartition test without a catalog](./autopartition-test-without-catalog).

---

## Getting Started

- [Getting setup for Lakehouse analytics](./getting-setup)

Next: [Working with Apache Iceberg](#working-with-apache-iceberg)

---

## Quick Decision Guide

- Need ad-hoc reads on S3 without a catalog?
  - Delta: [Read-only without a catalog](./read-only-without-catalog) or [Lakehouse Delta (no catalog)](./lakehouse-delta-no-catalog)
  - Iceberg: [Read-only without a catalog](./read-only-analytics-iceberg) or [Lakehouse Iceberg (no catalog)](./lakehouse-iceberg-no-catalog)
- Want interoperability across engines (Spark/Trino/PyIceberg)?
  - Lakekeeper: [Setup Lakekeeper catalog](./setup-lakekeeper-catalog-and-import) then [Attach & read](./lakehouse-attach-catalog-and-read)
  - Third-party: [Configure Polaris](./configure-polaris-catalog) or [Use S3Tables catalog](./lakehouse-s3tables-catalog)
- Reducing PGD storage and querying cold data?
  - No catalog: [PGD offloads without a catalog](./pgd-offloads-without-catalog) then [AutoPartition test](./autopartition-test-without-catalog)
  - Catalog-managed: [Offload to catalog tests](./offload-to-catalog-tests) then [Read from non-leader](./read-catalog-from-non-leader)

---

## Working with Apache Iceberg

- [Read-only analytics without a catalog: Iceberg](./read-only-analytics-iceberg)
- [Setup Lakekeeper catalog and import](./setup-lakekeeper-catalog-and-import)
- [Offload to catalog tests](./offload-to-catalog-tests)
- [Catalog reads from a non-leader node](./read-catalog-from-non-leader)
- [Verify catalog with PyIceberg and PySpark](./verify-catalog-with-pyiceberg-pyspark)
- [Configure Snowflake Open Catalog (Polaris)](./configure-polaris-catalog)
- [Use AWS S3Tables catalog with Lakehouse](./lakehouse-s3tables-catalog)
- [Lakehouse attach a catalog and read data](./lakehouse-attach-catalog-and-read)
- [Lakehouse read-only Iceberg without a catalog](./lakehouse-iceberg-no-catalog)

Next: [Working with Delta Lake](#working-with-delta-lake)

---

## Working with Delta Lake

- [Read-only analytics without a catalog: Delta Lake](./read-only-without-catalog)
- [Lakehouse read-only Delta without a catalog (full example)](./lakehouse-delta-no-catalog)

Next: [Implementing Tiered Tables](#implementing-tiered-tables)

---

## Tiered Tables (AutoPartition)

Run end-to-end Tiered Tables validation without a catalog:

- [PGD offloads without a catalog (setup and test)](./pgd-offloads-without-catalog)
- [BDR AutoPartition test without a catalog (observe storage savings)](./autopartition-test-without-catalog)

Next: [Use Cases for Analytics in Hybrid Manager](../../../hybrid-manager/analytics/learn/use-cases)

---

## No-Catalog Workflows

- [Read-only analytics without a catalog: Delta Lake](./read-only-without-catalog)
- [Read-only analytics without a catalog: Iceberg](./read-only-analytics-iceberg)
- [PGD offloads without a catalog (setup and test)](./pgd-offloads-without-catalog)
- [BDR AutoPartition test without a catalog (observe storage savings)](./autopartition-test-without-catalog)
 - [Lakehouse read-only Delta without a catalog (full example)](./lakehouse-delta-no-catalog)
 - [Lakehouse read-only Iceberg without a catalog](./lakehouse-iceberg-no-catalog)

---

## Lakehouse Workflows

- With a catalog:
  - [Setup Lakekeeper catalog and import](./setup-lakekeeper-catalog-and-import)
  - [Lakehouse attach a catalog and read data](./lakehouse-attach-catalog-and-read)
- Without a catalog:
  - [Lakehouse read-only Delta without a catalog (full example)](./lakehouse-delta-no-catalog)
  - [Lakehouse read-only Iceberg without a catalog](./lakehouse-iceberg-no-catalog)

---

## Third-Party Catalogs

- [Configure Snowflake Open Catalog (Polaris)](./configure-polaris-catalog)
- [Use AWS S3Tables catalog with Lakehouse](./lakehouse-s3tables-catalog)
- [Verify catalog with PyIceberg and PySpark](./verify-catalog-with-pyiceberg-pyspark)

---

## Guided Use Cases

### Ad-hoc reads without a catalog

If you want to explore existing Delta or Iceberg data in object storage without setting up a catalog:

1. Create a PGFS storage location pointing to your bucket.
2. Create PGAA reader tables that reference the dataset paths.
3. Run SQL queries; optionally validate table stats.

Resources:

- Delta: [Read-only without a catalog](./read-only-without-catalog), [Lakehouse Delta (no catalog)](./lakehouse-delta-no-catalog)
- Iceberg: [Read-only without a catalog](./read-only-analytics-iceberg), [Lakehouse Iceberg (no catalog)](./lakehouse-iceberg-no-catalog)

### Offload operational data without a catalog

When you need to reduce PGD storage but keep data queryable without catalog overhead:

1. Create a PGFS storage location for analytics offload.
2. Set the PGD node group `analytics_storage_location`.
3. Enable offload per table; validate reads via PGAA readers.
4. Optionally, enable AutoPartition and observe storage savings.

Resources:

- [PGD offloads without a catalog](./pgd-offloads-without-catalog)
- [BDR AutoPartition test without a catalog](./autopartition-test-without-catalog)

### Catalog-managed Lakehouse reads and offloads

For interoperability with PyIceberg, Spark, and other engines using a catalog:

1. Load or create tables in a catalog (e.g., Lakekeeper).
2. Add and attach the catalog to PGAA (PGD or Lakehouse).
3. Offload PGD tables to the catalog and validate the unified offloaded view.
4. Read from non-leader nodes for scale and validate with third-party tools.

Resources:

- [Setup Lakekeeper catalog](./setup-lakekeeper-catalog-and-import)
- [Lakehouse attach catalog and read](./lakehouse-attach-catalog-and-read)
- [Offload to catalog tests](./offload-to-catalog-tests)
- [Read catalog from non-leader](./read-catalog-from-non-leader)
- [Verify with PyIceberg/PySpark](./verify-catalog-with-pyiceberg-pyspark)

### Third-party catalogs (Polaris, S3Tables)

If your organization uses Snowflake Polaris or AWS S3Tables:

1. Obtain required credentials/tokens for the catalog.
2. Add the catalog in PGAA (Polaris via `iceberg-rest`, S3Tables via `iceberg-s3tables`).
3. Import and attach, then run reads and offloads as needed.

Resources:

- [Configure Snowflake Open Catalog (Polaris)](./configure-polaris-catalog)
- [Use AWS S3Tables catalog](./lakehouse-s3tables-catalog)

## Explore Further

- [Analytics Concepts in Hybrid Manager](../../../hybrid-manager/analytics/learn/analytics_concepts)
- [Use Cases for Analytics in Hybrid Manager](../../../hybrid-manager/analytics/learn/use-cases)
- [Persona-Based Guide to Using Analytics in Hybrid Manager](../personas)

---

Use these How-To Guides to configure and operate advanced analytics features in Hybrid Manager.
To match features to your business needs, see [Use Cases for Analytics in Hybrid Manager](../../../hybrid-manager/analytics/learn/use-cases).
For role-specific guidance, visit the [Persona-Based Guide](../personas).
