---
title: Setting up a Rancher RKE2 cluster for use with Hybrid Manager
navTitle: Set up an RKE2 cluster
description: Learn how to set up a Rancher RKE2 cluster for installing PG AI Hybrid Manager.
deepToC: true
---

After you've installed the necessary system tools, you're ready to set up a Rancher RKE2 cluster for use with Hybrid Manager (HM).

To successfully install HM on a RKE on-premises cluster, certain prerequisites and configurations must be in place.

## Preconfiguration cluster requirements

* HM currently supports:

    * RKE2 1.31.X with Kubernetes 1.31.X
    * RKE2 1.32.X with Kubernetes 1.32.X

## Configure node size and resources

To support HM and its associated components, the nodes in your Rancher RKE2 cluster must meet the resource requirements.

Please see the [Rancher RKE2](https://docs.rke2.io/install/requirements) for how to set up these master nodes before moving on to the worker nodes that run HM.

### Node sizing

With the RKE2 master nodes and Kubernetes cluster created, you can move onto the two sets of worker nodes required for HM: the HM Control Plane nodes and HM Postgres Data Nodes:

#### HM Control Plane nodes

HM Control Plane (CP) worker nodes must meet the minimum requirements for resource allocation (number of worker nodes and CPU, memory, and disk size for each) as outlined below.

The size of the telemetry stack (particularly Prometheus and Thanos) you require for your cluster is the primary factor driving scaling up/out for HM CP nodes.
Because the telemetry stack scales with the number of Postgres databases the CP is monitoring, CP nodes scale with the number of databases being managed.

-   General recommendations:
    -   3 CP nodes each with:
        -   CPU: 
            -   Minimum: 8 vCPUs (for up to 10 Postgres databases)
            -   Recommended: 16 vCPUs for medium-sized clusters (10-50 Postgres databases)
            -   For >50 Postgres databases: 16+ vCPUs
        -   Memory: 
            -   Minimum: 32 GB RAM (for up to 10 Postgres databases)
            -   Recommended: 64 GB RAM or more for larger workloads (10-50 Postgres databases)
            -   For >50 Postgres databases: 64+ GB RAM 
        -   Disk
            -   Minimum: 100 GB SSD (for up to 10 Postgres databases)
            -   Recommended: 200 GB SSD (for 10-50 Postgres databases)
            -   For >50 Postgres databases: >200 GB SSD

#### Postgres data nodes

At least three worker nodes for the Postgres data nodes are recommended for smaller workloads (5-20 Postgres databases).
As you scale up Postgres databases beyond around 20, you may require more worker nodes for the Postgres workloads and eventually even more CP nodes for supporting the increasing number of Postgres databases.

-   General recommendations:

    - 6 Postgres data nodes each with:
      -   CPU:
          -   Recommended: 16 vCPU
      -   Memory: 
          -   Recommended: 32 GB of RAM per node
        - Disk: 
          -   Minimum: 100 GB of persistent storage per node (adjust based on database and logging requirements). Use fast disks (SSD) for optimal performance.

#### GPU Node Provisioning and Operator Setup (Optional for AI/ML)

If you plan to leverage GPU capabilities for model deployment, follow these steps to provision and enable the GPU nodes:

##### Rancher Machine Pool Workaround

The Rancher UI may miss certain instance types with GPUs when creating a machine pool. 
Use this workaround to provision the required nodes:

Create a machine pool with any standard instance type, and set the initial replica count to 0.

Update the machine pool configuration manually to specify the required GPU instance type (e.g., g6e.12xlarge).
Rancher should accept the missing instance type during this update phase.

Scale up the machine pool to the desired number of GPU-enabled nodes.

##### Install NVIDIA GPU Operator

After the GPU nodes are provisioned, you must install the NVIDIA GPU Operator to make the GPU resources available to Kubernetes pods:

Install the operator following the official [Rancher RKE2 documentation](https://docs.rke2.io/advanced#deploy-nvidia-operator).

After installation, you can label these new nodes appropriately and leverage them for deploying AI/ML models.

### Taints and labels for CP vs Postgres data nodes machine sets

To (optionally) use different types of nodes for the HM CP nodes and the HM Postgres data nodes hosting the Postgres workloads, RKE2 requires two different machine sets, each with their own taints and labels:

-   CP nodes taints and labels:

    ```yaml
    spec:
        replicas: 3
        template:
        spec:
            metadata:
            labels:
                edbaiplatform.io/control-plane: "true"
        taints:
        - key: edbplatform.io/control-plane
            value: "true"
            effect: NoSchedule
    ```

-   Postgres data nodes taints and labels:

    ```yaml
    spec:
        replicas: 3
        template:
        spec:
            metadata:
            labels:
                edbaiplatform.io/postgres: “true”
            taints:
            - key: edbplatform.io/postgres
                value: "true"
                effect: NoSchedule
    ```

### Networking bandwidth

Ensure each node has adequate networking capacity to handle HM communication and external data transfer (for example, S3 backups).

Baseline recommendations:

| Component               | Recommended bandwidth  | Justification                                                                                    |
| ----------------------- | ---------------------- | ------------------------------------------------------------------------------------------------ |
| K8s control plane nodes | 1 Gbps+ per node       | Handles internal Kubernetes traffic, API server requests, logs/metrics, and orchestration tasks. |
| Worker nodes (HM)       | 1 Gbps+ per node       | For HM control components, PostgreSQL replication, internode communication, and metrics/logs.    |
| External bandwidth      | 1-20 Gbps (aggregated) | S3 backups and inter-cluster replication may require high throughput.                            |


## Network configuration and requirements

### Foundational planning and architecture

Make the high-level architectural decisions and plan your IP address space. 
These choices are the foundation for everything that follows.

- High-level network decisions: Decide on the fundamental characteristics of your network.

  - Network stack: Is your network IPv4 or Dual Stack?

  - Network type: Are you using a standard SDN or an OVN-based network?

  - DHCP: Will DHCP be available on the node network?

#### IP Addresses and CIDR planning

Based on the architecture, gather and allocate all necessary IP addresses and network ranges. 
This is your network blueprint.

##### Static IPs

Determine the API virtual IP, Ingress virtual IP, Default Gateway IP, DNS Server IP(s), and NTP Server IP(s).

##### Network ranges (CIDRs)

Determine the following CIDRs:

- Machine Network CIDR: For the physical or underlay network.

- Management Network CIDR: For administrative access.

- Cluster Network CIDR: For pod IP addresses within Kubernetes.

- Service Network CIDR: For internal Kubernetes services.

##### Downstream Cluster Space

Ensure you have a /16 IPv4 address space available for each downstream Kubernetes cluster, as this is a specific platform requirement that impacts your overall IP allocation.

### Core cluster networking

With the network plan in place, you can now set up the internal communication fabric for the Kubernetes cluster itself.

#### Configure container network interface (CNI)

A working CNI is essential for pods to communicate with each other. 
Ensure your chosen CNI ([Calico](https://docs.tigera.io/calico/latest/reference/configure-calico-node#configuring-cni-plugin) is preferred) is installed and configured correctly according to its documentation. 
This step makes the cluster's internal network functional.

### External access and firewall rules

Now that the cluster's internal network is running, configure how external traffic will reach the services running inside it.

#### Load balancer

A load balancer controller is required if using `LoadBalancer` type services.

If using an external load balancer, configure it to handle RKE2 API traffic and worker node communication.

**Port requirements**

Open the following ports on your firewall for the load balancer:

- 443
- 8444
- 9443
- 9445

#### NodePort configuration

If no load balancer is available, configure the `values.yaml` accordingly:

```yaml
beaconAgent:
  provisioning:
    loadBalancersEnabled: false
    nodePortDomain: "<your-node-port-domain>"
```

!!! Note
The `nodePortDomain` value is used as the URL for all Postgres instances.
It should be a DNS name pointing to the IP addresses of nodes where Postgres clusters are running.
!!!

**Port requirements**

Open up the following ports on your firewall for NodePort ingress:

- 32542
- 30288
- 30290
- 30292

If you need to change these values, do so in the `values.yaml` for these values:

- `ingress_http_node_port` - This is the the HTTP port of the HM Portal.
- `ingress_https_node_port` - This is the the HTTPS port of the HM Portal.
- `ingress_grpc_tls_node_port` - This is the the GRPC port for Beacon.
- `ingress_spire_tls_node_port` This is the the Spire TLS port.

and open up your firewall for the redefined ports.

###  DNS, TLS, and application configuration

With the external access path defined, you can now assign user-friendly names to your services and secure them.

#### DNS configuration

Point your public and internal domain nameds to the entry point you configured in the previous step.
-   Create a DNS A-record for your portal domain (portal_domain_name) pointing to the Load Balancer IP or the Node IPs covered by your `nodePortDomain`.
-   Ensure your `values.yaml` Helm chart is configured with the following parameters after deciding on a root domain name for HM:

  -   `portal_domain_name: "portal.<root_domain>`: The host name for the HM Portal.
  -   `server_host: "beacon.<root_domain>"`: The host name through which the beacon server API is reachable.
  -   `transporter-rw-service:domain_name: "transporter.<root_domain>": The domain name for the internal Transporter migration read/write service.
  -   `transporter-dp-agent:rw_service_url: "transporter.<root_domain>/transporter""`: The URL for the internal Transporter migration read/write service.

#### TLS certificate management

Secure your endpoints (such as the HM Portal) using [TLS certificates](product_docs/docs/edb-postgres-ai/1.3/hybrid-manager/install/customization/cert-man.mdx).

This requires DNS hostnames to be finalized so the certificates can be issued for the correct names.

[Custom certificates](product_docs/edb-postgres-ai/1.3/hybrid-manager/install/customization/cert-man/#set-up-a-custom-x509-certificate-for-the-hybrid-manager-portal) are strongly suggested, with auto-generated self-signed certificates as the default fallback option.

You can also set up a [custom cert-manager issuer](product_docs/edb-postgres-ai/latest/hybrid-manager/install/customization/cert-man/#set-up-a-custom-cert-manager-issuer-for-the-hm-portal) for the HM Portal and even setup your own [certificate authority](product_docs/edb-postgres-ai/latest/hybrid-manager/install/customization/cert-man/#bring-your-own-private-certificate-authority).

#### IDP setup

It is strongly recommended to [set up your identity provider](product_docs/docs/edb-postgres-ai/1.3/hybrid-manager/install/customization/byo_idp/index.mdx) (IDP) with HM for managing user access.
[Native users](product_docs/docs/edb-postgres-ai/1.3/hybrid-manager/using_hybrid_manager/managing_users/index.mdx) are supported by default, but managing users this way is not recommended in production.

### Network security policies

With traffic flowing and endpoints secured, implement fine-grained rules to control traffic flow both within the cluster and at its boundaries.

#### Network policies

Create Kubernetes `NetworkPolicy` resources to control which pods can communicate with each other. 
This is crucial for securing traffic between the control plane, data nodes, and other system components. 

### Ingress/egress rules

Harden the boundaries.
Validate and enforce ingress/egress rules on your firewall to ensure that only necessary traffic is allowed into and out of the cluster, restricting all other access.

## Sync EDB Postgres AI Platform container images into a customer owned registry

The software stack of our EDB PGAI is pushed into EDB Cloudsmith registry to provide artifacts for you to use in your local registry.

We require you to have your own secure and approved internal registry and know the EDB PGAI version that you want to install.
With this information, all the artifacts from Cloudsmith sync internally into your local registry before installing or upgrading the software stack with the Helm chart.

The sync process needs to preserve the container images' SHA256 to ensure images security and immutability across different environments. 

### Sync using edbctl

1. Ensure [edbctl is installed and configured](product_docs/edb-postgres-ai/hybrid-manager/using_hybrid_manager/edbctl/).

1. Configure the necessary environmental variables:

- Configure the EDB PGAI release to be taken:

  ```bash
  export EDBPGAI_RELEASE=<RELEASE_VERSION>
  ```

- Configure the EDB Cloudsmith access token:

  ```bash
  export CS_EDB_TOKEN=<CS_EDB_TOKEN>
  ```

- Configure the EDB Cloudsmith registry source:

  ```bash
  export EDB_SOURCE_REGISTRY=pgai-platform
  ```

1. Run the `sync-to-local-registry` command:

  ```bash
  edbctl image sync-to-local-registry \
      --destination-registry-uri "<LOCAL_REGISTRY_URI>" \
      --version "${EDBPGAI_RELEASE}" \
      --source-registry-username "${EDB_SOURCE_REGISTRY}" \
      --source-registry-password "${CS_EDB_TOKEN}" \
      --destination-registry-username "<LOCAL_REGISTRY_USER>" \
    --destination-registry-password "<LOCAL_REGISTRY_PWD>"
  ```

1. Sync the EDB PGAI Operator image to the destination registry:

  ```bash
  edbctl operator sync-to-local-registry \
      --destination-registry-uri "<LOCAL_REGISTRY_URI>" \
      --version "${EDBPGAI_RELEASE}" \
      --source-registry-username "${EDB_SOURCE_REGISTRY}" \
      --source-registry-password "${CS_EDB_TOKEN}" \
      --destination-registry-username "<LOCAL_REGISTRY_USER>" \
      --destination-registry-password "<LOCAL_REGISTRY_PWD>"
  ```

Your local registry is now synced with EDB's Cloudsmith registry.

## Namespaces and preliminary secrets

### Namespace for HM 

Create a dedicated namespace for HM components to ensure isolation and manageability.

### Preparation for object storage and general Kubernetes secrets

Create Kubernetes secrets for any required credentials , such as object storage credentials (example: `aws_secret_access_key`), database access tokens, or other sensitive information.

### ImagePullSecret namespace and required secrets

Use edbctl to create the ImagePullSecrect namespace and required secrets:

1. Create the necessary namespaces and pull secrets:

  ```bash
  edbctl image-pull-secret create \
    --username <container registry username> \
    --passowrd <container registry passowrd> \
    --registry <local registry URI>

1. When prompted with `Proceed? [y/N]` with the current Kubernetes context, select `y`.

  You should then see something like the following example:

  ```bash
  2025/02/10 10:10:10 Creating Kubernetes Namespaces and ImagePullSecrets with the provided credentials...
  2025/02/07 15:29:08 Namespaces and ImagePullSecrets creation completed
  ```

1. Create the ImagePullSecret list:

  ```bash
  edbctl image-pull-secret list
  ```

  You should then see something like the following example output:

    ```bash
    Current Kubernetes context is: <KubeContext>
    Namespace edbpgai-bootstrap: exists, all set!
      Secret edb-cred: exists, all set!
    Namespace upm-replicator: exists, all set!
      Secret edb-cred: exists, all set!
    ```

### Authentication and security keys

HM and its underlying components require secure authentication mechanisms to ensure proper communication between components and to protect sensitive data.

1.  Generate AES-256 encryption key.

    HM uses an AES-256 encryption to secure sensitive data during communication or at rest (for example, database credentials, tokens).
    To generate a random AES-256 encryption key:

    ```bash
    export AES_256_KEY=$(openssl rand -base64 32)
    ```

1.  Store the key in Kubernetes.

    To make the key accessible to HM and associated services, create a Kubernetes secret in the appropriate namespace:

    1.  Run the following command to create the secret:

        ```bash
        kubectl create secret generic hm-auth-key \
            --namespace <hm-namespace-created-above> \
            --from-literal=aes-256-key=$AES_256_KEY
        ```

    1.  Verify the secret:

        ```bash
        kubectl get secret hm-auth-key --namespace <hm-namespace>
        ```

### Optional KMS key setup

If you are planning on using Transparent Data Encryption (TDE), [configuring a Key Management Store](product_docs/docs/edb-postgres-ai/1.3/hybrid-manager/install/customization/kms_for_tde/index.mdx) (KMS) is required.

### Optional secrets

1. [Create a secret for GenAI Builder and configure DeltaLake object storage](/edb-postgres-ai/hybrid-manager/install/customization/genai_secret/) if you plan to use GenAI Builder.

1. [Create a secret for Catalog](/edb-postgres-ai/hybrid-manager/install/customization/data_catalog_secret/) if you are planning to use Catalog.

1. [Create custom secrets for Migration Portal](/edb-postgres-ai/hybrid-manager/install/customization/migration_portal_secrets/) if you want to secure internal communication for Migration Portal.

## Storage and cluster preparation

### Block storage

HM uses Block Storage for all primary, stateful workloads.
A strategic approach to using Storage Classes to manage performance and cost is recommended.

#### Workload segregation

Define and use custom Storage Classes to match the storage I/O profile to the specific type of workloads:

| HM component                                    | Storage type                   | I/O requirement                                            | Example optimization |
| ---------------------- | ----------------------------------------------- | ------------------------------ | ---------------------------------------------------------- |
| HM Control Plane (CP)                        | Internal DBs and microservices (such as Thanos)                                                | Moderate IOPS, High throughput                               | Standard SSD tier for internal state.                                                                                |
| HM Postgres data nodes                       | Primary database I/O                                                                           | High IOPS, low latency                                       | Premium/high-performance SSD tier (crucial for production)                                                           |                                                                                              |                                                              |                                                                                                                      |

#### Custom storage classes

Each individual database can and should potentially use its own Storage Class.
This is vital for HM to ensure cost-efficiency and performance isolation:

- Production OLTP/OLAP: Use a Storage Class configured for the highest IOPS and lowest latency.

- Development/testing: Use a cheaper Storage Class with lower performance to manage cloud/infrastructure costs.

- Metrics/log storage: Might prefer a class optimized for high throughput or cost-effectiveness.

### Cluster prerequisites and driver verification (Driver)

With your Block Storage strategy outlined (see previous step), you are ready to prepare the cluster for Block Storage and install the appropriate CSI driver.

#### Cluster prerequisites

Before deployment, ensure the Kubernetes environment (RKE2) is configured to handle dynamic volume provisioning and data protection.

#### CSI driver installation

The chosen Storage Class dictates the required Container Storage Interface (CSI) Driver.

Verify the following two types of drivers are installed and properly configured in your RKE2 cluster:

-   **Persistent storage driver**: Install the CSI driver that is compatible with your underlying storage backend (currently, [TopoVLM](https://github.com/topolvm/topolvm) is supported for local storage). This enables the creation of Persistent Volumes (PVs).

-   **Snapshot controller**: Configure the CSI Snapshot Controller (and its related `VolumeSnapshotClass`). This is mandatory for HM to perform consistent volume backups and recovery.

### Object storage configuration (MinIO setup)

HM requires S3-compatible Object Storage (like MinIO) for data protection and archival. This bucket stores all non-block-storage data:

- Postgres WALs and Backups (enabling Point-in-Time Recovery).

- Managed Storage Locations.

- Archived Logs and Metrics.

#### MinIO user and policy creation

Set up a dedicated bucket, user, and policy for the HM platform.

1. Download the [MinIO Client `mc`](https://min.io/docs/minio/linux/reference/minio-mc.html) and configure `mc` client for managing your MinIO instance.

1. Define envrionmental variables: Replace the bracketed values with your desired names and credentials.

  ```shell
  export MINIO_DEPLOYMENT_NAME=<minio_deployment_name> # deployment name to MinIO
  export BUCKET_NAME=<minio_bucket_name> # bucket name
  export AWS_ACCESS_KEY_ID=<minio_user_name> # user name
  export AWS_SECRET_ACCESS_KEY=<minio_user_password> # user's password
  export MINIO_POLICY_NAME=<minio_policy_name> # policy to MinIO
  ```

1. Create a [MinIO bucket](https://min.io/docs/minio/linux/reference/minio-mc/mc-mb.html) using the `mc mb` command.

1. Create a new user:

```bash
mc admin user add ${MINIO_DEPLOYMENT_NAME} ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY}
```

1. Create one MinIO policy:

```bash
cat << EOF > policy.json
{
    "Version" : "2012-10-17",
    "Statement": [
        {
            "Effect" : "Allow",
            "Action" : [
                "s3:*"
            ],
            "Resource" : [
                "arn:aws:s3:::${BUCKET_NAME}",
                "arn:aws:s3:::${BUCKET_NAME}/*"
            ]
        }
    ]
}
EOF
```

1. Apply the policy:

```bash
mc admin policy create ${MINIO_DEPLOYMENT_NAME} ${MINIO_POLICY_NAME} ./policy.json
```

1.  Attach the policy to the user: 

    ```shell 
    mc admin policy attach ${MINIO_DEPLOYMENT_NAME} ${MINIO_POLICY_NAME} --user ${AWS_ACCESS_KEY_ID}
    ```

#### Apply the secret for bucket access

After preparing the dedicated user for HM access to the bucket, create and apply the following secret that associates the created user with HM to provide access for object storage:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: edb-object-storage # name cannot be changed
  namespace: default # namespace cannot be changed
stringData:
    auth_type: credentials

    # Optional: Used only when the object storage server's certificate is not issued by a well-known CA
    #
    # Base64 string of the CA bundle for the certificate used by the object storage server
    aws_ca_bundle_base64: <aws_ca_bundle_base64>

    # Required: Endpoint URL to the object storage
    aws_endpoint_url_s3: <endpoint-url-to-object-storage>

    # Required: AWS Static Credentials - AWS_ACCESS_KEY_ID
    aws_access_key_id: <AWS_ACCESS_KEY_ID>

    # Required: AWS Static Credentials - AWS_SECRET_ACCESS_KEY
    aws_secret_access_key: <AWS_SECRET_ACCESS_KEY>

    # Required: Bucket name
    bucket_name: <bucket_name>

    # Required: Region of the bucket
    aws_region: <aws_region>

    # Optional: true or false
    # When server-side encryption is disabled, set this to true. By default, its value is false, indicating that server-side encryption is enabled.
    server_side_encryption_disabled: <boolean>
```

## Proceed to setting necessary environmental variables

With the cluster configured, proceed to the next phase of the pre-installation: [setting the necessary environmental variables for installation](setvariables.mdx).

