---
title: Enabling a self-hosted model for the Migration Portal AI Copilot
navTitle: Self-hosted model (AI Factory)
description: Learn how to enable a self-hosted AI Factory model to power the Migration Portal AI Copilot.
---

You can use a self-hosted AI Factory model to serve the AI Copilot. This example uses NVIDIA NIM to serve the requests and Llama 3 to process it and generate an answer.

- Chat completions: [nvidia/llama-3_3-nemotron-super-49b-v1](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1)
- Embeddings: [nvidia/llama-3.2-nv-embedqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2)

!!!warning 
    There are significant safety implications to consider when using self-hosted models with Migration Copilot.
   
    The models provided by third-party vendors like OpenAI amd Azure OpenAI include content filtering and other safeguards which are designed to reduce the risk of the model responding to, generating or contributing to unsafe content. When you use self-hosted models these additional protections are no longer present.

    In addition, because you are hosting the models, you now bear responsibility for the risks and potential liability associated with any unsafe behavior.

## Prerequisites

Prepare the resources your environment requires to deploy the Migration Portal AI Copilot with a self-hosted solution.

-   Ensure your Hybrid Manager installation has [GPU resources configured](/edb-postgres-ai/1.3/hybrid-manager/ai-factory/learn/how-to/setup-gpu/) (required for AI Factory).

-   You have administrative access to the HM environment.

-   Your organization has created a [chat completion and a text embeddings model](/edb-postgres-ai/1.3/ai-factory/pipeline/models/using-with/using-nvidia-nim/using-nim-in-your-environment/) with the Hybrid Manager's [AI Factory](/edb-postgres-ai/1.3/ai-factory/model/deployment/#where-to-start). They have provided the endpoints for each model, which you can set as environment variables.

    ```bash
    export COMPLETIONS_SVC=llama-3-3-nemotron-super-49b-v1
    export EMBEDDINGS_SVC=llama-3-2-nv-embedqa-1b-v2
    
    export COMPLETIONS_ENDPOINT=$(kubectl get inferenceservice $COMPLETIONS_SVC -o     jsonpath='{.status.url}')
    export EMBEDDINGS_ENDPOINT=$(kubectl get inferenceservice $EMBEDDINGS_SVC -o     jsonpath='{.status.url}')
    ```

## Enabling the AI Copilot

1.  Check if the `edb-migration-copilot` namespace exists:

    ```bash
    kubectl get namespaces edb-migration-copilot
    ```

    The namespace is created during the installation of the Hybrid Manager. If you are enabling the AI Copilot before installing the HM, you must create the namespace in advance.

1.  If the `edb-migration-copilot` namespace doesn't exist yet, create it:

    ```bash
    kubectl create ns edb-migration-copilot
    ```

1.  Set the following environment variables to link the secret with the model endpoints:

    ```bash
    export OPENAI_API_BASE=${COMPLETIONS_ENDPOINT}/v1
    export OPENAI_EMBEDDINGS_API_BASE=${EMBEDDINGS_ENDPOINT}/v1
    export OPENAI_API_KEY=<openai api key> # set to a placeholder value like `noop` if models are deployed in a way that no key is required
    ```

    !!!note
        The AI Copilot uses OpenAI-compatible APIs to communicate with all models, including self-hosted ones. This is why some configuration parameters contain `openai` in their names, even when you're using a different model to serve queries.

1.  Create the `ai-vendor-secrets` secret to point at the models' endpoints:

    ```bash
    kubectl create secret generic ai-vendor-secrets \
        --namespace=edb-migration-copilot \
        --type=opaque \
        --from-literal=AI_VENDOR=NIM \
        --from-literal=RAGCHEW_OPENAI_API_BASE="${OPENAI_API_BASE}" \
        --from-literal=RAGCHEW_OPENAI_EMBEDDINGS_API_BASE="$    {OPENAI_EMBEDDINGS_API_BASE}" \
        --from-literal=OPENAI_API_KEY="${OPENAI_API_KEY}"
    ```

1.  Create a new file called `migration-portal-values.yaml` with the following helm value to override the default AI vendor secrets with the secret created in the previous step.

    ```
    parameters:
      edb-migration-copilot:
        ai_vendor_secrets: ai-vendor-secrets
    ```

1.  Update the Hybrid Manager installation file to include the AI Copilot configuration. This involves either updating the YAML values you used for installation or running the `helm upgrade` with the AI Copilot configuration parameters.

    <TabContainer syncKey="installation_config">
    <Tab title="Helm or operator install">
    
    Add this configuration block to either the `values.yaml` file you used to install HM with Helm, or to     the CRD you used to install HM with the operator.
    
    ```yaml
    [...]
    parameters:
      edb-migration-copilot:
        ai_vendor_secrets: ai-vendor-secrets                    # Allows the default AI vendor secrets to     be overridden by the secret
                                                                # created above. This *must* match the     name of the above secret.
        chat_model: nvidia/llama-3.3-nemotron-super-49b-v1      # This must match the model name as     listed by `${COMPLETIONS_ENDPOINT}/v1/models`.
        embeddings_model: nvidia/llama-3.2-nv-embedqa-1b-v2     # This must match the model name as     listed by `${EMBEDDINGS_ENDPOINT}/v1/models`.
        chat_model_profile: llama3                              # Enables prompt rules and other     parameters which can help improve the quality of
                                                                # responses from chat completion models     of the llama3 family.
        embeddings_dimension: '"2048"'                          # This must match the size of the vectors     generated by the embeddings model.
        tokenizer: huggingface                                  # Causes tokenizers from the Hugging Face     Transformers library to be used for token counting.
        tokenizer_model: nvidia/llama-3.3-nemotron-super-49b-v1 # Sets the specific tokenizer to use,     ensuring it corresponds to a pretrained model on Hugging Face Hub.
        chat_request_max_tokens: '"1024"'                       # Limits the total number of requested     tokens to prevent excessively-verbose responses.
        default_similarity_limit: '"10"'                        # Limits the number of contextual chunks     included in queries to prevent overwhelming
                                                                # the model with input.
        stream_no_stop_sequence: '"true"'                       # Prevents OpenAI-specific stop sequences     being sent to the model. This should always be
                                                                # set when using NIM models in order to     avoid duplicated final chunks in response     streams.
    [...]
    ```
    
    </Tab>
    
    <Tab title="Helm upgrade">
    
    Run the `helm upgrade` command while including the following configuration parameters. Remember to include these parameters each subsequent time you invoke `helm upgrade`. Otherwise, your secrets will be overridden with default secrets.
    
    ```bash
    helm upgrade \
        -n edbpgai-bootstrap \
        --install \
        [...]
        --set parameters.edb-migration-copilot.ai_vendor_secrets=ai-vendor-secrets \
        --set parameters.edb-migration-copilot.chat_model=nvidia/llama-3.3-nemotron-super-49b-v1 \
        --set parameters.edb-migration-copilot.embeddings_model=nvidia/llama-3.2-nv-embedqa-1b-v2 \
        --set parameters.edb-migration-copilot.chat_model_profile=llama3 \
        --set parameters.edb-migration-copilot.embeddings_dimension='"2048"' \
        --set parameters.edb-migration-copilot.tokenizer=huggingface \
        --set parameters.edb-migration-copilot.tokenizer_model=nvidia/llama-3.3-nemotron-super-49b-v1 \
        --set parameters.edb-migration-copilot.chat_request_max_tokens='"1024"' \
        --set parameters.edb-migration-copilot.default_similarity_limit='"10"' \
        --set parameters.edb-migration-copilot.stream_no_stop_sequence='"true"' \
       [...]
    ```
    
    </Tab>
    </TabContainer>

1.  Restart the `edb-migration-copilot` services to trigger a reconciliation of the new values with the system.

    ```bash
    kubectl rollout restart edb-migration-copilot -n edb-migration-copilot
    ```
