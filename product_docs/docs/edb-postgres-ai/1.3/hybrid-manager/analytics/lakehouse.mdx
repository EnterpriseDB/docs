---
title: EDB Postgres Lakehouse Clusters on Hybrid Manager
navTitle: Lakehouse Clusters
description: Learn how Hybrid Manager provisions and manages EDB Postgres Lakehouse clusters for fast analytical queries on data stored in object storage.
---

EDB Postgres Lakehouse Clusters provide high-speed analytical compute for querying open lakehouse table formats (Apache Iceberg®, Delta Lake) in object storage.

Hub quick links: [Analytics Hub](/pgaa/1.3/) — [Concepts](/pgaa/1.3/) — [How-Tos](/pgaa/1.3)

Hybrid Manager (HM) enables you to provision and manage Lakehouse Clusters through its console and API, integrating them with your existing PGD clusters and broader analytics ecosystem.

For foundational concepts about EDB Postgres Lakehouse architecture, see [EDB Postgres Lakehouse: An Overview](/pgaa/1.3/).

## Why use Lakehouse Clusters in Hybrid Manager

- **Managed analytical compute:** Provision Lakehouse Clusters in HM to run fast queries on object storage.
- **Separation of compute and storage:** Keep data in object storage, scale analytical compute as needed.
- **Integration with PGD:** Use Lakehouse Clusters with PGD Tiered Tables and offloaded data.
- **Interoperability:** Lakehouse Clusters can query Iceberg and Delta Lake tables shared with other analytics engines.
- **Familiar Postgres interface:** Use standard SQL clients and tools with EDB Postgres Lakehouse Clusters.

## Key terms and architecture overview

For definitions of core analytics terms used in Hybrid Manager—such as PGFS, PGAA, Lakehouse Cluster, and Analytics Offload—see [Analytics concepts (hub)](/pgaa/1.3/).

## When should I use Lakehouse Clusters in Hybrid Manager?

Use Lakehouse Clusters in Hybrid Manager when you want to:

- **Run fast, scalable analytics** on large datasets stored in object storage.
- **Query data from PGD Tiered Tables** or offloaded PGD transactional data.
- **Integrate Postgres with external data lakes** built on Iceberg or Delta Lake.
- **Support BI tools and ad-hoc users** with Postgres SQL access to data lake content.
- **Manage analytical compute separately** from transactional databases—scale only when needed.

## Key capabilities of Lakehouse Clusters in Hybrid Manager

### Running fast queries on object storage

**What:** Run analytical queries on Apache Iceberg® and Delta Lake tables stored in object storage.

**Why:** Enable fast, scalable analytics on large datasets without moving them into Postgres storage.

**How:** Provision Lakehouse Clusters in HM and query data using PGAA with vectorized query execution.

**Where:** S3-compatible object storage buckets connected via PGFS.

How-To: [Read Iceberg/Delta with or without a catalog](/pgaa/1.3/)

### Supporting PGD Tiered Tables and offloading

**What:** Use Lakehouse Clusters as the query target for PGD offloaded and tiered data.

**Why:** Optimize PGD operational storage while keeping historical data queryable.

**How:** Configure PGD node groups for analytics replication to object storage in Iceberg format.

**Where:** Offloaded PGD data in Iceberg, queried through Lakehouse Clusters.

Learn concepts for [Tiered Tables](/pgaa/1.3/)

### Querying external data lakes

**What:** Query Iceberg or Delta Lake tables created by other tools (Spark, Trino, Flink).

**Why:** Avoid data duplication and ETL by querying external data lakes from Postgres.

**How:** Connect Lakehouse Clusters to external Iceberg catalogs or directly to Delta Lake storage.

**Where:** S3-compatible object storage, with optional catalog integration.

How-To: [Configure an Iceberg catalog connection](/pgaa/1.3/)
How-To: [Read Iceberg/Delta with or without a catalog](/pgaa/1.3/)

### Centralized management of analytical compute

**What:** Provision, monitor, and manage analytical clusters in HM alongside your PGD clusters.

**Why:** Manage analytical and transactional compute in one control plane.

**How:** Use the HM console or API to create and configure Lakehouse Clusters.

**Where:** Lakehouse Clusters run in cloud environments managed by HM.

How-To: [Create a Lakehouse cluster](/pgaa/1.3/)

## Getting started with Lakehouse Clusters in Hybrid Manager

To begin using Lakehouse Clusters with Hybrid Manager:

1. [Provision a Lakehouse cluster](/pgaa/1.3/).
2. [Configure PGFS storage locations](/edb-postgres-ai/1.3/ai-factory/pipeline/pgfs/) pointing to object storage.
3. (Optional) [Configure an Iceberg catalog connection](/pgaa/1.3).
4. Learn [Tiered Tables concepts](/pgaa/1.3/) if integrating with PGD.
5. Create Lakehouse tables or external readers for Iceberg/Delta data.
6. Query your data using Postgres clients.

## Related How-Tos

- [Create a Lakehouse cluster](/pgaa/1.3/)
- [Configure an Iceberg catalog connection](/pgaa/1.3/)
- [Read Iceberg/Delta with or without a catalog](/pgaa/1.3/)

## Next topic

[Apache Iceberg® in Hybrid Manager](./iceberg)
