---
title: Common workloads and use cases
navTitle: Use cases
description: Explore why organizations choose Sovereign AI and Data Factory over cloud-based platforms and the workloads it supports.
---

Sovereign AI and Data Factory is purpose-built for organizations that require strict control over data, infrastructure, and AI execution. It supports a wide range of workloads while remaining locally managed.

## Why use the engineered system over cloud?

Many modern workloads can run in cloud environmentsâ€”but not all organizations can or should rely on them. Sovereign AI and Data Factory exists to meet the needs of the following.

### Data sovereignty and compliance

- All data, models, and workloads remain inside your physical infrastructure.
- No external APIs or third-party storage are required.
- Enables deployment in financial, government, healthcare, and national security sectors.

### Regulated environments

- Operates with limited internet dependency.
- Supports deployment lifecycles.
- Enables LLM and AI use cases in environments previously closed to them.

### Unified platform for Postgres + AI

- Combines HA Postgres, vector search, and GPU inference in one system, controlled by your organization.
- Simplifies orchestration, monitoring, and support.
- Eliminates fragmented solutions and duplicate infrastructure.

### Predictable performance and cost

- No egress fees, throttling, or unpredictable GPU queues.
- Optimized I/O and compute across known hardware.
- Total cost of ownership is clear and controlled.

### Lifecycle-managed infrastructure

- Joint EDB + Supermicro support agreements.
- Hardware and software patches integrated and coordinated.
- No reliance on customer-led infrastructure management or DevOps.

## Supported workload patterns

The following workloads are supported end to end on the Sovereign AI and Data Factory system.

### High-availability Postgres

- Run mission-critical systems of record with automatic failover.
- Observability and lifecycle are baked in.

### Hybrid DBaaS

- Serve internal teams with multiple clusters from one interface.
- Role-based access control and cluster-level visibility.

### Embedding and vector pipelines

- Transform data into embeddings using local models.
- Store and query with pgvector in Postgres.
- No API keys or cloud endpoints required.

### Retrieval-augmented generation (RAG)

- Query structured internal data and pass it to an LLM.
- Host both retrieval and inference within your own boundary.

### On-premises inference

- Serve generative and embedding models on dedicated GPU hardware.
- Use standard containers (for example, NVIDIA NIM) to abstract model serving.

### Agentic AI workflows

- Build AI agents that execute multi-step tasks using internal tools and data.
- No external APIs or data leakage.
- Deploy interfaces in Slack, custom dashboards, or internal tools.
