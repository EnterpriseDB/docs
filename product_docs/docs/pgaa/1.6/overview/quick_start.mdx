---
title: Analytics Accelerator quickstart guide
navTitle: Quickstart
description: Install PGAA on a Postgres instance and query sample data.
redirects:
- /edb-postgres-ai/preview/analytics/quick_start/
---


In this quickstart guide, you will:

1. Install the PGAA extension on a standalone Postgres instance.
1. Create a storage location pointing to sample benchmark datasets in object storage.
1. Run a complex analytical query and verify that Seafowl engine is working.
1. Perform an analysis of the query plan.


## Prerequisites

- An environment running a Postgres instance. See [Compatibility](compatibility) for a full list of supported platforms.
- An EDB access token.

## Downloading and installing the package on your PostgreSQL instance

1. Download the package from the EDB repository:


<TabContainer syncKey="platform">

<Tab title="Debian/Ubuntu" active>

```shell
export EDB_SUBSCRIPTION_TOKEN=<your-token>
curl -1sSLf "https://downloads.enterprisedb.com/$EDB_SUBSCRIPTION_TOKEN/standard/setup.deb.sh" | sudo -E bash
sudo apt-get install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

<Tab title="CentOS/RHEL">

```shell
export EDB_SUBSCRIPTION_TOKEN=<your-token>
curl -1sSLf "https://downloads.enterprisedb.com/$EDB_SUBSCRIPTION_TOKEN/standard/setup.rpm.sh" | sudo -E bash
sudo dnf install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

</TabContainer>

Edit your `postgresql.conf` file to add `pgaa` to to your `shared_preload_libraries`, and enable the automatic management and startup of the Seafowl query engine via the Postgres background worker:

```ini
shared_preload_libraries = 'pgaa'
pgaa.autostart_seafowl = on
```

After saving the changes, restart your PostgreSQL service. Once restarted, log in to your Postgres instance and create the PGAA extension:

```sql
CREATE EXTENSION IF NOT EXISTS pgaa CASCADE;
```

Using `CASCADE` ensures that any required dependencies (like `PGFS`) are also created. Use `\dx` to verify that the extensions have been installed and check their versions.

You can also use the function `pgaa.pgaa_version()` for full version information:

```sql
SELECT pgaa.pgaa_version();
```


## Configuring a storage location

Create a storage location pointing to our public S3 bucket containing TPC-H benchmark data:

```sql
SELECT pgfs.create_storage_location(
    'quickstart-sample-data',
    's3://beacon-analytics-demo-data-us-east-1-prod',
    '{"skip_signature": "true", "region": "us-east-1"}'
);
```

Verify that the storage location was created:

```sql
SELECT pgfs.list_storage_locations();
```

Test connectivity and verify read access (the function must return `NULL`):

```sql
SELECT pgaa.test_storage_location ('quickstart-sample-data', false);
```

## Creating analytical tables

Create the following analytical tables. These tables map directly to Delta Lake files in our sample [benchmark datasets](../reference/datasets).

```sql
CREATE TABLE supplier () USING PGAA WITH (pgaa.storage_location = 'quickstart-sample-data', pgaa.path = 'tpch_sf_1/supplier', pgaa.format = 'delta');
CREATE TABLE lineitem () USING PGAA WITH (pgaa.storage_location = 'quickstart-sample-data', pgaa.path = 'tpch_sf_1/lineitem', pgaa.format = 'delta');
CREATE TABLE orders () USING PGAA WITH (pgaa.storage_location = 'quickstart-sample-data', pgaa.path = 'tpch_sf_1/orders', pgaa.format = 'delta');
CREATE TABLE nation () USING PGAA WITH (pgaa.storage_location = 'quickstart-sample-data', pgaa.path = 'tpch_sf_1/nation', pgaa.format = 'delta');
```

## Running an analytical query

This query identifies suppliers in Saudi Arabia who were the cause of delays in multi-supplier orders. This is a heavy analytical task involving large joins and multiple subqueries.

```sql
SELECT
    s_name,
    COUNT(*) AS numwait
FROM
    supplier,
    lineitem l1,
    orders,
    nation
WHERE
    s_suppkey = l1.l_suppkey
    AND o_orderkey = l1.l_orderkey
    AND o_orderstatus = 'F'
    AND l1.l_receiptdate > l1.l_commitdate
    AND EXISTS (
        SELECT
            *
        FROM
            lineitem l2
        WHERE
            l2.l_orderkey = l1.l_orderkey
            AND l2.l_suppkey <> l1.l_suppkey
    )
    AND NOT EXISTS (
        SELECT
            *
        FROM
            lineitem l3
        WHERE
            l3.l_orderkey = l1.l_orderkey
            AND l3.l_suppkey <> l1.l_suppkey
            AND l3.l_receiptdate > l3.l_commitdate
    )
    AND s_nationkey = n_nationkey
    AND n_name = 'SAUDI ARABIA'
GROUP BY
    s_name
ORDER BY
    numwait DESC,
    s_name
LIMIT 100;
```

## Analyzing the query plan

To see how PGAA accelerates this query, prepend `EXPLAIN` to the statement above and inspect the output:

```sql
                                                                                                                                                    QUERY PLAN                                                                                                                                                     
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 SeafowlDirectScan: Logical Plan
   Limit:  skip=0, fetch=100
     Sort:  numwait DESC NULLS FIRST, supplier.s_name ASC NULLS LAST
       Projection:  supplier.s_name, count(Int64(1)) AS count(*) AS numwait
         Aggregate:  groupBy=[[supplier.s_name]], aggr=[[count(Int64(1))]]
           Filter:  supplier.s_suppkey = l1.l_suppkey AND orders.o_orderkey = l1.l_orderkey AND orders.o_orderstatus = Utf8("F") AND l1.l_receiptdate > l1.l_commitdate AND EXISTS (<subquery>) AND NOT EXISTS (<subquery>) AND supplier.s_nationkey = nation.n_nationkey AND nation.n_name = Utf8("SAUDI ARABIA")
             Subquery: 
               Projection:  l2.l_orderkey, l2.l_partkey, l2.l_suppkey, l2.l_linenumber, l2.l_quantity, l2.l_extendedprice, l2.l_discount, l2.l_tax, l2.l_returnflag, l2.l_linestatus, l2.l_shipdate, l2.l_commitdate, l2.l_receiptdate, l2.l_shipinstruct, l2.l_shipmode, l2.l_comment
                 Filter:  l2.l_orderkey = outer_ref(l1.l_orderkey) AND l2.l_suppkey != outer_ref(l1.l_suppkey)
                   SubqueryAlias:  l2
                     TableScan:  lineitem
             Subquery: 
               Projection:  l3.l_orderkey, l3.l_partkey, l3.l_suppkey, l3.l_linenumber, l3.l_quantity, l3.l_extendedprice, l3.l_discount, l3.l_tax, l3.l_returnflag, l3.l_linestatus, l3.l_shipdate, l3.l_commitdate, l3.l_receiptdate, l3.l_shipinstruct, l3.l_shipmode, l3.l_comment
                 Filter:  l3.l_orderkey = outer_ref(l1.l_orderkey) AND l3.l_suppkey != outer_ref(l1.l_suppkey) AND l3.l_receiptdate > l3.l_commitdate
                   SubqueryAlias:  l3
                     TableScan:  lineitem
             Cross Join:  
               Cross Join:  
                 Cross Join:  
                   TableScan:  supplier
                   SubqueryAlias:  l1
                     TableScan:  lineitem
                 TableScan:  orders
               TableScan:  nation
(24 rows)
```

The presence of `SeafowlDirectScan` at the top of the plan indicates that PGAA has taken full control of the query execution, bypassing the standard PostgreSQL executor to run the query directly against the data lake.

Instead of pulling billions of rows into PostgreSQL to process them, the entire logical plan—including joins, filters, and aggregations—is pushed down into the optimized analytical engine.

- **Subquery pushdown**: Notice the `EXISTS` and` NOT EXISTS` subqueries on the `lineitem` table. These are not being executed one-by-one by Postgres. Instead, they are part of the global logical plan being processed in parallel against the Parquet files.

- **Vectorized joins**: The `Cross Join` nodes combined with the `Filter` higher up indicate that the engine is performing hash joins or nested loop joins at the storage layer.

- **Predicate pushdown**: Filters like `n_name = Utf8("SAUDI ARABIA")` and `o_orderstatus = Utf8("F")` are applied during the initial scan. This means only the relevant data is read from the lake, significantly reducing I/O.

- **Final aggregation and sort**: The `Aggregate` and the `Sort` happen at the end of the pipeline before the final 100 rows are handed back to the PostgreSQL client.


## Conclusion

You have successfully installed PGAA, configured it to reach cloud-based object storage, and verified that complex SQL logic—including subqueries and joins—is being natively offloaded.

By leveraging Seafowl DirectScan, your Postgres instance can now act as a gateway to petabyte-scale data lakes, providing analytical performance that scales independently of your transactional compute.






## Prerequisites

- An environment running a Postgres instance. See [Compatibility](compatibility) for a full list of supported platforms.
- An EDB access token.

## Downloading and installing the package on your PostgreSQL instance

1. Download the package from the EDB repository:

```shell
export EDB_SUBSCRIPTION_TOKEN=<your-token>
curl -1sSLf "https://downloads.enterprisedb.com/$EDB_SUBSCRIPTION_TOKEN/standard/setup.rpm.sh" | sudo -E bash
```

<TabContainer syncKey="platform">

<Tab title="Debian/Ubuntu" active>

```shell
sudo apt-get install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

<Tab title="CentOS/RHEL">

```shell
sudo dnf install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

</TabContainer>

Edit your `postgresql.conf` file to add `pgaa` to to your `shared_preload_libraries`, and enable the automatic management and startup of the Seafowl query engine via the Postgres background worker:

```ini
shared_preload_libraries = 'pgaa'
pgaa.autostart_seafowl = on
```

After saving the changes, restart your PostgreSQL service. Once restarted, log in to your Postgres instance and create the PGAA extension:

```sql
CREATE EXTENSION IF NOT EXISTS pgaa CASCADE;
```

Using `CASCADE` ensures that any required dependencies (like `PGFS`) are also created. Use `\dx` to verify that the extensions have been installed and check their versions.

You can also use the function `pgaa.pgaa_version()` for full version information:

```sql
SELECT pgaa.pgaa_version();
```


## Configuring a storage location

Create a storage location that points to our public bucket which contains sample benchmark datasets.

```sql
SELECT pgfs.create_storage_location(
'quickstart-sample-data',
's3://beacon-analytics-demo-data-us-east-1-prod',
'{"skip_signature": "true", "region": "us-east-1"}'
);
```

Verify that the storage location was created:

```sql
SELECT pgfs.list_storage_locations();
```

Test connectivity and verify read access (the function must return `NULL`):

```sql
SELECT pgaa.test_storage_location ('quickstart-sample-data', false);
```

## Creating local and remote tables

Create the table `warehouse_analytics`, which represents your historical, large-scale dimensional data. The parameter `pgaa.path` points to `tpcds_sf_10/warehouse`, one of our sample [benchmark datasets](../reference/datasets).

```sql
CREATE TABLE warehouse_analytics () 
USING PGAA WITH (
    pgaa.storage_location = 'quickstart-sample-data', 
    pgaa.path = 'tpcds_sf_10/warehouse' , 
    pgaa.format = 'delta'
    );
```

Create the table `warehouse_inventory` which represents your live, high-velocity transactional data—current stock levels or active shipments—which changes too frequently to be kept in the data lake.

```sql
CREATE TABLE warehouse_inventory (
    inventory_id SERIAL PRIMARY KEY,
    w_warehouse_sk BIGINT, -- This is our Join Key
    product_id INT,
    current_stock INT,
    last_updated TIMESTAMP DEFAULT NOW()
);
```

Insert sample data into it:

```sql
INSERT INTO warehouse_inventory (w_warehouse_sk, product_id, current_stock, last_updated)
SELECT 
    (id % 5) + 1,                     -- Distributes rows across warehouse_sk 1 to 5
    (random() * 1000 + 100)::int,     -- Random product_id between 100 and 1100
    (random() * 5000)::int,           -- Random stock levels up to 5000
    NOW() - (random() * interval '30 days') -- Random timestamps over the last month
FROM generate_series(1, 50000) AS id;
```

## Run analytical queries

Run the following query that aggregates the local inventory data and joins it against the remote warehouse metadata to see regional performance.

```sql
SELECT 
    w.w_state,
    w.w_warehouse_name,
    COUNT(DISTINCT i.product_id) as unique_products,
    SUM(i.current_stock) as total_stock_on_hand,
    AVG(i.current_stock)::numeric(10,2) as avg_stock_per_product
FROM warehouse_analytics w
JOIN warehouse_inventory i ON w.w_warehouse_sk = i.w_warehouse_sk
GROUP BY w.w_state, w.w_warehouse_name
ORDER BY total_stock_on_hand DESC;
```

In this scenario, PostgreSQL handles the fresh, transactional data (OLTP) while Seafowl manages the massive historical or reference datasets residing in object storage (OLAP). By leveraging this hybrid architecture, you avoid the cost and latency of moving heavy analytical data into your operational database. Instead, you gain the benefit of "to-the-second" accuracy from your local transactional tables combined with the scale of the data lake. Under the hood, the Analytics Accelerator utilizes DirectScan to pull only the specific columns required from the object storage, while PostgreSQL efficiently coordinates the join logic for a seamless unified output.

## Analyzing the query plan

Run the `EXPLAIN` command to verify how PostgreSQL and the Seafowl engine collaborate to execute a cross-engine join.

```sql
EXPLAIN SELECT 
    w.w_state,
    w.w_warehouse_name,
    COUNT(DISTINCT i.product_id) as unique_products,
    SUM(i.current_stock) as total_stock_on_hand,
    AVG(i.current_stock)::numeric(10,2) as avg_stock_per_product
FROM warehouse_analytics w
JOIN warehouse_inventory i ON w.w_warehouse_sk = i.w_warehouse_sk
GROUP BY w.w_state, w.w_warehouse_name
ORDER BY total_stock_on_hand DESC;
```

The output should look something like the below:

```sql
                                                                   QUERY PLAN                                                                    
-------------------------------------------------------------------------------------------------------------------------------------------------
 Sort  (cost=37536.03..37536.53 rows=200 width=96)
   Sort Key: (sum(i.current_stock)) DESC
   ->  GroupAggregate  (cost=33775.39..37528.39 rows=200 width=96)
         Group Key: w.w_state, w.w_warehouse_name
         ->  Sort  (cost=33775.39..34400.39 rows=250000 width=72)
               Sort Key: w.w_state, w.w_warehouse_name, i.product_id
               ->  Hash Join  (cost=25.43..1105.92 rows=250000 width=72)
                     Hash Cond: (i.w_warehouse_sk = w.w_warehouse_sk)
                     ->  Seq Scan on warehouse_inventory i  (cost=0.00..868.00 rows=50000 width=16)
                     ->  Hash  (cost=25.30..25.30 rows=10 width=68)
                           ->  Custom Scan (SeafowlCompatScan) on warehouse_analytics w  (cost=25.00..25.30 rows=10 width=68)
                                 SeafowlPlan: Logical Plan
                                   Projection:  r1.w_warehouse_sk, r1.w_warehouse_name, r1.w_state
                                     SubqueryAlias:  r1
                                       TableScan:  public.warehouse_analytics
                                   SeafowlQuery: select r1.w_warehouse_sk, r1.w_warehouse_name, r1.w_state from public.warehouse_analytics as r1
```

The plan is read from the bottom up:

1. The Seafowl component (`SeafowlCompatScan`): The bottom of the plan shows the work performed on your object storage:
    - `SeafowlQuery`: The Seafowl engine generates a optimized sub-query to fetch only the required columns (`w_warehouse_sk`, `w_warehouse_name`, `w_state`) from the remote Delta files.
    - `Custom Scan` (`SeafowlCompatScan`): Because this query involves a join with a local PostgreSQL table, PGAA uses CompatScan. This mode allows Seafowl to stream the remote results back to PostgreSQL so they can be joined with local data.

1. The local PostgreSQL component: Once the remote data is prepared, PostgreSQL takes over to handle the transactional side:
    - `Seq Scan` on `warehouse_inventory`: PostgreSQL performs a standard sequential scan on your local OLTP table to retrieve the live inventory counts.
    - `Hash Join`: PostgreSQL creates a hash table from the Seafowl results and joins it with the local inventory records based on the `w_warehouse_sk` key.

1. Final Processing
    - `GroupAggregate`: After the data is joined, PostgreSQL calculates the totals (`SUM`) and counts (`COUNT`) requested in the query.
    - `Sort`: The results are sorted by the calculated `total_stock_on_hand` in descending order.


This hybrid approach ensures you get the massive scale of a data lake without losing the real-time accuracy of your local Postgres data.




