---
title: Installing and configuring Analytics Accelerator
navTitle: Installing and configuring
description: Installation instructions for Analytics Accelerator.
---


Install the EDB Analytics Accelerator (PGAA) extension on your PostgreSQL cluster.

## Prerequisites

## Downloading and installing the package on your PostgreSQL cluster

1. Download the package from the EDB repository:

```shell
export EDB_SUBSCRIPTION_TOKEN=<your-token>
curl -1sSLf "https://downloads.enterprisedb.com/$EDB_SUBSCRIPTION_TOKEN/standard/setup.rpm.sh" | sudo -E bash
```

<TabContainer syncKey="platform">

<Tab title="Debian/Ubuntu" active>

```shell
sudo apt install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

<Tab title="CentOS/RHEL">

```shell
sudo dnf install -y postgresql-<version>-pgaa
```

Where `<version>` is your PostgreSQL version.

</Tab>

</TabContainer>


Log into your PostgreSQL instance and install the extension:

```sql
CREATE EXTENSION IF NOT EXISTS pgaa CASCADE;
```

Using `CASCADE` ensures that any required dependencies (like `PGFS`) are also created.

Verify that the extension have been install and their version:

```sql
\dx
```

## Configuring a storage location

- Creating PGFS storage locations
- Supported backends: AWS S3, Azure Blob/ADLS Gen2, GCS, local filesystem
- Authentication: IAM roles, static credentials, environment variables
- Public vs private buckets
- Testing storage access: `pgaa.test_storage_location()`


For a public bucket:

```sql
SELECT pgfs.create_storage_location(
    'my_lake_data', 
    's3://your-bucket-name/path', 
    '{"aws_skip_signature": "true"}'
);
```

For a private bucket (needs credentials):

```sql
SELECT pgfs.create_storage_location(
    'my_lake_data', 
    's3://your-bucket-name/path', 
    '{"aws_region": "us-east-1", "aws_access_key_id": "...", "aws_secret_access_key": "..."}'
);
```


Define a storage location in your PostgreSQL database that points to the bucket containing your analytical data. See [PGFS storage location](/edb-postgres-ai/latest/ai-factory/pipeline/pgfs/functions/#creating-a-storage-location) for function reference.

```sql
SELECT pgfs.create_storage_location(
    'my-s3-bucket', 
    's3://your-bucket-name/path', 
    '{"aws_region": "us-east-1"}'
);
```

The following example points to our sample public bucket: 

```sql
SELECT pgfs.create_storage_location(
'biganimal-sample-data',
's3://beacon-analytics-demo-data-us-west-2-prod',
'{"skip_signature": "true", "region": "us-west-2"}'
);
```

Create tables using the `PGAA` access method, and specifying:
- pgaa.storage_location
- pgaa.path
- pgaa.format 


Examples:

Point to parquet files:

```sql
CREATE TABLE public.customer_address () USING PGAA
    WITH (pgaa.storage_location = 'biganimal-sample-data', pgaa.path = 'tpcds_sf_10/customer_address', pgaa.format = 'parquet');
```


Create external table over Iceberg data:

```sql
CREATE TABLE sales_history () USING PGAA
WITH (
  pgaa.format = 'iceberg',
  pgaa.storage_location = 'lakehouse_storage',
  pgaa.path = 'warehouse/sales.db/transactions'
);
```


## Run queries

Query seamlessly across storage tiers:

```sql
SELECT
  DATE_TRUNC('month', sale_date) as month,
  SUM(amount) as revenue
FROM sales_history
WHERE sale_date >= '2024-01-01'
GROUP BY 1;
```
```

### Unified OLTP + OLAP Architecture

Combine operational and analytical workloads in a single platform:

```sql
-- Hot data in PostgreSQL
CREATE TABLE sales_current (
  sale_id BIGINT PRIMARY KEY,
  sale_date DATE,
  amount DECIMAL(10,2)
);

-- Cold data in lakehouse
CREATE TABLE sales_archive () USING PGAA
WITH (
  pgaa.format = 'iceberg',
  pgaa.managed_by = 'lakehouse_catalog',
  pgaa.catalog_table = 'sales_historical'
);

-- Unified view across tiers
CREATE VIEW sales_complete AS
SELECT * FROM sales_current
UNION ALL
SELECT * FROM sales_archive;
```


## Enabling seafowl