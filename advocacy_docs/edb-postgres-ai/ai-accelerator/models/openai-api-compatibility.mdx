---
title: "Using an OpenAI-compatible API with Pipelines"
navTitle: "OpenAI-compatible models"
description: "Using an OpenAI-compatible API with Pipelines by setting options and credentials."
---

To make use of an OpenAI-compliant API, you can use the embeddings or completions model providers. A retriever needs to encode first so you can only use the embeddings model provider with a retriever.

## Why use an OpenAI-compatible API?

Some examples of why you might want to use an OpenAI-compatible API include:

* If you have a local system running [Ollama](https://ollama.com) and you want that local system to handle embeddings. This assumes you've configured [Ollama as a server](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server).

* If you have access to a service that provides different or specifically tuned models, you can use it instead of other models.

## Creating the model

The starting point for this process is creating a model. When you create a model, you can pass `options` and `credentials` to the registration. The defaults point to the OpenAI service endpoint. By overriding the defaults, you can point to any service. 

This example creates a model that uses a local Ollama server:

```sql
select aidb.create_model(
'my_local_ollama',
'embeddings',
'{"model":"llama3.1", "url":"http://llama.local:11434/v1/embeddings", "dimensions":2000}'::JSONB,
'{"api_key":""}'::JSONB);
```

### Model name and model provider

The model name is the first parameter. For the example, it's set to `my_local_ollama`.

Specify the model provider as `embeddings`, which is the provider that defaults to using OpenAI servers. You can use the configuration parameter, to override this value to talk to any compliant server.

### Configuration

The next parameter is the configuration. This is a JSON string. When expanded, it has three parameters: the model, the url, and the dimensions.

```json
'{"model":"llama3.1", "url":"http://llama.local:11434/v1/embeddings", "dimensions":2000}'::JSONB
```

This example sets the model to [“llama3.3”](https://ollama.com/library/llama3.3), a relatively new and powerful model. Remember to run `ollama run llama3.3` to pull and start the model on the server.

The next JSON setting is the important one, overriding the endpoint that the aidb model will use. In this example:

* The server is running on a machine called `llama.local`.
* It has port 11434 (the default port for Ollama) open to service requests over HTTP (not HTTPS in this case).
* The path to the endpoint on the server is `/v1/embeddings`, which is the same as OpenAI.

Putting those components together results in `[`http://llama.local:11434/v1/embeddings`](http://art.local:11434/v1/embeddings","api_key":"","dimensions":2000}'::JSONB)` as the endpoint.

The last JSON parameter in this example is `"dimensions"`, which is a hint to the system about how many vector values to expect from the model. If you [look up llama3.3’s properties](https://ollama.com/library/llama3.3/blobs/4824460d29f2), you can see the `llama.embedding_length` value is 8192. The provider defaults to 1536 (with some hardwired exceptions, depending on the model), but it doesn’t know about llama3.3's max. Another factor is that [pgvector is limited to 2000 dimensions](https://github.com/pgvector/pgvector?tab=readme-ov-file#what-if-i-want-to-index-vectors-with-more-than-2000-dimensions). So you pass a  dimension value of 2000 in the configuration to get the maximum dimensions available with pgvector.

That completes the configuration parameter.

### Credentials

The last parameter is credentials, which is another JSON string. It’s usually used for carrying the `api_key` for the OpenAI service and any other necessary credential information. It isn't part of the configuration. By being separate, it can be securely hidden from users with lesser permissions. 

The ollama connection in this example doesn't need an `api_key`, but the model provider currently requires that one is specified. Specify an empty string for `api_key` to satisfy this requirement.

## Using the model

Use the model name you created earlier to use the model just like any other Pipelines model. This example shows how to use the model to get an embedding:

```sql
select aidb.encode_text('my_local_ollama','I like it');
```

Pipelines takes care of all the connection management, freeing you to focus on your data and the model results.
