---
title: AI Factory generic concepts
navTitle: Generic concepts
description: Industry concepts that underpin modern AI workloads, data architectures, and AI Factory capabilities.
---

AI Factory leverages modern AI concepts, architectures, and technologies to enable advanced data-driven and intelligent applications.
This page explains **generic AI concepts** that form the foundation of AI Factory’s design and capabilities.

For EDB-specific implementations, see [AI Factory concepts](../.ai-factory-concepts).
For definitions of key terms, see [AI Factory terminology](./terminology).

## Core AI concepts

### Machine learning (ML)

The ability of systems to learn from data and improve performance on specific tasks without explicit programming.

Common approaches:

- **Supervised learning** — Learn from labeled data (predict sales based on past sales data)
- **Unsupervised learning** — Find patterns in unlabeled data (customer segmentation)
- **Reinforcement learning** — Learn through interaction and rewards (training an AI to play a game)

**Infrastructure needs:** large data storage, compute for training (CPUs, GPUs), and scalable environments for inference.

### Deep learning (DL) & neural networks

A subfield of ML using deep neural networks with multiple layers.

Strengths:

- Excels at complex data types (images, audio, video, text)
- Powers applications such as image recognition, speech recognition, text generation

**Infrastructure needs:**
High-performance compute (GPUs or TPUs), optimized data pipelines, efficient serving infrastructure.

### Natural language processing (NLP)

AI techniques to understand and generate human language.

Common tasks:

- Text classification
- Sentiment analysis
- Machine translation
- Question answering
- Conversational AI interfaces

**Infrastructure needs:**
Powerful compute for large models, low-latency serving for real-time interactions.

### Large language models (LLMs)

Advanced NLP models based on deep learning architectures such as Transformers.

Key characteristics:

- Trained on massive text corpora
- Capable of generating coherent, context-aware text
- Support advanced applications such as chatbots, content generation, code generation

**Infrastructure needs:**
Extremely large compute and storage resources for training; optimized hardware (GPUs) for inference.

### Embeddings and vector databases

**Embeddings** are dense vector representations of data — capturing semantic meaning in a numerical space.

Applications:

- Semantic search
- Recommendation systems
- Anomaly detection
- Retrieval-augmented generation (RAG) for LLMs

**Infrastructure needs:**
Specialized vector databases with efficient approximate nearest neighbor (ANN) search algorithms.

See also: [Vector databases and semantic search](#vector-databases-and-semantic-search)

## AI for databases

### Intelligent database management

Use of ML to automate and optimize database operations:

- Automated performance tuning
- Intelligent query optimization
- Proactive resource management
- Predictive maintenance and self-healing capabilities

Goal: self-driving, highly optimized database infrastructure.

### In-database machine learning

Perform ML model training and inference **directly within the database**, reducing data movement and enabling:

- Faster insights
- Simplified architectures
- Real-time prediction capabilities
- Streamlined MLOps workflows

Example: using SQL to invoke ML models inside Postgres.

### Vector databases and semantic search

Databases optimized to store and query embeddings (vector representations of data).

Key features:

- Fast similarity search (approximate nearest neighbor)
- Support for high-dimensional data
- Scalability to billions of vectors

Common use cases:

- Semantic search
- Personalized recommendations
- Enhanced LLM-based applications (RAG)
- Anomaly detection

## AI for infrastructure

### AI-accelerated hardware (GPUs, TPUs)

Specialized hardware optimized for AI workloads.

- **GPUs** — General-purpose acceleration for training and inference
- **TPUs** — Custom ASICs optimized for deep learning
- **Other accelerators** — FPGAs, inference chips

Benefits:

- Orders-of-magnitude speedup for AI model training
- Low-latency, high-throughput inference
- Efficient resource utilization in AI clusters

### Leveraging cloud compute for AI workloads

Modern AI workloads rely heavily on cloud compute capabilities:

- Access to GPU and TPU instances on demand
- Managed Kubernetes services for scalable training and serving
- Optimized software stacks (CUDA, TensorFlow, PyTorch, JAX)
- Advanced networking for distributed model training
- Flexible hybrid and multi-cloud strategies

Key goals:

- Minimize cost
- Maximize flexibility
- Ensure portability of data and models across environments

## Related concepts

- [Vectorized query engines](../../analytics/learn/explained/generic-concepts#vectorized-query-engines)
- [Data lakehouse architectures](../../analytics/learn/explained/generic-concepts#data-lakehouse)
- [Separation of storage and compute](../../analytics/learn/explained/generic-concepts#separation-of-storage-and-compute)

## Next steps

To understand how these concepts power **EDB’s AI Factory**, see:

- [AI Factory concepts](../../ai-factory-concepts)
- [AI Factory terminology](./terminology)
