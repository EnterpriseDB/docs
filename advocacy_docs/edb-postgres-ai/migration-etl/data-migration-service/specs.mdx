---
title: Optimizing migration performance
navTitle: Optimization
description: Learn about recommendended specifications to improve the performance and speed of database migrations using EDB Hybrid Manager.
deepToC: true

---

You can leverage the EDB Data Migration Service (DMS), a service included with the Hybrid Manager (HM), to migrate your data from existing database(s) (Oracle, self-managed Postgres, etc.) to a HM-managed database cluster. EDB DMS supports two migration methods: snapshot and snapshot + streaming, allowing you to migrate your data with minimal to no downtime.

## Understanding migration workloads in HM

The EDB Data Migration Service runs within the same Kubernetes environment where your EDB Hybrid Manager is deployed (on-premises, AWS, Red Hat OpenShift, and Google Cloud). This means that most of the workloads required for migration operations are managed and optimized by your Kubernetes cluster.

Generally, you don't need to extensively fine-tune resources, as your Kubernetes environment meeting the general requirements for running the Hybrid Manager should ensure a smooth migration. However, for large datasets, optimizing specific components can significantly accelerate the process. 

The key components to consider for resource allocation are:

- **EDB DMS Reader:** This component runs on your source database's host machine.

- **Destination database cluster:** This can be the database managed by HM or a different external self-managed database, however here we'll focus on an HM-managed database. This is where your data will reside.

- **EDB DMS Writer:** This component is only required if your migration target is an external (self-managed) database cluster, and it runs on that database's host machine.

Whether you perform a snapshot or snapshot + streaming migration also affects performance differently: 

- **Snapshot migrations:** They primarily focus on efficiently transferring a large volume of existing data from a source database to the target. Performance for these is heavily influenced by total data size and sustained disk I/O.

- **Snapshot + streaming migrations:** They first transfer a snapshot of existing data and then continuously stream ongoing changes (transactions) from the source to the destination. Performance here is driven by the sustained transaction rate (TPS) and the ability to keep up with the source's write activity.

## Optimizing migration performance for large datasets

The HM and DMS are designed to handle resource usage automatically. For datasets smaller than 100 GB, they are configured to perform fast migrations without further tuning. However, for datasets larger than 100 GB, a few targeted alterations can lead to significantly better performance and quicker migrations.

### Tuning destination database performance

Optimizing the destination database cluster (whether an HM-managed database or a self-managed database) provides the most significant performance benefits from resource tuning.

!!!note 
   These recommendations are based on current testing and are subject to change. EDB continuously works to improve migration performance and reduce migration times.

<br/>

#### Storage recommendations

During a migration, storage IOPS can peak at 2000, with an average throughput of roughly 300 MB/s and peaks up to 350 MB/s. To accommodate these demands:

- Separate WAL and data volumes: Aim for storage volumes that can handle a minimum of 2500-3000 IOPS and at least 200-250 MB/s of throughput for both your data and Write-Ahead Log (WAL) volumes. Using separate volumes for WAL and data is highly recommended for optimal performance.

- Unified storage and WAL volume: A unified storage and WAL volume can adversely affect performance. For example, using a single volume with 3000 IOPS and 125 MB/s throughput can increase migration time by over 40%. If you must use a unified storage and WAL volume, we recommend a throughput of 300-400 MB/s. Keep in mind that if the WAL becomes full, the migration will fail.

<details><summary>How to adjust this?</summary> 

For HM-managed database clusters: 

- Adjust WAL and data volume allocation by [editing](/edb-postgres-ai/hybrid-manager/using_hybrid_manager/cluster_management/manage-clusters/edit-clusters/#editing-a-cluster) your cluster's **Storage**. 

- By default, HM-managed clusters use separate volumes for data and WAL volumes, but if you unselected the **Use a separate storage volume for Write-Ahead Logs** option during cluster creation, you won't be able to enable this setting post creation. 

- Volume speeds (IOPS or throughput) however, are controlled by the infrastructure provider, see [Request Amazon EBS volume notifications](https://docs.aws.amazon.com/ebs/latest/userguide/requesting-ebs-volume-modifications.html) or [Google Cloud - About Persistent Disk performance](https://cloud.google.com/compute/docs/disks/performance) for more information. 

</details> 

<br/>

#### Memory recommendations

While 5 GB of memory is sufficient, a larger amount will facilitate optimal performance during a migration. Use at least 10 GB of memory for the destination database.

<details><summary>How to adjust this?</summary>  

For HM-managed database clusters: 

- Adjust memory allocation by [editing](/edb-postgres-ai/hybrid-manager/using_hybrid_manager/cluster_management/manage-clusters/edit-clusters/#editing-a-cluster) your cluster's **Cluster Settings** > **Instance size**.

- Additionally, you can also adjust the `shared_buffers`, `effective_cache_size` and `wal_buffers` parameters in the **DB Configuration**.

</details> 

<br/>

#### CPU recommendations

The destination database cluster should have at least 4 CPU cores. A lower core count will negatively impact migration performance.

<details><summary>How to adjust this?</summary> 

For HM-managed database clusters, adjust CPU allocation by [editing](/edb-postgres-ai/hybrid-manager/using_hybrid_manager/cluster_management/manage-clusters/edit-clusters/#editing-a-cluster) your cluster's **Instance size**.

</details> 

<br/>

!!!note 
   Apply these optimizations for the duration of the migration process. After the migration is complete, you can adjust these values as needed based on your ongoing workload requirements.
   
Extensive testing has shown that the biggest factor affecting migration performance is volume speed (IOPS and throughput). While decreasing CPU below recommendations can reduce performance, it's not nearly to the degree that low IOPS and throughput for storage will. Halving the storage throughput can increase migration time by over 40%, whereas halving the CPU count to 2 cores might increase it by 10-15%. These recommendations are designed to yield the best migration performance; using lower values may still result in successful migrations but at a slower speed.

### Tuning EBD DMS Reader and Writer performance

Optimal performance for the EDB DMS Reader and Writer is achieved when the host machines where these components are installed follow these specifications.

#### EDB DMS Reader 

**CPU:** 3 CPU cores

**Memory:** 4 GB 

#### EDB DMS Writer

**CPU:** 2 CPU cores

**Memory:** 2.5 GB 

!!!note
   If both the EDB DMS Reader and EDB DMS Writer are installed on the same physical or virtual machine, ensure that the host machine has sufficient combined resources. For example, a machine hosting both would ideally have at least 5 CPU cores and 6.5 GB of memory to provide optimal performance for both components.

## Performance benchmarking for migrations

To provide a clear understanding of expected migration performance, EDB conducted a series of tests to benchmark migration speeds under various conditions. These tests involved migrating different numbers and sizes of tables. 

### Approach

All migration performance tests were conducted using an automated test suite. The testing infrastructure was hosted on an EC2 `t4i.4xlarge` instance. Resources on this instance were shared across three primary Docker containers:

**EDB DMS Reader container:** 4 CPU, 16 GB memory

**Test container:** 4 CPU, 16 GB memory

**Oracle database:** 8 CPU, 32 GB memory

Each migration originated from an Oracle database and targeted an HM-managed EDB Postgres Advanced Server (EPAS) cluster. The HM-managed was hosted on a Hybrid Manager deployment running on an `m6a.2xlarge` instance.

### EPAS database cluster configuration

**Volumes:** Minimum 3,000 IOPS and 500 MB/s throughput.

**Storage:** Separate storage and WAL volumes were used for each test on the EPAS cluster, with volumes being approximately the same size.

**GUC Settings:** Default GUC settings were used on the PostgreSQL cluster. Further testing with increased memory allocations and fine-tuned GUC values did not show a significant impact on migration speed.

### Migration data

The actual migrations utilized multiple tables. Each table contained two columns: one serving as the primary key and the other populated with random integers. A combination of snapshot-only and snapshot + streaming migrations were performed, with varying amounts of streaming data inserted at different rates.

### Migration procedure

1. Populated the Oracle database with the specified amount of tables.

1. Initiated the migration process.

1. Inserted streaming data continuously into the Oracle database.

### Results 

For the current release, only the results of 1 x 100 GB + snapshot migration testing are published (a single 100 GB table migrated without streaming enabled). However, the conclusions gathered in this topic are based on comprehensive testing across various configurations (multiple tables, different table sizes, streaming enabled and disabled etc.). Future releases will include published results for these additional migration examples. 

| Nr. of tables | Snapshot size (GB) | Total data size (GB) | Streaming enabled | Streaming TPS | Total streaming data size (GB) | EPAS resources (CPU/memory in GB) | Time to finish (mins) | Average transfer speed (GB/min) | Max. target CPU/ memory/ IOPs usage |
|:--------------|:-------------------|:---------------------|:------------------|:--------------|:-------------------------------|:----------------------------------|:----------------------|:--------------------------------|:------------------------------------|
| 1             | 100                | 100                  | False             | N/A           | N/A                            | 4/10                              | 56.84±0.455           | 1.89                            | 80%/ 91%/ 1.2k                      |

<br/>

#### Conclusions

- Migration time for snapshot-only migrations generally scales linearly with both table count and total data size. The overall size of the source database is the primary determinant for migration duration in these scenarios.

- Average transfer speeds for snapshot-only migrations ranged from approximately 1.85 to 1.93 GB/min.

- For **snapshot + streaming** migrations, average transfer speeds ranged from 1.37 to 1.9 GB/min.

- Migration speed decreases as the streaming data insertion size and Transactions Per Second (TPS) increase. The highest throughput (1.9 GB/min) was observed with the lowest insertion TPS and data size, while the lowest throughput (1.37 GB/min) corresponded to the largest insertion TPS and data size.
