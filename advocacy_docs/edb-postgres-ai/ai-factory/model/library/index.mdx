---
title: Model Library (AI Factory)
navTitle: Model Library
description: Discover, manage, and deploy AI models in AI Factory using the Model Library, powered by the Image and Model Library in Hybrid Control Plane.
---

# Model Library

The **Model Library** is your central resource in AI Factory for discovering, managing, and deploying AI models.
It provides the foundation for Model Serving across the AI Factory ecosystem.

The Model Library is **powered by the Image and Model Library** in Hybrid Control Plane (HCP), which offers unified governance of all container images — both database images and AI model images — used across your HCP environment.

→ [Learn more about the Image and Model Library](/image-and-model-library).

---

## What does the Model Library provide?

- **Discover containerized AI models** ready for deployment via Model Serving.
- **Select optimized GPU models** to match your use case and hardware.
- **Integrate with Knowledge Bases** to drive vector search and RAG applications.
- **Support Gen AI Builder** → Assistants and Retrievers use these models for inference.
- **Leverage private registries** → Bring your own models and manage their lifecycle.

---

## Where does the Model Library fit in AI Factory?

The Model Library powers Model Serving for AI Factory, supporting:

- **AIDB Knowledge Bases** → embedding models, rerankers, OCR.
- **Gen AI Builder** → chat completion models with tool calling, retrievers.
- **Custom AI Factory workflows** → any Griptape pipeline or agent using Model Serving.

Typical usage flow:

1. **Discover model images** → via Model Library (powered by Image Library).
2. **Deploy models** → to Model Serving via KServe.
3. **Consume models** → in Knowledge Bases, Assistants, and AI Factory pipelines.

---

## Core Concepts

- **Repository** → AI model family (ex: `meta/llama-3.3-49b-nemotron-super`).
- **Image Tag** → Model version (ex: `1.8.5`, `latest`).
- **GPU Requirements** → Defined per model; informs node sizing.
- **Serving Runtime** → Managed via KServe integration.

→ [KServe Concepts](../../../learn/explained/kserve-explained) (placeholder)

---

## Guides and How-Tos

### Deploying AI models

- [Deploy AI models using Library images](../../../how-to/gen-ai/image-library/deploy-ai-models)
- [Setup GPU resources for Model Serving](../../../how-to/gen-ai/model-serving/setup-gpu)

### Managing registries and repositories

- [Integrate private registry for custom models](../../../how-to/gen-ai/image-library/integrate-private-registry)
- [Define repository rules for Model Library](../../../how-to/gen-ai/image-library/define-repository-rules)

### Running advanced AI workloads

- [GPU Model Serving Runbook](../../../tutorials/gen-ai/gpu-serving-runbook)
- [Provision GPU nodes for Model Serving](../../../how-to/gen-ai/model-serving/setup-gpu)

---

## Key Models and Usage Patterns

In HCP 1.2, these model types are validated and recommended:

| Model Type        | Example Model Image                                   | Used by |
|-------------------|-----------------------------------------------------|---------|
| **Chat Completion** | `llama-3.3-nemotron-super-49b`                       | Gen AI Builder |
| **Text Embedding**  | `arctic-embed-l`                                     | AIDB Knowledge Base, Gen AI Builder |
| **Text Reranking**  | `llama-3.2-nv-rerankqa-1b-v2`                        | AIDB Knowledge Base, Gen AI Builder |
| **Image Embedding** | `nvclip`                                            | AIDB Knowledge Base |
| **OCR**            | `paddleocr`                                         | AIDB Knowledge Base |

→ See full details in [Deploy AI models using Library images](../../../how-to/gen-ai/image-library/deploy-ai-models).
→ Recommended node sizing in [GPU Model Serving Runbook](../../../tutorials/gen-ai/gpu-serving-runbook).

---

## Related Reading

- [AI Factory Overview](../../../learn/explained/ai-factory-concepts)
- [GPU Configuration and Runbook](../../../tutorials/gen-ai/gpu-serving-runbook)
- [Image and Model Library (HCP Hub)](/image-and-model-library)

---

## Future Topics (Coming Soon)

- Advanced serving workflows with multiple runtimes.
- Using vLLM for model files (non-container serving).
- Supporting hybrid deployment patterns (vLLM + container serving).
- Automating model promotion pipelines via CI/CD.

---

