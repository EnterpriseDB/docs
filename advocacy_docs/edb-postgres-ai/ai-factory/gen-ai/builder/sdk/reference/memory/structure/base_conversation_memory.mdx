---
title: base_conversation_memory
navTitle: BaseConversationMemory

---

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory"></span>

Bases:
 [`SerializableMixin`](../../mixins/serializable_mixin.mdx#griptape.mixins.serializable_mixin.SerializableMixin "SerializableMixin (griptape.mixins.serializable_mixin.SerializableMixin)")
, <span title="abc.ABC">ABC</span>

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
@define
class BaseConversationMemory(SerializableMixin, ABC):
    conversation_memory_driver: BaseConversationMemoryDriver = field(
        default=Factory(lambda: Defaults.drivers_config.conversation_memory_driver), kw_only=True
    )
    runs: list[Run] = field(factory=list, kw_only=True, metadata={"serializable": True})
    meta: dict[str, Any] = field(factory=dict, kw_only=True, metadata={"serializable": True})
    autoload: bool = field(default=True, kw_only=True)
    autoprune: bool = field(default=True, kw_only=True)
    max_runs: Optional[int] = field(default=None, kw_only=True, metadata={"serializable": True})

    def __attrs_post_init__(self) -> None:
        if self.autoload:
            self.load_runs()

    def before_add_run(self) -> None:
        pass

    def add_run(self, run: Run) -> BaseConversationMemory:
        self.before_add_run()
        self.try_add_run(run)
        self.after_add_run()

        return self

    def after_add_run(self) -> None:
        if self.max_runs:
            while len(self.runs) > self.max_runs:
                self.runs.pop(0)
        self.conversation_memory_driver.store(self.runs, self.meta)

    @abstractmethod
    def try_add_run(self, run: Run) -> None: ...

    @abstractmethod
    def to_prompt_stack(self, last_n: Optional[int] = None) -> PromptStack: ...

    def load_runs(self) -> list[Run]:
        runs, meta = self.conversation_memory_driver.load()
        self.runs.extend(runs)
        self.meta = dict_merge(self.meta, meta)

        return self.runs

    def add_to_prompt_stack(
        self, prompt_driver: BasePromptDriver, prompt_stack: PromptStack, index: Optional[int] = None
    ) -> PromptStack:
        """Add the Conversation Memory runs to the Prompt Stack by modifying the messages in place.

        If autoprune is enabled, this will fit as many Conversation Memory runs into the Prompt Stack
        as possible without exceeding the token limit.

        Args:
            prompt_driver: The Prompt Driver to use for token counting.
            prompt_stack: The Prompt Stack to add the Conversation Memory to.
            index: Optional index to insert the Conversation Memory runs at.
                   Defaults to appending to the end of the Prompt Stack.
        """
        num_runs_to_fit_in_prompt = len(self.runs)

        if self.autoprune:
            should_prune = True
            temp_stack = PromptStack()

            # Try to determine how many Conversation Memory runs we can
            # fit into the Prompt Stack without exceeding the token limit.
            while should_prune and num_runs_to_fit_in_prompt > 0:
                temp_stack.messages = prompt_stack.messages.copy()

                # Add n runs from Conversation Memory.
                # Where we insert into the Prompt Stack doesn't matter here
                # since we only care about the total token count.
                memory_inputs = self.to_prompt_stack(num_runs_to_fit_in_prompt).messages
                temp_stack.messages.extend(memory_inputs)

                # Convert the Prompt Stack into tokens left.
                tokens_left = prompt_driver.tokenizer.count_input_tokens_left(
                    prompt_driver.prompt_stack_to_string(temp_stack),
                )
                if tokens_left > 0:
                    # There are still tokens left, no need to prune.
                    should_prune = False
                else:
                    # There were not any tokens left, prune one run and try again.
                    num_runs_to_fit_in_prompt -= 1

        if num_runs_to_fit_in_prompt:
            memory_inputs = self.to_prompt_stack(num_runs_to_fit_in_prompt).messages
            if index is None:
                prompt_stack.messages.extend(memory_inputs)
            else:
                prompt_stack.messages[index:index] = memory_inputs

        return prompt_stack
```

</details>

-   `autoload = field(default=True, kw_only=True)` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.autoload"></span> 

-   `autoprune = field(default=True, kw_only=True)` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.autoprune"></span> 

-   `conversation_memory_driver = field(default=Factory(lambda: Defaults.drivers_config.conversation_memory_driver), kw_only=True)` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.conversation_memory_driver"></span> 

-   `max_runs = field(default=None, kw_only=True, metadata={'serializable': True})` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.max_runs"></span> 

-   `meta = field(factory=dict, kw_only=True, metadata={'serializable': True})` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.meta"></span> 

-   `runs = field(factory=list, kw_only=True, metadata={'serializable': True})` <small>class-attribute</small> <small>instance-attribute</small>  <span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.runs"></span> 

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.__attrs_post_init__"></span>

### **attrs_post_init**()

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def __attrs_post_init__(self) -> None:
    if self.autoload:
        self.load_runs()
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.add_run"></span>

### add_run(run)

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def add_run(self, run: Run) -> BaseConversationMemory:
    self.before_add_run()
    self.try_add_run(run)
    self.after_add_run()

    return self
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.add_to_prompt_stack"></span>

### add_to_prompt_stack(prompt_driver, prompt_stack, index=None)

Add the Conversation Memory runs to the Prompt Stack by modifying the messages in place.

If autoprune is enabled, this will fit as many Conversation Memory runs into the Prompt Stack as possible without exceeding the token limit.

#### Parameters

| Name            | Type                                                                                                                                                                                                  | Description                                                                                                          | Default    |
| --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ---------- |
| `prompt_driver` | <a href="../../drivers/prompt#griptape.drivers.prompt.BasePromptDriver" title="BasePromptDriver (griptape.drivers.prompt.BasePromptDriver)"><code class="language-python">BasePromptDriver</code></a> | The Prompt Driver to use for token counting.<br/>                                                                    | `required` |
| `prompt_stack`  | <a href="../../common#griptape.common.PromptStack" title="PromptStack (griptape.common.PromptStack)"><code class="language-python">PromptStack</code></a>                                             | The Prompt Stack to add the Conversation Memory to.<br/>                                                             | `required` |
| `index`         | `Optional[int]`                                                                                                                                                                                       | Optional index to insert the Conversation Memory runs at. Defaults to appending to the end of the Prompt Stack.<br/> | `None`     |

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def add_to_prompt_stack(
    self, prompt_driver: BasePromptDriver, prompt_stack: PromptStack, index: Optional[int] = None
) -> PromptStack:
    """Add the Conversation Memory runs to the Prompt Stack by modifying the messages in place.

    If autoprune is enabled, this will fit as many Conversation Memory runs into the Prompt Stack
    as possible without exceeding the token limit.

    Args:
        prompt_driver: The Prompt Driver to use for token counting.
        prompt_stack: The Prompt Stack to add the Conversation Memory to.
        index: Optional index to insert the Conversation Memory runs at.
               Defaults to appending to the end of the Prompt Stack.
    """
    num_runs_to_fit_in_prompt = len(self.runs)

    if self.autoprune:
        should_prune = True
        temp_stack = PromptStack()

        # Try to determine how many Conversation Memory runs we can
        # fit into the Prompt Stack without exceeding the token limit.
        while should_prune and num_runs_to_fit_in_prompt > 0:
            temp_stack.messages = prompt_stack.messages.copy()

            # Add n runs from Conversation Memory.
            # Where we insert into the Prompt Stack doesn't matter here
            # since we only care about the total token count.
            memory_inputs = self.to_prompt_stack(num_runs_to_fit_in_prompt).messages
            temp_stack.messages.extend(memory_inputs)

            # Convert the Prompt Stack into tokens left.
            tokens_left = prompt_driver.tokenizer.count_input_tokens_left(
                prompt_driver.prompt_stack_to_string(temp_stack),
            )
            if tokens_left > 0:
                # There are still tokens left, no need to prune.
                should_prune = False
            else:
                # There were not any tokens left, prune one run and try again.
                num_runs_to_fit_in_prompt -= 1

    if num_runs_to_fit_in_prompt:
        memory_inputs = self.to_prompt_stack(num_runs_to_fit_in_prompt).messages
        if index is None:
            prompt_stack.messages.extend(memory_inputs)
        else:
            prompt_stack.messages[index:index] = memory_inputs

    return prompt_stack
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.after_add_run"></span>

### after_add_run()

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def after_add_run(self) -> None:
    if self.max_runs:
        while len(self.runs) > self.max_runs:
            self.runs.pop(0)
    self.conversation_memory_driver.store(self.runs, self.meta)
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.before_add_run"></span>

### before_add_run()

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def before_add_run(self) -> None:
    pass
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.load_runs"></span>

### load_runs()

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
def load_runs(self) -> list[Run]:
    runs, meta = self.conversation_memory_driver.load()
    self.runs.extend(runs)
    self.meta = dict_merge(self.meta, meta)

    return self.runs
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.to_prompt_stack"></span>

### to_prompt_stack(last_n=None)abstractmethod

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
@abstractmethod
def to_prompt_stack(self, last_n: Optional[int] = None) -> PromptStack: ...
```

</details>

<span id="griptape.memory.structure.base_conversation_memory.BaseConversationMemory.try_add_run"></span>

### try_add_run(run)abstractmethod

<details><summary>Source Code in <code>griptape&#47;memory&#47;structure&#47;base_conversation_memory.py</code></summary>

```python
@abstractmethod
def try_add_run(self, run: Run) -> None: ...
```

</details>
