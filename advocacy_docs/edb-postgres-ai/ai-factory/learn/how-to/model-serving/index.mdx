---
title: Model Serving How-To Guides
navTitle: Model Serving How-To Guides
description: Practical guides for deploying, configuring, and managing model serving in AI Factory using KServe.
---

# Model Serving How-To Guides

This section provides step-by-step guides to help you deploy, configure, and manage models with the AI Factory Model Serving capability.

These guides focus on real-world tasks you will perform when using KServe-based model serving within Hybrid Manager Kubernetes infrastructure.

## Getting started

-   [Model Serving Quickstart](../../learn/model-serving/quickstart)

## Deployment

-   [Configure KServe ServingRuntime](./configure-servingruntime)
-   [Create an InferenceService](./create-inferenceservice)
-   [Deploy NVIDIA NIM Containers](./deploy-nim-container)

## GPU configuration and tuning

-   [Update GPU resources](./update-gpu-resources)

## Monitoring and observability

-   [Monitor InferenceService](./monitor-inferenceservice)

* * *

## Related Concepts

-   [Model Serving Explained](../../learn/explained/model-serving-explained)
-   [Supported Models](../../models/supported-models)

* * *

For broader architecture context, see [AI Factory Concepts](/edb-postgres-ai/ai-factory/learn/explained/ai-factory-concepts/#model-serving-kserve).
