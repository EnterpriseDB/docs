give me copy pastable block, in markdown like you have before


---
title: KServe Concepts
navTitle: KServe Concepts
description: Learn how KServe enables Kubernetes-native, scalable model serving in AI Factory.
---

# KServe Concepts: Kubernetes-Native Model Serving

Use this guide to understand the core concepts of KServe, a standard for serverless inferencing on Kubernetes. Understanding these concepts helps you deploy, manage, and scale machine learning models within AI Factory.

## What is KServe?

KServe is an open-source, Kubernetes-native platform that provides a standardized, scalable, and simplified way to deploy and manage machine learning models in production. It builds on Kubernetes, Knative (optional, for serverless scaling), and Istio (optional, for advanced traffic management).

## Purpose in model serving

KServe simplifies the process of taking trained machine learning models and making them available as reliable, scalable inference services. It provides a consistent deployment experience across different model frameworks.

AI Factory uses KServe as the model serving engine for Model Serving, enabling scalable, GPU-accelerated model inferencing on Kubernetes.

## Core architectural principles

### Standardization

KServe defines a standard InferenceService Custom Resource Definition (CRD), providing a consistent interface for deploying models.

### Scalability

KServe leverages Kubernetes autoscaling and optionally Knative for serverless scaling. Models scale automatically based on demand.

### Serverless inferencing

KServe can scale models to zero when idle and back up when needed (with Knative integration). This optimizes resource usage.

### Pluggable and extensible

KServe supports a wide range of model servers and allows for adding pre-processing and post-processing steps through Transformers, and model explainability through Explainers.

## Key components and Custom Resource Definitions (CRDs)

### InferenceService

The primary CRD for deploying a model. It includes:

- **Predictor**: The core model serving component.
- **Transformer** (optional): Pre-processing or post-processing service.
- **Explainer** (optional): Generates model explanations.

### Predictor

Runs the model server and handles inference requests. It specifies:

- Model server to use (e.g., NVIDIA Triton, MLServer).
- Model location (e.g., S3, GCS, container image).
- Resource allocations (CPU, memory, GPU).
- Autoscaling parameters.

### Transformer (optional)

Handles input transformations before inference and output transformations after inference.

### Explainer (optional)

Provides explanations of model predictions using explainability tools.

### ServingRuntime / ClusterServingRuntime

Define templates for model server pods:

- **ServingRuntime**: Namespace-scoped.
- **ClusterServingRuntime**: Cluster-scoped, reusable across namespaces.

They define the container image, environment variables, resource limits, and supported model formats.

### Model server

The software that loads the model and exposes the inference endpoint (e.g., NVIDIA Triton, TorchServe, TensorFlow Serving).

## How KServe works

1. You apply an InferenceService manifest to the Kubernetes cluster.
2. The KServe controller creates the necessary Kubernetes resources:
- Pods, Services, and autoscaling resources.
3. The model server pod loads the model and serves inference requests.
4. A client sends inference requests to the service URL.
5. Requests optionally pass through a Transformer and/or Explainer before returning results.

## Key features and capabilities

- **Multi-framework support**: TensorFlow, PyTorch, scikit-learn, XGBoost, ONNX, TensorRT, and more.
- **GPU acceleration**: Native support for NVIDIA GPUs and Kubernetes accelerators.
- **Autoscaling**: Including scale-to-zero with Knative.
- **Standard inference protocols**: V2 Inference Protocol (gRPC/HTTP).
- **Canary rollouts**: Traffic shifting for safe model updates.
- **Observability**:
- Prometheus metrics.
- Logging via Kubernetes.
- Integration with tracing systems.
- **Batching**: Improves throughput with server-side batching.
- **Model explainability**: Supports various explainability tools.

## Role in AI Factory

In AI Factory:

- KServe serves as the core model serving layer.
- AI Factory simplifies KServe deployment and management.
- The HCP Model Library integrates with KServe for model discovery and deployment.
- GPU resource management is integrated with KServe deployments.
- KServe services power Gen AI Builder agents and AIDB Knowledge Bases.
- Centralized observability is provided through AI Factory observability tooling.

## Further reading

- [KServe Official Documentation](https://kserve.github.io/website/)
- [KServe GitHub Repository](https://github.com/kserve/kserve)
- [AI Factory Model Serving Quickstart](../model-serving/quickstart)
