---
title: Model Serving Explained
navTitle: Model Serving Explained
description: Understand how Model Serving works in AI Factory, using KServe to provide scalable, production-ready model inference.
---

# Model Serving Explained

Model Serving in AI Factory allows you to deploy AI models as scalable, production-grade inference services.

It provides a standardized serving architecture based on Kubernetes and KServe, giving your models the ability to serve predictions and embeddings over network-accessible APIs.

AI Factory Model Serving is optimized to support enterprise-class AI workloads with:

- GPU-accelerated infrastructure
- Flexible scaling
- Monitoring and observability
- Seamless integration with other AI Factory capabilities

## Key Concepts

### KServe in AI Factory

Model Serving is powered by **KServe**, a popular open-source model serving engine for Kubernetes.
KServe enables:

- Serving models as standardized Kubernetes resources called `InferenceService`.
- Auto-scaling and resource management.
- REST and gRPC APIs for model consumption.
- Support for a wide range of model formats.

In AI Factory version 1.2, Model Serving focuses on streamlined support for **NVIDIA NIM** containers.
Future releases will add expanded support for additional model types and formats.

### Model Serving Stack

| Layer | Purpose |
|-------|---------|
| AI Factory | Provides infrastructure and Model Serving APIs |
| Hybrid Manager Kubernetes Cluster | Hosts model-serving workloads |
| KServe | Manages model serving lifecycle and APIs |
| NVIDIA NIM containers | Containerized models (LLM, embedding, reranker, OCR) |
| User applications | Call model endpoints for inference |

### Supported Models

AI Factory Model Serving currently supports **NVIDIA NIM** containers for:

- Text Completion
- Text Embeddings
- Text Reranking
- Image Embeddings
- Image OCR

See the [Supported Models index](../../models/supported-models/index) for full details.

### Infrastructure and Scaling

- Models run on GPU-enabled Kubernetes nodes provided by Hybrid Manager (HCP).
- AI Factory automatically manages GPU node groups and KServe runtimes.
- You control model deployment and updates via:
  - `InferenceService` resources
  - `ClusterServingRuntime` definitions
  - GPU resource tuning

### Consumption Model

- Applications interact with models via:
  - REST APIs
  - gRPC APIs
- Example use cases:
  - AI Assistants using embedding models
  - RAG pipelines calling text rerankers
  - Image processing pipelines using OCR

## Deployment Architecture
```Applications → Model Endpoints (REST/gRPC) → KServe → GPU-enabled Kubernetes → Model Containers
```

- Each model is isolated in its own KServe `InferenceService`.
- KServe manages model lifecycle, scaling, and routing.
- Monitoring is integrated with Prometheus-compatible scrapers.

## Summary

Model Serving in AI Factory provides a robust, scalable architecture for serving production AI models:

✅ Kubernetes-native serving with KServe
✅ GPU acceleration
✅ Enterprise observability
✅ Seamless integration with AI Factory pipelines and applications

**Next steps:**

- [Model Serving Quickstart](../model-serving/quickstart)
- [How Model Serving Deployment Works](../../model/serving/deployment/index)
- [Deploy NVIDIA NIM Containers](../../model/serving/deployment/deploying-nim)
- [Supported Models](../../models/supported-models/index)

---

For background on KServe and how it fits into AI Factory architecture, see [AI Factory Concepts](./ai-factory-concepts#model-serving-kserve).
