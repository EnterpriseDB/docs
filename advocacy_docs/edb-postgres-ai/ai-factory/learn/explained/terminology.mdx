---
title: AI Factory terminology
navTitle: Terminology
description: Definitions of key concepts, technologies, and architecture patterns used in the AI Factory and Hybrid Manager AI Factory.
---

AI Factory terminology defines key concepts and technologies used across the **EDB AI Factory**, **Hybrid Manager AI Factory**, and related components.

This page complements conceptual explanations in:

- [AI Factory concepts](./ai-factory-concepts)
- [Generic concepts](./generic-concepts)

**Why it matters:**
The AI Factory integrates cutting-edge AI capabilities with database services and cloud-native infrastructure. Understanding these terms will help you build effective AI-powered applications and data pipelines within the EDB ecosystem.

---

## Core AI concepts

### Machine learning (ML)

**What:** Algorithms that learn from data and improve performance on tasks.
**Why:** Enables predictive analytics, automation, and decision-making.

Types:

- Supervised learning (uses labeled data)
- Unsupervised learning (finds patterns in unlabeled data)
- Reinforcement learning (learns via trial and error)

[Learn more](https://en.wikipedia.org/wiki/Machine_learning)

### Deep learning (DL)

**What:** ML using neural networks with many layers.
**Why:** Powers state-of-the-art AI for image recognition, speech, text, and more.

- Requires large datasets and high compute
- Commonly used for LLMs, computer vision, audio processing

[Learn more](https://en.wikipedia.org/wiki/Deep_learning)

### Natural language processing (NLP)

**What:** AI that enables computers to understand and generate human language.
**Why:** Drives chatbots, semantic search, text summarization, and more.

Applications:

- Text classification
- Sentiment analysis
- Question answering
- Conversational AI

[Learn more](https://en.wikipedia.org/wiki/Natural_language_processing)

### Large language models (LLMs)

**What:** Deep learning models trained on large text corpora.
**Why:** Power chatbots, text generation, coding assistants, translation.

Examples:

- OpenAI GPT
- Anthropic Claude
- Meta LLaMA

[Learn more](https://huggingface.co/blog/llm)

### Embeddings

**What:** Dense vector representations of data (text, images, users) in multi-dimensional space.
**Why:** Allow semantic comparisons for search, recommendations, and RAG.

[Learn more](https://sebastianraschka.com/blog/2023/llm-embeddings.html)

### Vector databases

**What:** Databases optimized for storing and querying embeddings.
**Why:** Power semantic search and LLM-based apps (RAG pipelines).

Popular options:

- pgvector (Postgres extension)
- Pinecone
- Weaviate
- Milvus

[Learn more](https://vectorsearch.dev/)

### Retrieval-augmented generation (RAG)

**What:** Technique where LLMs retrieve documents from vector stores to ground their responses.
**Why:** Makes LLM output more accurate, current, and domain-specific.

[Intro to RAG](https://huggingface.co/blog/rag)

---

## AI for databases

### Intelligent database management

**What:** ML applied to optimize database operations.
**Why:** Automates tuning and improves performance.

Examples:

- Automated performance tuning
- Predictive scaling
- Anomaly detection

### In-database machine learning (In-DB ML)

**What:** Running ML inside the database.
**Why:** Eliminates ETL, enables real-time ML with SQL.

Benefits:

- Reduced data movement
- Simplified architecture
- Real-time insights

### Vector search in Postgres

**What:** Perform similarity search on embeddings inside Postgres.
**Why:** Power AI use cases without leaving the database.

- Uses [pgvector](https://github.com/pgvector/pgvector)
- Integrated in AIDB

### AIDB

**What:** AI-in-Database for HCP-managed Postgres.
**Why:** Brings vector search, embedding handling, and ML into Postgres.

- Foundation for semantic search and RAG inside Postgres

### Natural language interfaces to databases

**What:** Query databases using natural language (instead of SQL).
**Why:** Democratizes data access for non-technical users.

- Powered by LLMs and NLP
- Used for data exploration and reporting

---

## AI infrastructure

### AI-accelerated hardware

**What:** Specialized processors for AI:

- GPUs (primary for training + inference)
- TPUs (Google)
- FPGAs (custom acceleration)

**Why:** Essential for large models (LLMs, computer vision, etc.)

[More](https://developer.nvidia.com/cuda-gpus)

### KServe

**What:** Kubernetes-native model serving platform.
**Why:** Standard for serving ML models in production.

- InferenceService CRDs
- Autoscaling
- Serverless option
- GPU-aware scheduling

[KServe documentation](https://kserve.github.io/website/)

### Gen AI Builder

**What:** Application builder powered by Griptape, integrated with AI Factory.
**Why:** Rapidly build complex AI apps with LLMs, tools, and structured workflows.

Uses:

- KServe-hosted models
- AIDB Knowledge Bases
- Datalake object storage

---

## Griptape concepts

### Structures

**What:** Orchestrate AI workflows:

- **Agent** — autonomous planning + tool use
- **Pipeline** — linear sequence of tasks
- **Workflow** — general orchestration with branching

### Tasks

**What:** Units of work:

- **PromptTask** — interact with LLM
- **ToolTask** — run a tool
- **QueryTask** — query vector DB or RAG store
- **CodeExecutionTask** — run code in sandbox

[Griptape documentation](https://griptape.ai/)

### Tools

**What:** External capabilities accessible to Griptape structures.

- Web search
- API calls
- DB queries
- Enterprise systems

### Drivers

**What:** Connect to services:

- LLM drivers — KServe-hosted models
- Embedding drivers — KServe embedding models
- Vector store drivers — pgvector, Pinecone, etc.
- Storage drivers — persist memory to Datalake

### Memory

**What:** Manages state and context for AI apps.

- Conversation memory (short-term)
- Task memory
- Long-term memory in object storage (Datalake)

### Rulesets and rules

**What:** Guide AI agent behavior.

- Rules — constraints or instructions (tone, safety, behavior)
- Rulesets — collections of rules applied to agents or structures

---

## KServe concepts

### InferenceService

**What:** Primary KServe CRD for model deployment.

Components:

- **Predictor** — runs model server
- **Transformer** — pre/post-processing
- **Explainer** — optional model explainability

### ServingRuntime / ClusterServingRuntime

**What:** Blueprint for model server pods.

- Container image
- Resources
- Model formats supported

### Model server

**What:** Software that loads the model and serves it via API.

Options:

- NVIDIA Triton
- TorchServe
- TensorFlow Serving
- MLServer
- Custom servers

---

## Image and Model Library terms

### AI model image

**What:** Container image packaging an AI model + serving stack.

- Deployed via KServe in AI Factory

### Database image (PG image)

**What:** Container image configured to run Postgres.

### Container registry

**What:** Stores and delivers container images.

Examples:

- Docker Hub
- AWS ECR
- GCP GAR
- Private registries

### EDB-provided registry

**What:** Registry managed by EDB hosting official Postgres and AI model images.

### Private registry

**What:** Customer-managed registry integrated with AI Factory.

### Image tag (version tag)

**What:** Label identifying image version.

### Repository rule

**What:** Pattern used to select images/tags from private registry.

### NVIDIA Inference Microservice (NIM)

**What:** Optimized AI model images from NVIDIA for high-performance inference.

### SHA digest

**What:** Unique identifier (hash) for image content.

---

## Summary

**AI Factory terminology** spans:

- AI concepts and patterns
- AI-for-database patterns (AIDB, vector search, In-DB ML)
- AI infrastructure (KServe, GPUs, model serving)
- AI application development (Griptape, Gen AI Builder)
- Container-based delivery (Images and Model Library)

**Next:**
To explore these terms in depth, see:

- [AI Factory concepts](./ai-factory-concepts)
- [Generic concepts](./generic-concepts)
