---
title: Model Serving
navTitle: Model Serving
description: Explore the Model Serving capability in AI Factory — enabling production deployment of supported models as scalable inference services.
---

Use the Model Serving capability of AI Factory to deploy models as network-accessible inference services using a standardized Kubernetes-native serving engine.

AI Factory uses KServe as its model serving engine. In AI Factory version 1.2, Model Serving is used for deploying NVIDIA NIM containers. While KServe supports a broad range of model formats, current AI Factory support focuses on providing a streamlined and integrated deployment experience for NIM-based models.

KServe operates within the managed Hybrid Manager (HCP) Kubernetes infrastructure provided by AI Factory, giving you access to scalable, GPU-accelerated model serving.

For a deeper understanding of the model serving architecture, see [KServe in AI Factory concepts](../learn/explained-ai-factory-concepts#model-serving-kserve).

---

## How Model Serving works

- Models are deployed as KServe InferenceServices within your HCP project.
- AI Factory provides GPU-enabled Kubernetes infrastructure to run these services.
- The HCP Model Library helps you discover and manage supported models.
- Your applications can consume model endpoints using standard HTTP or gRPC APIs.

---

## Where to start

- [Model Serving Quickstart](../learn/model-serving/quickstart) — get started fast
- [Model Serving Learning Paths](../learn/paths/index) — structured learning journeys
- [KServe in AI Factory concepts](../learn/explained-ai-factory-concepts#model-serving-kserve)

---

## Key topics

### Learning and implementation

- [Model Serving Learning Paths](../learn/paths/index)
- [Model Serving How-To Guides](../learn/model-serving/index)

### Deployment

- [How Model Serving deployment works](./deployment/index)
- [Deploying NVIDIA NIM containers](./deployment/deploying-nim)
- [Deploying other models](./deployment/deploying-other-models) *(future)*

### Model list

- [Supported models index](../../models/supported-models/index)

### Monitoring and observability

- [Observability for model serving](./observability)

### FAQ

- [Model Serving FAQ](./faq)

---

For product-specific guidance on using Model Serving within Hybrid Manager (HCP), see the [Hybrid Manager Model Serving spoke](../hybrid-manager/ai-factory/model-serving/index).
