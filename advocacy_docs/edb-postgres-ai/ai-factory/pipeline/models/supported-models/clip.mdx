---
title: "CLIP"
navTitle: "CLIP"
description: "CLIP (Contrastive Language-Image Pre-training) is a model that learns visual concepts from natural language supervision."
---

Model name: `clip_local`

## About CLIP

CLIP (Contrastive Language-Image Pre-training) is a model that learns visual concepts from natural language supervision. It's a zero-shot learning model that can be used for a wide range of vision and language tasks.

Read more about [CLIP on OpenAI's website](https://openai.com/research/clip/).


## Supported aidb operations

* encode_text
* encode_text_batch
* encode_image
* encode_image_batch

## Supported models

* openai/clip-vit-base-patch32 (default)

See more in the [Support matrix](./support_matrix).

## Creating the default model

```sql
SELECT aidb.create_model('my_clip_model', 'clip_local');
```

There's only one model, the default `openai/clip-vit-base-patch32`, so you don't need to specify the model in the configuration. No credentials are required for the CLIP model.

## Creating a specific model

There are no other model configurations available for the CLIP model.


## Model configuration settings

The following configuration settings are available for CLIP models:

* `model` &mdash; The CLIP model to use. The default is `openai/clip-vit-base-patch32` and is the only model available.
* `revision` &mdash; The revision of the model to use. The default is `refs/pr/15`. This entry is a reference to the model revision in the Hugging Face repository. It's used to specify the model version to use, in this case [this branch](https://huggingface.co/openai/clip-vit-base-patch32/tree/refs%2Fpr%2F15).
* `image_size` &mdash; The size of the image to use. The default is `224`.

## Model credentials

No credentials are required for the CLIP model.
