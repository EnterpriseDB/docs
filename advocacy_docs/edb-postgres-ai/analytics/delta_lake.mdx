---
title: Delta Lake
navTitle: Delta Lake
description: Understand how Delta Lake enhances modern data lakes and how Analytics Accelerator leverages it within the EDB Postgres ecosystem.
---

# Delta Lake

Delta Lake is an open-source storage layer that brings ACID transactions, data reliability, scalability, and performance to data lakes. EDB enables its use within the EDB Postgres ecosystem to support robust analytics on data lake storage.

For details on how Delta Lake is used and managed within Hybrid Manager (HM), see [Working with Delta Lake in EDB HM](../../hm_analytics_spoke_root_v2).

## What is Delta Lake

Delta Lake is an open table format that enhances existing data lakes (S3, GCS, Azure Data Lake Storage) by adding features traditionally associated with data warehouses.

Key characteristics:

- **ACID transactions:** Guarantees data integrity even with concurrent reads and writes
- **Schema enforcement and evolution:** Maintains data quality and supports safe schema changes
- **Time travel:** Enables querying previous versions of data
- **Batch and streaming:** Supports both batch and streaming pipelines
- **Open format:** Built on [Apache Parquet](https://parquet.apache.org/) with a transaction log (`_delta_log`)

Related concept: [Generic concepts - open table formats](../../explained/generic-concepts#open-table-formats)

## Key features and benefits

Delta Lake helps build reliable and performant data lakehouses:

- **Reliability:** Prevents common data corruption problems in data lakes
- **Improved data quality:** Enforces schema on write
- **Simplified pipelines:** Supports updates, deletes, and merges on lake data
- **Audit history:** Maintains a complete transaction log
- **Performance optimizations:** Enables data skipping and Z-ordering for faster queries
- **Scalability:** Designed for petabyte-scale data lakes

## Why Delta Lake matters for EDB analytics

Delta Lake support enables the Analytics Accelerator to:

- **Query existing data lakes:** Use familiar Postgres SQL to query Delta Lake tables directly
- **Enhance Lakehouse capabilities:** Combine Postgresâ€™ analytical power with Delta Lake reliability
- **Enable interoperability:** Share data across tools (Spark, Presto, Trino, EDB Postgres Lakehouse)
- **Simplify architectures:** Avoid unnecessary data movement between operational and analytical systems

Related concept: [Analytics Accelerator concepts](../../analytics-concepts#edb-postgres-lakehouse)

## How EDB leverages Delta Lake

EDB Postgres Lakehouse provides seamless access to Delta Lake data:

- **PGAA extensions:** PGAA components in Lakehouse nodes support Delta protocol
- **Reading Delta tables:** Users define tables with `CREATE TABLE ... USING PGAA WITH (pgaa.format = 'delta', ...)` that point to Delta tables
- **Vectorized execution:** Lakehouse nodes use [Apache DataFusion](https://datafusion.apache.org/) to process Parquet files efficiently

Current primary use case: **read existing Delta Lake tables**.
PGD offloads currently target Iceberg, but Delta read support allows integration with existing Delta-based data lakes.

## Related concepts

- [Generic concepts - data lakehouse](../../explained/generic-concepts#data-lakehouse)
- [Generic concepts - columnar storage formats](../../explained/generic-concepts#columnar-storage-formats)
- [Analytics Accelerator concepts - EDB Postgres Lakehouse](../../analytics-concepts#edb-postgres-lakehouse)

## Next steps

- For Delta Lake usage and management in Hybrid Manager, see [Working with Delta Lake in EDB HM](../../hm_analytics_spoke_root_v2)
- For a broader understanding of related concepts, see [Generic concepts](../../explained/generic-concepts)

