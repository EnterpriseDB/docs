---
title: Analytics/Lakehouse
navTitle: Overview

---

EDB Postgres Lakehouse extends the power of Postgres to analytical workloads
by adding a vectorized query engine and separating storage from compute. Building
a data Lakehouse has never been easier: just use Postgres.

## Rapid analytics for Postgres

Postgres Lakehouse is a core offering of the EDB Postgres® AI HCP platform, extending
Postgres to support analytical queries over columnar data in object storage,
while keeping the simplicity and ease of use that Postgres users prefer.

With Postgres Lakehouse, you can query your Postgres data with a Lakehouse node,
an ephemeral, scale-to-zero compute resource powered by Postgres that's optimized for
vectorized query execution over columnar data.

## Postgres native

Never leave the Postgres ecosystem.

Postgres Lakehouse nodes run either EDB Advanced Server or EDB Postgres
Extended as the Postgres engine, with data for analytics stored as
columnar tables in object storage using the open Delta Lake or Iceberg protocol.

EDB Postgres Lakehouse is essentially Postgres. You can query it with any Postgres
client, and it fully supports all Postgres queries, functions, and statements, so
there's no need to change existing queries or reconfigure business
intelligence software.

## Vectorized execution

Postgres Lakehouse uses Apache DataFusion's vectorized SQL query engine to
execute analytical queries 5-100x faster (30x on average) compared to native
Postgres, while still falling back to native execution when necessary.

## Columnar storage

Postgres Lakehouse is optimized to query Lakehouse tables in object storage,
extending the power of open source database to open table formats. Currently,
it supports querying Delta Tables stored according to the Delta Lake protocol 
and Iceberg tables.

## Fully managed service

You can launch Postgres Lakehouse nodes from the EDB Postgres® AI HCP platform.
Point a Lakehouse node at a storage bucket with some Delta Tables in it,
and get results of analytical (OLAP) queries in less time than if you queried the
same data in a transactional Postgres database.

## Try it today

It's easy to start using Postgres Lakehouse. Provision a Lakehouse node in less than a minute,
and start querying preloaded benchmark data like TPC-H, TPC-DS, Clickbench, and the 1 Billion Row challenge.
