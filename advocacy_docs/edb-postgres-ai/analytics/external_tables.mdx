---
title: Querying Delta Lake Tables in S3-compatible object storage
navTitle: External tables

---

## Overview

External tables allow you to access and query data stored in S3-compatible object storage using SQL. You can create an external table that references data in S3-compatible object storage and query the data using standard SQL commands.

## Prerequisites

-   An EDB Postgres AI HCP installation with a Lakehouse node.
-   An S3-compatible object storage location with data stored as Delta Lake Tables.
    -   See [Bringing your own data](reference/loadingdata.mdx) for more information on how to prepare your data.
-   Credentials to access the S3-compatible object storage location, unless it's a public bucket.
    -   These credentials will be stored in the database. We recommend creating a separate user with limited permissions for this purpose.

!!!Note

Using an S3 bucket that isn't in the same region as your node:

-   Is slow because of cross-region latencies.
-   Typically incurs costs for data transfer.
!!!

## Creating an external storage location

The first step is to create an external storage location that references S3-compatible object storage where your data resides. A storage location is an object in the database that you refer to to access the data. Each storage location has a name for this purpose.

EDB Postgres AI HCP Lakehouse uses PGFS to manage and access storage locations. The following example creates a public S3-compatible object storage location:

```sql
SELECT pgfs.create_storage_location(  
  name => 'sample-data',  
  url => 's3://pgaa-sample-data-eu-west-1',  
  options => '{}',  
  credentials => '{}',  
  msl_id => NULL  
);
```

The next example creates a private S3-compatible object storage location:

```sql
SELECT pgfs.create_storage_location(  
  name => 'private-data',  
  url => 's3://my-private-bucket',  
  options => '{}',  
  credentials => '{"access_key_id": "my-access-key-id","secret_access_key": "my-secret-access-key"}',  
  msl_id => NULL  
);
```

See the [PGFS documentation](https://www.enterprisedb.com/docs/edb-postgres-ai/ai-accelerator/pgfs/) for more information on how to manage storage locations.

## Creating an external table

After creating the external storage location, you can create an external table that references the data in the storage location.
The following example creates an external table that references a Delta Lake Table in the S3-compatible object storage location:

```sql
CREATE TABLE public.customer () USING PGAA WITH (pgaa.storage_location = 'sample-data', pgaa.path = 'tpch_sf_1/customer');
```

The schema isn't defined in the `CREATE TABLE` statement. The PGAA extension expects the schema to be defined in the storage location, and the schema itself is derived from the schema stored at the path specified in the `pgaa.path` option. The PGAA extension infers the best Postgres-equivalent data types for the columns in the Delta Table.

## Querying an external table

After creating the external table, you can query the data in the external table using standard SQL commands. The following example queries the external table created in the previous step:

```sql
SELECT COUNT(*) FROM public.customer;
```

## Querying Iceberg external tables

To query an external table stored in the Iceberg format, use the `pgaa.format = 'iceberg'` table option when creating the table:

```sql
CREATE TABLE iceberg_external_table ()  
USING PGAA  
WITH (  
  pgaa.storage_location = 'sample-data',  
  pgaa.path = 'path/to/iceberg/table',  
  pgaa.format = 'iceberg'  
);
```

The root of the table path must contain a `version-hint.text` file pointing to the latest Iceberg metadata file to use. This is also known as an *HDFS catalog* or a *filesystem catalog*. The file contains just the ID of the latest version as follows:

```shell
$ cat metadata/version-hint.text  
1  
```

You can also override the metadata path and point PGAA to a specific metadata file:

```sql
CREATE TABLE iceberg_external_table ()  
USING PGAA  
WITH (  
  pgaa.storage_location = 'sample-data',  
  pgaa.path = 'path/to/iceberg/table/metadata/v1.metadata.json',  
  pgaa.format = 'iceberg'  
);  
```

Support for querying data in an external Iceberg catalog from a Lakehouse instance is coming soon.
