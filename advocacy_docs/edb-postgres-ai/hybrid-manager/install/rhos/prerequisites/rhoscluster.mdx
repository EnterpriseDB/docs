---
title: Creating a Red Hat OpenShift cluster for use with Hybrid Manager
navTitle: Create/configure RHOS cluster

---

After you've [installed your system](installsystem.mdx), you're ready to create a Red Hat OpenShift (RHOS) cluster for use with Hybrid Manager (HM).

## Preparing an OpenShift cluster for HM installation

To successfully install HM on a Red Hat OpenShift (RHOS) on-premises cluster, certain prerequisites and configurations must be in place.

### Preconfiguration cluster requirements

```
* HM currently supports:
    * OpenShift 4.17 with Kubernetes 1.30
or:
    * OpenShift 4.18 with Kubernetes 1.31
```

### Step 1: Configure networking

-   Define network policies. Ensure appropriate network policies are set up to manage traffic between Kubernetes control plane, worker nodes, and external systems.

-   Define ingress/egress rules.

-   Ensure communication between cluster components and the HM stack is secure.
    -   Validate ingress/egress network policies to allow cluster components to communicate while restricting unnecessary external access.

-   Set up load balancer: If using an external load balancer, configure it to handle OpenShift API traffic and worker node communication.

-   Configure DNS: Ensure DNS records are set up for the cluster, including internal service discovery for HM.

-   Ensure the cluster has adequate private network isolation for worker nodes. OpenShift typically handles this via the software-defined network (SDN) configuration.

### Step 2: Configure node size and resources

To support HM and its associated components, the nodes in your OpenShift cluster must meet the resource requirements.

#### Node sizing

-   **Master nodes**: OpenShift control plane nodes must meet the standard requirements for resource allocation (for example, CPU, memory, and disk size).

    -   General recommendations:
        -   3 master nodes each with:
            -   CPU: 
                -   Minimum: 4 vCPUs (for up to 10 worker nodes)
                -   Recommended: 8 vCPUs for medium-sized clusters (10-50 worker nodes)
                -   For >50 worker nodes: 16+ vCPUs
            -   Memory: 
                -   Minimum: 32 GB RAM (for up to 10 worker nodes)
                -   Recommended: 64 GB RAM or more for larger workloads (10-50 worker nodes)
                -   For >50 worker nodes: 64+ GB RAM
            -   Disk
                -   Minimum: 100 GB SSD
                -   Recommended: 200 GB SSD for larger workloads
                -   For >50 worker nodes: >200 GB SSD

-   **Worker nodes**: Allocate sufficient resources to worker nodes to handle HM control components and Postgres clusters: 3 worker nodes for the HM control components and 3 worker nodes for the Postgres workloads are recommended for smaller workloads (5-20 Postgres clusters). As you scale up Postgres workloads from this, you may require more worker nodes for the Postgres workloads and eventually even more control worker nodes for supporting the increasing number of Postgres workload worker nodes.

    -   General recommendations:
        ```
        * 6 worker nodes each with:
            * CPU:
                * Minimum: 4 vCPUs (this could be higher depending on the type of workload)
            * Memory: 
                * Minimum: 32 GB of RAM per node
            * Disk: 
                * Minimum: 100 GB of persistent storage per node (adjust based on database and logging requirements). Use fast disks (SSD) for optimal performance.
        ```
        To (optionally) use different types of nodes for the HM control nodes and the HM Postgres data nodes hosting the Postgres workloads, RHOS requires additional machine sets:

    -   Control nodes machine set:

        ```yaml
        spec:
          replicas: 3
          template:
            spec:
              metadata:
                labels:
                  edbaiplatform.io/control-plane: "true"
            taints:
            - key: edbplatform.io/control-plane
              value: "true
              effect: NoSchedule
        ```

    -   Data nodes hosting Postgres workloads machine set:

        ```yaml
        spec:
          replicas: 3
          template:
            spec:
              metadata:
                labels:
                  edbaiplatform.io/control-plane: “true”
                taints:
                - key: edbplatform.io/control-plane
                  value: "true"
                  effect: NoSchedule
        ```

#### Resource reservations

-   To prevent contention with HM workloads, reserve CPU and memory for system components.

-   Allocate additional resources for HM monitoring and observability tools (for example, Grafana, Loki, and Prometheus).

-   Node pool configuration:

    -   If desired, use taints and tolerations to isolate HM workloads on specific nodes.

    -   Label nodes for specific tasks (for example, "monitoring", "database") to optimize resource allocation.

-   Scaling:

    -   Set up autoscaling to ensure the cluster can dynamically handle increased demand. OpenShift's Horizontal Pod Autoscaler (HPA) can be used to scale pods as needed.

#### Networking bandwidth

Ensure each node has adequate networking capacity to handle HM communication and external data transfer (for example, S3 backups).

Baseline recommendations:

| Component               | Recommended bandwidth  | Justification                                                                                    |
| ----------------------- | ---------------------- | ------------------------------------------------------------------------------------------------ |
| K8s control plane nodes | 1 Gbps+ per node       | Handles internal Kubernetes traffic, API server requests, logs/metrics, and orchestration tasks. |
| Worker nodes (HM)       | 1 Gbps+ per node       | For HM control components, PostgreSQL replication, internode communication, and metrics/logs.    |
| External bandwidth      | 1-20 Gbps (aggregated) | S3 backups and inter-cluster replication may require high throughput.                            |

### Step 3: Install the required CSI drivers

HM uses storage and snapshot features that depend on CSI drivers. Verify the following drivers are installed and properly configured in your OpenShift cluster:

-   Persistent storage driver: Install a storage class driver compatible with your storage backend (for example, OpenShift Container Storage, Ceph, or other on-prem solutions).

-   Snapshot controller: Configure the CSI snapshot controller for volume snapshot management.

### Step 4: Configure storage classes

**Default storage class**: Set up a default storage class optimized for HM workloads.

**Custom storage classes**: Create additional storage classes if specific workloads require custom configurations.

### Step 5: Prepare namespace and secrets

**Namespace for HM**: Create a dedicated namespace for HM components to ensure isolation and manageability.

````
```bash
oc create ns edbpgai-bootstrap
```
````

**Secrets**: Create Kubernetes secrets for any required credentials, such as object storage credentials, database access tokens, or other sensitive information.
The following are HM-specific secrets and configurations.
The first ImagePullSecret is necessary for HM to function properly.
Griptape and DeltaLake object storage and secret for Catalog are optional, as they depend on whether you're setting up your cluster for GenAI Builder or Catalog.

-   **Configure ImagePullSecret and namespace** (necessary for HM)

    1.  Set environmental variables.

        So that you can more easily log in to OC Registry, there are a few container registry variables to set:

        ```bash
        export CONTAINER_REGISTRY_URI=<local-registry-uri>
        export CONTAINER_REGISTRY_USERNAME=<container-registry-username>
        export CONTAINER_REGISTRY_PASSWORD=<container-registry-password>
        ```

    2.  Prepare the pull secret file.

        Prepare the directory for the image pull secret:

        ```bash
        tmpdir=$(mktemp -d /tmp/pull-secret-<your-pull-secret-name)
        chmod 0700 "$(tmpdir)"
        ```

    3.  Log in to the registry and update the secret.

        Log in to OC Registry:

        ```bash
        oc registry login \
        --registry=$${CONTANER_REGISTRY_URI}" \
        --auth-basic="${CONTAINER_REGISTRY_USERNAME}:${CONTAINER_REGISTRY_PASSWORD}"
            --to="$tmpdir/pull-secret"
        ```

        Then update the secret:

        ```bash
        oc set data secret/pull-secret -n openshift-config \
            --from-file=.dockerconfigjson=”$tmpdir/pull-secret”     
        ```

    4.  Clean up.

        Clean up the temporary directory you made for the secret:

        ```bash
        rm -rf $tmpdir
        ```

-   **Create a secret for Griptape and configure DeltaLake object storage (optional for GenAI Builder)**

      See the instructions on this [page](/edb-postgres-ai/hybrid-manager/install/customization/genai_secret/).

-   **Create a secret for Catalog (optional for using Catalog)**

      See the instructions on this [page](/edb-postgres-ai/hybrid-manager/install/customization/data_catalog_secret/).

### Step 6: Configure authentication and security keys

HM and its underlying components require secure authentication mechanisms to ensure proper communication between components and to protect sensitive data.

1.  **Generate AES-256 encryption key.**

    HM uses an AES-256 encryption to secure sensitive data during communication or at rest (for example, database credentials, tokens).
    To generate a random AES-256 encryption key:

    ```bash
    export AES_256_KEY=$(openssl rand -base64 32)
    ```

2.  **Store the key in Kubernetes.**

    To make the key accessible to HM and associated services, create a Kubernetes secret in the appropriate namespace:

    1.  Run the following command to create the secret:

        ```bash
        kubectl create secret generic hm-auth-key \
            --namespace <hm-namespace> \
            --from-literal=aes-256-key=$AES_256_KEY
        ```

    2.  Verify the secret:

        ```bash
        kubectl get secret hm-auth-key --namespace <hm-namespace>
        ```

### Step 7: Configure object storage

-   Object storage backend: To support HM backups, connect the cluster to an object storage solution (for example, MinIO, Ceph Object Gateway, or AWS S3-compatible storage).
    See [Bring your own object storage](<>) for more information regarding the configuration of your object store.

### Step 8: Validate network security

-   Firewall rules: Ensure firewall rules allow necessary communication between the OpenShift cluster, external object storage, and any external dependencies of HM.

-   Ingress/egress rules: Restrict access to only the required endpoints and IP ranges to minimize exposure.

### Step 9: Adjust cluster configurations

Some default OpenShift configurations may need adjustments:

-   Node pool tuning: Optimize node configurations for HM workloads, including resource allocation and taints/tolerations.

-   Storage class adjustments: Replace or modify default storage classes as needed.

-   Networking policies: Refine networking configurations to ensure compatibility with HM requirements.

### Step 10: Install required add-ons

-   Install and configure any required OpenShift operators or add-ons:

    -   Service mesh: HM relies on istio service mesh.

    -   Monitoring and logging: Set up OpenShift monitoring and logging solutions to integrate with HM observability components.

### Proceed to setting necessary environmental variables

With the cluster configured, proceed to the next phase of the installation: [setting the necessary environmental variables](setvariables.mdx).
