
---
title: Model Serving in Hybrid Manager
navTitle: Model Serving
description: How Model Serving works within Hybrid Manager (HCP AI Factory workload) and key deployment considerations.
---

Hybrid Manager enables **Model Serving** through its AI Factory workload, using **KServe** on HCP Kubernetes infrastructure.

Models are deployed as **InferenceServices** and exposed via HTTP/gRPC endpoints within your Hybrid Manager project.

---

## How it works in Hybrid Manager

- KServe runs within your projectâ€™s Kubernetes cluster.
- GPU resources must be provisioned and configured:
- [How to set up GPU resources](../../learn/how-to/model-serving/update-gpu-resources)
- Model images can be deployed from the Model Library.
- Your applications (including AI Assistants) can call model endpoints.

---

## Links to learn more

- [Deploying NVIDIA NIM models in Hybrid Manager](../../learn/how-to/model-serving/deploy-nim-container)
- [Verify deployed models](../../learn/how-to/model-serving/verify-models)
- [KServe Concepts Hub](/ai-factory/learn/explained/model-serving-concepts)
- [Model Serving FAQ](/ai-factory/learn/how-to/model-serving/faq)
