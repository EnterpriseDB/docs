---
title: Model Serving in Hybrid Manager
navTitle: Model Serving
description: Use Model Serving in Hybrid Manager to deploy AI models as scalable inference services using HCP-managed Kubernetes.
---

Model Serving in Hybrid Manager (HCP) provides the operational environment to deploy and manage AI models using the AI Factory Model Serving capability. It integrates the KServe serving engine into HCP-managed Kubernetes clusters, allowing you to deploy models as scalable inference services with GPU acceleration.

Model Serving enables you to build and support AI-driven applications with optimized model serving infrastructure.

---

## How it works in Hybrid Manager

- Model Serving in HCP uses the [AI Factory Model Serving architecture](../../../../ai-factory/model-serving).
- Models are deployed as **KServe InferenceServices** inside HCP Kubernetes clusters.
- GPU node groups and NVIDIA device plugins must be configured in HCP Kubernetes.
- NVIDIA API keys must be securely stored as Kubernetes secrets.
- Current HCP 1.2 support focuses on deploying **NVIDIA NIM containers** as model services.

---

## Why use Model Serving in HCP

- Enables **AIDB Knowledge Bases** to use scalable model services for embeddings and reranking.
- Provides **shared model services** for GenAI Builder assistants.
- Supports **GPU-accelerated inference** with KServe on HCP-managed Kubernetes.
- Gives you a **single pane of glass** for managing AI models across Postgres clusters and applications.

---

## Workflow overview

1. Prepare HCP Kubernetes cluster:
- Configure GPU nodes.
- Deploy NVIDIA device plugin.
- Store NVIDIA API keys.
2. Deploy ClusterServingRuntime and InferenceService:
- Deploy NIM containers using KServe manifests.
- Use recommended models for AIDB and GenAI Builder.
3. Monitor and operate models:
- Check deployed InferenceServices.
- Manage GPU resources.
- Connect AIDB and GenAI Builder to model endpoints.

---

## Links to Model Serving hub

- [Model Serving overview](../../../../ai-factory/model-serving)
- [KServe in AI Factory concepts](../../../../ai-factory/learn/explained-ai-factory-concepts#model-serving-kserve)
- [Model Serving How-To Guides](../../../../ai-factory/learn/model-serving/index)
- [Supported Models](../../../../ai-factory/models/supported-models/index)

---

## Key HCP-specific setup

- [Preparing GPU nodes and NVIDIA device plugin](./gpu-setup)
- [Deploying NIM models in HCP](./deployment/deploying-nim)
- [Verifying deployed models in HCP](./deployment/verifying-models)

---

## Next steps

- If you are new to Model Serving, start with the [Model Serving Getting Started](../../../../ai-factory/model-serving/getting-started).
- Review [AI Factory Learning Paths](../../../../ai-factory/learn/paths/index) to understand the full lifecycle.
- Follow the How-To Guides to deploy and manage models in your HCP project.

---

## Related

- [AI Factory concepts](../../../../ai-factory/learn/explained-ai-factory-concepts)
- [AI Factory terminology](../../../../ai-factory/learn/explained-ai-factory-terminology)
- [AIDB documentation](../aidb/index)
- [GenAI Builder documentation](../genai-builder/index)
