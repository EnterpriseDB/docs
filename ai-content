Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/agent-studio.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/agent-studio.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/agent-studio.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/agent-studio.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,26 +0,0 @@
----
-title: Agent Studio in Hybrid Manager
-navTitle: Agent Studio
-description: How Agent Studio works within Hybrid Manager and how to build and manage AI Assistants.
----
-
-**Agent Studio** in Hybrid Manager provides an integrated way to create and manage AI Assistants.
-
-You can use Agent Studio to build agents that access models, knowledge bases, and tools—all running within the Hybrid Manager project infrastructure.
-
----
-
-## How it works in Hybrid Manager
-
-- Assistants run in your project’s Kubernetes cluster.
-- Assistants can call **models deployed via Model Serving**.
-- Assistants can retrieve data from **Knowledge Bases** built within the same project.
-- Assistants can call **Tools** implemented in your environment.
-
----
-
-## Key links
-
-- [Create an Assistant — How-To Guide](/ai-factory/learn/how-to/gen-ai/create-assistant)
-- [Agent Studio Hub reference](/ai-factory/gen-ai/agent-studio)
-- [Knowledge Bases in Hybrid Manager](../pipeline/knowledge-base)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/builder.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/builder.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/builder.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/builder.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,25 +0,0 @@
----
-title: Gen AI Builder in Hybrid Manager
-navTitle: Builder
-description: How Gen AI Builder is used within Hybrid Manager to develop agentic AI applications.
----
-
-**Gen AI Builder** allows you to develop advanced agentic AI applications that run within Hybrid Manager’s AI Factory workload.
-
-Builders run as containers in your project’s Kubernetes cluster, leveraging Model Serving endpoints and Knowledge Bases.
-
----
-
-## Key Hybrid Manager considerations
-
-- Builder applications run on **GPU-enabled infrastructure** when required.
-- Builders access **KServe-based Model Serving** endpoints within the project.
-- Builders can access Knowledge Bases created within the same project or shared.
-
----
-
-## Learn more
-
-- [Create an AI Tool — How-To Guide](/ai-factory/learn/how-to/gen-ai/create-tool)
-- [Deploy AI Models in Hybrid Manager](../model/serving)
-- Full Builder reference: [Gen AI Hub](/ai-factory/gen-ai/builder)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/gen-ai/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,35 +0,0 @@
----
-title: Gen AI in Hybrid Manager
-navTitle: Gen AI
-description: Learn how Gen AI capabilities are delivered within Hybrid Manager (HCP AI Factory workload), including Agent Studio and Builder.
----
-
-Hybrid Manager includes full support for the **Gen AI** capabilities of AI Factory.
-
-Using HCP’s Kubernetes-native infrastructure, you can deploy and manage **agentic AI applications** that integrate with your Postgres and AI ecosystem.
-
-The **AI Factory Hub** provides in-depth concepts and how-to material for building agents. This spoke page highlights key considerations when using Gen AI within Hybrid Manager.
-
----
-
-## Core components
-
-- **Agent Studio** — low-code interface for building and managing AI Assistants.
-- **Gen AI Builder** — code-driven agent development, using Griptape and integrated pipelines.
-
----
-
-## Hybrid Manager context
-
-- AI agents run within the Hybrid Manager project’s Kubernetes cluster.
-- You can deploy models via Model Serving and access them in your Agents.
-- Model endpoints can be secured using HCP-native controls.
-- Knowledge Bases can be built and managed from within the project.
-
----
-
-## Where to learn more
-
-- [Agent Studio in Hybrid Manager](./agent-studio)
-- [Gen AI Builder in Hybrid Manager](./builder)
-- Full feature details: [Gen AI Hub](/ai-factory/gen-ai)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/gpu.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/gpu.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/gpu.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/gpu.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,49 +0,0 @@
----
-title: GPUs in Model Serving
-navTitle: GPUs
-description: Understand the role of GPUs in Model Serving with AI Factory, how Hybrid Manager uses them, and how to prepare GPU resources.
----
-
-GPU acceleration is essential for running modern deep learning models in production. Many Large Language Models (LLMs), embedding models, and vision models require GPUs to deliver acceptable inference performance.
-
-Model Serving in AI Factory relies on GPU-enabled Kubernetes nodes for hosting KServe InferenceServices that run these models.
-
-## Why GPUs matter for Model Serving
-
-- Many **NVIDIA NIM containers** are designed to run on GPUs, with optimized inference serving.
-- Model Serving in AI Factory supports **GPU scheduling and resource control** through Kubernetes.
-- GPU nodes enable serving models that would otherwise be too slow or expensive to run on CPUs.
-- GPU-based serving supports **AIDB Knowledge Bases** and **GenAI Builder assistants** at scale.
-
-## GPU usage in Hybrid Manager
-
-Hybrid Manager (HCP) manages the Kubernetes infrastructure where Model Serving runs. In this context:
-
-- GPU-enabled node groups (AWS EKS) or node pools (GCP GKE, RHOS) must be provisioned.
-- These nodes must be labeled and tainted to allow **KServe model pods** to schedule properly.
-- The NVIDIA Kubernetes device plugin must be installed to expose GPU resources to Kubernetes.
-- Kubernetes secrets must be created to store **NVIDIA API keys** required by NIM models.
-
-## Actions you can take
-
-To enable GPU-based Model Serving:
-
-1. Provision GPU node groups in your HCP Kubernetes cluster.
-2. Label and taint GPU nodes correctly.
-3. Deploy the NVIDIA device plugin DaemonSet.
-4. Create a Kubernetes secret with your NVIDIA API key.
-5. Deploy ClusterServingRuntime and InferenceService manifests targeting GPU nodes.
-
-## Related concepts
-
-- [Model Serving overview](./index)
-- [KServe in AI Factory concepts](../learn/explained-ai-factory-concepts#model-serving-kserve)
-- [AI Factory Learning Paths](../learn/paths/index)
-- [Model Serving How-To Guides](../learn/model-serving/index)
-
-## Next steps
-
-- Follow the [How-To Guide: Setup GPU resources in HCP](../../hybrid-manager/learn/how-to/ai-factory/model-serving/setup-gpu).
-- Review the [Model Serving Quickstart](./quickstart).
-- Explore [Supported Models](../models/supported-models/index) to understand GPU requirements for specific models.
-
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,26 +0,0 @@
----
-title: Model capabilities in Hybrid Manager
-navTitle: Model
-description: Overview of Model Serving and Model Library capabilities in Hybrid Manager (HCP AI Factory workload).
----
-
-Hybrid Manager provides full support for **Model Serving** and **Model Library**, delivered through the AI Factory workload.
-
-Models are deployed as scalable **Inference Services** using KServe on Hybrid Manager’s Kubernetes infrastructure.
-
-The **Model Library** provides discovery and management of supported models and container images.
-
----
-
-## Key components
-
-- **Model Serving** — deploy models as network-accessible inference services.
-- **Model Library** — discover and manage AI models and container images.
-
----
-
-## Where to learn more
-
-- [Model Serving in Hybrid Manager](./serving)
-- [Model Library in Hybrid Manager](./library)
-- Full feature details: [Model Serving Hub](/ai-factory/model/serving), [Model Library Hub](/ai-factory/model/library)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/library.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/library.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/library.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/library.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,31 +0,0 @@
----
-title: Model Library in Hybrid Manager
-navTitle: Model Library
-description: How Model Library works within Hybrid Manager and how to manage model images.
----
-
-Hybrid Manager uses **Model Library**, powered by the HCP Image Library, to manage and deploy AI model images.
-
-You can use the Model Library to:
-
-- Discover available NIM models.
-- Integrate your own private registries.
-- Manage metadata for model images.
-
----
-
-## Hybrid Manager considerations
-
-- Model Library in Hybrid Manager overlays **HCP Image Library**.
-- Images can be deployed as KServe InferenceServices within your project.
-- Repository configuration is project-scoped.
-- You can add your own image registries:
-- [Integrate private registry — How-To](../../learn/how-to/model-library/integrate-private-registry)
-
----
-
-## Learn more
-
-- [How to deploy AI models in Hybrid Manager](../../learn/how-to/model-library/how-to-deploy-ai-models)
-- [Manage repository metadata](../../learn/how-to/model-library/manage-repository-metadata)
-- Full Model Library reference: [Model Library Hub](/ai-factory/model/library)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/serving.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/serving.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/serving.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/model/serving.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,29 +0,0 @@
-
----
-title: Model Serving in Hybrid Manager
-navTitle: Model Serving
-description: How Model Serving works within Hybrid Manager (HCP AI Factory workload) and key deployment considerations.
----
-
-Hybrid Manager enables **Model Serving** through its AI Factory workload, using **KServe** on HCP Kubernetes infrastructure.
-
-Models are deployed as **InferenceServices** and exposed via HTTP/gRPC endpoints within your Hybrid Manager project.
-
----
-
-## How it works in Hybrid Manager
-
-- KServe runs within your project’s Kubernetes cluster.
-- GPU resources must be provisioned and configured:
-- [How to set up GPU resources](../../learn/how-to/model-serving/update-gpu-resources)
-- Model images can be deployed from the Model Library.
-- Your applications (including AI Assistants) can call model endpoints.
-
----
-
-## Links to learn more
-
-- [Deploying NVIDIA NIM models in Hybrid Manager](../../learn/how-to/model-serving/deploy-nim-container)
-- [Verify deployed models](../../learn/how-to/model-serving/verify-models)
-- [KServe Concepts Hub](/ai-factory/learn/explained/model-serving-concepts)
-- [Model Serving FAQ](/ai-factory/learn/how-to/model-serving/faq)
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/pipeline/knowledge-base/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/pipeline/knowledge-base/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/pipeline/knowledge-base/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/pipeline/knowledge-base/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,12 +0,0 @@
----
-title: Knowledge Bases
-navigation:
-  - Deeper%20Concepts/%20Terminology
-  - ../../ Creating Your Knowledge Base with AIDB on HM
-  - >-
-    ../../ Data Ingestion and Embedding Generation for Your AIDB Knowledge Base
-    on HM
-  - ../../ Querying Your Knowledge Base with AIDB on HM
-  - ../../ Managing Your Knowledge Base with AIDB on HM
-
----
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/ai-factory/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,111 +0,0 @@
----
-title: AI Factory in Hybrid Manager
-navTitle: AI Factory
-description: Learn how to use the AI Factory workload within Hybrid Manager (HCP), including Gen AI, model serving, pipelines, and vector search. Explore learning paths, use cases, and Hybrid Manager-specific capabilities.
----
-
-# AI Factory in Hybrid Manager
-
-The **AI Factory workload** in Hybrid Manager brings scalable AI, machine learning, and Gen AI capabilities to your Hybrid Control Plane (HCP). It enables you to operationalize AI across your Hybrid Manager-managed clusters and data — with deep integration across Postgres, vector search, model serving, and pipelines.
-
-With AI Factory in Hybrid Manager, you can:
-
-- **Deploy Gen AI assistants and agents** for internal or external-facing use
-- **Serve AI models at scale** with integrated KServe-powered inferencing and GPU acceleration
-- **Build pipelines** to prepare and transform data for AI and vector use cases
-- **Create knowledge bases** and perform **retrieval-augmented generation (RAG)** with powerful vector search
-- **Manage your model library** and deploy trusted models within Hybrid Manager governance
-
----
-
-## Example AI Solutions You Can Build
-
-Hybrid Manager AI Factory unlocks solutions across a wide range of real-world needs:
-
-- **Enterprise search and knowledge assistants**
-Build RAG-based assistants that integrate with corporate documents, databases, and intranet content.
-
-- **Document intelligence and automation**
-Process PDFs, scanned documents, web data, and structured sources — applying OCR, summarization, and classification pipelines.
-
-- **Customer support chatbots**
-Deploy assistants powered by your own data and domain-specific models, with response generation and retrieval.
-
-- **AI-driven data apps**
-Expose AI-powered endpoints for applications — such as semantic search, recommendations, similarity search, or NLP-based querying.
-
-- **Operational AI for internal tools**
-Build models and agents to assist with DevOps, customer success, HR automation, sales enablement, and more.
-
-- **Domain-specific model serving**
-Serve proprietary or fine-tuned models (LLMs, classification, ranking) as scalable inference services, integrated with business systems.
-
-You can start small with a single assistant or inference service — and scale to full Gen AI applications that combine pipelines, vector search, model serving, and conversational agents.
-
----
-
-## Learning Paths
-
-Follow our curated learning paths based on your experience level:
-
-- [AI Factory 101](/ai-factory/learn/paths/101) — Introductory concepts and usage
-- [AI Factory 201](/ai-factory/learn/paths/201) — Building and managing Gen AI and AI Factory workloads
-- [AI Factory 301](/ai-factory/learn/paths/301) — Advanced integration, scaling, governance, and optimization
-
----
-
-## Use Cases and Solutions
-
-We provide detailed guidance and patterns to help you build full solutions with AI Factory:
-
-- [Common Use Cases](/ai-factory/learn/use-cases) — Start with proven patterns for AI Factory-powered applications
-- [Industry Solutions](/ai-factory/learn/solutions) — Explore industry-specific ideas and recommended best practices
-
----
-
-## AI Factory in Hybrid Manager Workloads
-
-Hybrid Manager supports the full range of AI Factory capabilities, integrated into its control plane:
-
-### Gen AI Workloads
-
-- [Gen AI Workloads](./gen-ai/index.mdx) — Overview of capabilities in Hybrid Manager
-- [Agent Studio](./gen-ai/agent-studio.mdx) — Assistants, tools, structures, and rulesets for Gen AI
-- [Gen AI Builder](./gen-ai/builder.mdx) — Knowledge bases and data lakes
-
-### Model Management and Serving
-
-- [Model Library](./model/library.mdx) — Manage and govern your model assets
-- [Model Serving](./model/serving.mdx) — Deploy and scale model inference services on Kubernetes
-- [GPU Resource Management](./model/gpu.mdx) — Configure and allocate GPU capacity for serving
-
-### Pipelines and Vector Engine
-
-- [Pipeline Knowledge Base](./pipeline/knowledge-base/index.mdx) — Pipelines for data preparation and document intelligence
-- [Vector Engine](./vector-engine/index.mdx) — Integrated vector search and similarity capabilities with Postgres
-
----
-
-## Hybrid Manager Learn Content
-
-In addition to AI Factory content in the Hub, Hybrid Manager provides additional **Learn** content for HM-specific usage:
-
-- [AI Factory Concepts in Hybrid Manager](../learn/explained/ai-factory/index.mdx)
-- [How-to guides for Hybrid Manager AI Factory](../learn/how-to/ai-factory/index.mdx)
-- [Learning Paths in Hybrid Manager](../learn/paths/ai-factory/index.mdx)
-
----
-
-## Get Started
-
-To get started building with AI Factory in Hybrid Manager:
-
-- Follow the [AI Factory learning paths](/ai-factory/learn/paths/index)
-- Explore [use cases](/ai-factory/learn/use-cases) and [industry solutions](/ai-factory/learn/solutions)
-- Read Hybrid Manager-specific [how-to guides](../learn/how-to/ai-factory/index.mdx)
-- Deploy your first Gen AI assistant or model with [Agent Studio](./gen-ai/agent-studio.mdx) or [Model Serving](./model/serving.mdx)
-
-AI Factory in Hybrid Manager gives you a scalable, secure platform to operationalize AI across your hybrid data estate. Start building today.
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/assistants/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/assistants/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/assistants/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/assistants/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,95 +0,0 @@
----
-title: Understanding and Managing Assistants in Gen AI Builder
-description: Understand what Assistants are in Gen AI Builder, why they matter, and how they power AI-driven interactions and applications.
----
-
-## What are Assistants
-
-**Assistants** in Gen AI Builder are AI-powered agents that perform tasks, interact with users, access knowledge, and adhere to predefined behavioral guidelines.
-They are the central building block for creating interactive, AI-driven applications within the AI Factory ecosystem.
-
-In short:
-**Assistants turn your Knowledge Bases, Rulesets, and Tools into useful, conversational AI experiences.**
-
-For a deep dive, see: [Assistants explained](../../../learn/explained/assistants-explained).
-
-## Why use Assistants
-
-Assistants enable you to:
-
-- Build **interactive AI applications** quickly and reliably.
-- **Ground AI responses** in your organization’s data (via Knowledge Bases).
-- Enforce **behavioral guidelines** through Rulesets.
-- Enable advanced **task execution** through Tools.
-- Maintain **conversational memory** to support multi-turn interactions.
-
-Assistants are the primary interface between end users and your AI Factory content + capabilities.
-
-## When to use Assistants
-
-- Whenever you want to expose AI capabilities to users — internal or external.
-- When building:
-- Support chatbots
-- Internal knowledge agents
-- Financial advisors
-- Sales assistants
-- Executive assistants
-- Compliance checkers
-- When building **multi-modal Agents** that combine:
-- Natural language conversation
-- Knowledge retrieval (RAG)
-- Behavior control
-- Task execution
-
-## How Assistants fit into Gen AI Builder
-
-Typical Assistant flow:
-
-User Input → Assistant → Retriever → Knowledge Bases → Retrieved Content
-→ Rulesets → Behavioral Guidance
-→ Tools → Action Execution (if configured)
-→ Response Generation → User Output
-
-
-
-At runtime:
-
-- The Assistant receives user input.
-- It retrieves relevant knowledge.
-- It applies behavioral Rulesets.
-- It uses Tools if needed.
-- It generates a response via its selected LLM.
-
-Assistants provide a **unified layer** over all these components.
-
-## Key features of Assistants
-
-- **LLM integration:** Powered by your choice of Large Language Models (LLMs).
-- **Knowledge Base connectivity:** Link one or more Knowledge Bases for Retrieval-Augmented Generation (RAG).
-- **Ruleset application:** Enforce behavioral guidelines and tone.
-- **Tool usage:** (If configured) enable Assistants to perform external actions.
-- **Conversation management:** Maintain memory across interactions.
-- **Customization:** Tailor Assistant behavior, knowledge access, and generation parameters.
-
-## Getting started
-
-See [Create an Assistant](../../../how-to/gen-ai/create-assistant) for a full step-by-step guide.
-
-Typical workflow:
-
-1. Create an Assistant.
-2. Select LLM model.
-3. Add Knowledge Bases.
-4. Add Rulesets.
-5. Optionally configure Tools and Memory.
-6. Test and deploy the Assistant.
-
-## Related topics
-
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Create an Assistant](../../../how-to/gen-ai/create-assistant)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Structures explained](../../../learn/explained/structures)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/rulesets/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/rulesets/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/rulesets/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/rulesets/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,87 +0,0 @@
----
-title: Working with Rulesets in Gen AI Builder
-description: Understand what Rulesets are in Gen AI Builder, why they matter, and how they guide the behavior of Assistants and Structures.
----
-
-## What are Rulesets
-
-**Rulesets** in Gen AI Builder are collections of natural language instructions that guide the behavior of your Assistants and Structures.
-They help ensure responses are aligned with your specific instructions and organizational guidelines.
-
-Rulesets serve as a **layer of explicit control** over how your AI behaves — complementing the knowledge it retrieves from Knowledge Bases.
-
-In short:
-**Rulesets tell your Assistants *how* to behave, not just *what* to retrieve.**
-
-For a deep dive, see: [Rulesets explained](../../../learn/explained/rulesets-explained).
-
-## Why use Rulesets
-
-- **Guide behavior:** Ensure Assistants respond in ways that match your tone, style, and policies.
-- **Enforce constraints:** Prohibit certain types of content or behaviors (e.g., avoid legal advice, don’t mention competitors).
-- **Support multiple personas:** Define distinct behavioral patterns for different Assistants.
-- **Complement RAG pipelines:** Combine retrieved knowledge with consistent behavior.
-
-Rulesets are particularly powerful when combined with:
-
-- Knowledge Bases (what content to retrieve)
-- Structures (how to process tasks)
-- Tools (how to interact with external systems)
-
-## When to use Rulesets
-
-- When building Assistants that interact with end users.
-- When defining behavior for internal Agents and Pipelines.
-- Whenever you want to ensure **consistent tone, style, or compliance** across AI applications.
-
-**Examples:**
-
-- Customer support Assistants using polite and professional tone.
-- Internal policy chatbots with strict legal disclaimers.
-- Marketing copy Assistants that reflect brand guidelines.
-
-## How Rulesets fit into Gen AI Builder
-
-The typical workflow is:
-
-```Create Ruleset → Add Rules → Create Assistant → Assign Ruleset to Assistant```
-
-
-At runtime:
-
-- The assigned Ruleset is applied to every message processed by the Assistant.
-- The Assistant uses these Rules in combination with retrieved content and its base LLM.
-
-## Components of a Ruleset
-
-- **Name** — Required, unique name.
-- **Description** — Optional description.
-- **Rules** — One or more Rules, each written in natural language.
-- **Alias** — Optional unique alias (for versioning or API access).
-- **Metadata** — Optional JSON object for advanced configuration.
-
-**Rules** themselves consist of:
-
-- Name (required)
-- Rule text (required) — written as an instruction in natural language.
-
-## Getting started
-
-See [Create a Ruleset](../../../how-to/gen-ai/create-ruleset) for a full step-by-step guide.
-
-Example flow:
-
-1. Create a **Ruleset** → "Polite Support Tone"
-2. Create Rules:
-- "Use Formal Salutations"
-- "Offer Assistance Clearly"
-3. Create an Assistant and assign the Ruleset.
-4. The Assistant now responds with this defined tone and style.
-
-## Related topics
-
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Create a Ruleset](../../../how-to/gen-ai/create-ruleset)
-- [Structures explained](../../../learn/explained/structures)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/structures/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/structures/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/structures/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/structures/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,47 +0,0 @@
-
-At runtime:
-
-- Structures can run:
-- On demand via UI.
-- Programmatically via API.
-- Triggered by Assistant Tools.
-
-## Key features of Structures
-
-- Based on **Griptape Python agents, pipelines, workflows**.
-- Deployed from:
-- Data Lake zip file
-- GitHub repository
-- Provided samples
-- Configurable via:
-- Environment variables
-- Structure Config files (YAML / Python)
-- Executable via:
-- PG.AI Web Console
-- API
-- Third-party integrations
-
-## Getting started
-
-See [Create a Structure](../../../how-to/gen-ai/create-structure) for a full step-by-step guide.
-
-Typical workflow:
-
-1. Package your Griptape Structure as a zip file.
-2. Upload to your Data Lake.
-3. Create a Structure in Gen AI Builder.
-4. Configure Structure parameters.
-5. Execute Structure:
-- Directly
-- As part of Data Source transformation
-- As an Assistant Tool
-
-## Related topics
-
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Create a Structure](../../../how-to/gen-ai/create-structure)
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/threads/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/threads/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/threads/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/threads/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,121 +0,0 @@
----
-title: Working with Threads in Gen AI Builder
-description: Understand what Threads are in Gen AI Builder, why they matter, and how they provide conversation history, state persistence, and quality insights.
----
-
-## What are Threads
-
-**Threads** in Gen AI Builder represent **individual conversations** or interaction histories, typically with an AI Assistant.
-They allow you to review past interactions, understand message flow, and analyze Assistant behavior.
-
-A Thread captures:
-
-- The entire sequence of messages between a user and an AI Assistant.
-- Associated metadata (timestamps, IDs, context state).
-- System-level details (Tools used, Rulesets applied, memory state).
-
-In short:
-**Threads provide the *conversation memory and history* that enables analysis, debugging, and state persistence in your AI Factory applications.**
-
-For a deep dive, see: [Threads explained](../../../learn/explained/threads-explained).
-
-## Why use Threads
-
-- **Review conversation history:** Understand exactly what the user and Assistant said.
-- **Debug behavior:** Analyze how Assistants behaved in specific interactions.
-- **Analyze user needs:** See patterns of user queries and intents.
-- **Support compliance audits:** Provide a record of AI responses.
-- **Manage context state:** Track conversation state for Assistants with memory.
-
-Threads are essential for **production-grade AI applications** where quality, compliance, and traceability matter.
-
-## When to use Threads
-
-Use Threads when you need to:
-
-- Review recent interactions for QA or user support.
-- Audit Assistant responses for compliance.
-- Debug unexpected Assistant behavior.
-- Identify points of friction or failure in conversations.
-- Analyze common user questions and improve knowledge coverage.
-- Manage or reset conversation context for long-running Assistants.
-
-## How Threads fit into Gen AI Builder
-
-```
-User Input → Assistant → Retriever → Knowledge Bases → Retrieved Content
-→ Rulesets → Behavioral Guidance
-→ Tools → Action Execution
-→ Memory → Context Maintenance
-→ LLM → Response → User Output → Thread recorded
-```
-
-
-Every conversation with an Assistant:
-
-- Creates or continues a **Thread**.
-- Records:
-- Messages
-- Message metadata
-- Tool calls and results
-- Memory state (if applicable)
-
-## Key features of Threads
-
-- **Conversation history:** Full message log with timestamps.
-- **Searchable:** Find Threads by name, alias, or content.
-- **Thread metadata:** ID, created/updated times, Assistant association.
-- **Message metadata:** Message ID, content, Tool calls.
-- **Context state:** Memory and conversation context tracked across turns.
-- **QA and debugging:** Inspect message flow and Assistant behavior.
-
-## Typical patterns of use
-
-### Conversation QA
-
-- Review recent Threads to verify tone, accuracy, and compliance.
-
-### Debugging
-
-- Investigate specific user complaints or unexpected behaviors.
-
-### Compliance auditing
-
-- Provide conversation transcripts for audit purposes.
-
-### Memory management
-
-- Track and manage memory state across Threads.
-
-### Conversation analytics
-
-- Identify trends and common user questions across Threads.
-
-## Getting started
-
-See [View and manage Threads](../../../how-to/gen-ai/view-threads) for a full step-by-step guide.
-
-Typical workflow:
-
-1. Navigate to **Threads**.
-2. Search or browse to find relevant Thread.
-3. Open Thread and review:
-- Conversation flow
-- Assistant responses
-- Tool invocations
-- Context state
-4. Take action:
-- Edit or delete Thread (if needed).
-- Export Thread for QA or audit.
-- Use findings to improve Assistant configuration.
-
-## Related topics
-
-- [Threads explained](../../../learn/explained/threads-explained)
-- [View and manage Threads](../../../how-to/gen-ai/view-threads)
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/tools/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/tools/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/tools/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/tools/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,99 +0,0 @@
----
-title: Working with Tools in Gen AI Builder
-description: Understand what Tools are in Gen AI Builder, why they matter, and how they extend the capabilities of AI Assistants and Structures.
----
-
-## What are Tools
-
-**Tools** in Gen AI Builder are Griptape-powered components that allow AI Assistants and Structures to perform actions beyond basic text generation.
-They provide the ability to interact with external systems, perform calculations, retrieve live data, and execute custom logic.
-
-Tools make AI applications **active** — capable of not just answering questions, but taking meaningful action.
-
-In short:
-**Tools give your AI Assistants and Structures *superpowers* by extending them with actionable capabilities.**
-
-For a deep dive, see: [Tools explained](../../../learn/explained/tools-explained).
-
-## Why use Tools
-
-- **Extend LLM capabilities:** Perform actions that LLMs alone cannot do.
-- **Integrate with external systems:** APIs, databases, services.
-- **Enhance user experience:** Provide dynamic responses based on real-time data.
-- **Enable business processes:** Automate workflows triggered by user interactions.
-- **Modularize logic:** Package reusable functions that can be invoked across Assistants and Structures.
-
-Without Tools, your Assistants and Structures are limited to retrieving knowledge and generating text.
-With Tools, they can **act**, **compute**, **query**, and **integrate**.
-
-## When to use Tools
-
-Use Tools when you want Assistants or Structures to:
-
-- Query external APIs (e.g., stock prices, weather, exchange rates).
-- Perform calculations or business logic.
-- Access internal databases or services.
-- Enrich conversations with live or dynamic data.
-- Automate multi-step processes.
-- Provide custom retrieval or transformation logic.
-
-## How Tools fit into Gen AI Builder
-
-Runtime flow with Tools:
-
-```
-User Input → Assistant → Retriever → Knowledge Bases → Retrieved Content
-→ Rulesets → Behavioral Guidance
-→ Tools → Action Execution
-→ Memory → Context Maintenance
-→ LLM → Response → User Output
-```
-
-
-
-Tools can also be invoked:
-
-- **Directly** by Assistants via Tool calls.
-- **Indirectly** as part of a Structure execution.
-- **In Data Source pipelines** for data transformation.
-
-## Key features of Tools
-
-- Implemented as **Griptape Tools** (Python classes).
-- Packaged as Zip files or deployed from GitHub.
-- Configurable with:
-- Tool Config file (YAML / Python)
-- Environment variables (API keys, configuration)
-- Executable by:
-- Assistants (via LLM-driven tool invocation)
-- Structures (as components of workflows)
-
-## Typical Tool patterns
-
-- **API Connectors:** Query external services.
-- **Calculators:** Perform complex calculations or conversions.
-- **Data Fetchers:** Retrieve data from internal systems.
-- **Data Transformers:** Preprocess or post-process data.
-- **Process Automators:** Orchestrate multi-step workflows.
-
-## Getting started
-
-See [Create a Tool](../../../how-to/gen-ai/create-tool) for a full step-by-step guide.
-
-Typical workflow:
-
-1. Develop Griptape Tool locally.
-2. Package Tool as Zip.
-3. Upload Tool Zip to Data Lake.
-4. Create Tool in Gen AI Builder.
-5. Assign Tool to Assistants or Structures.
-
-## Related topics
-
-- [Tools explained](../../../learn/explained/tools-explained)
-- [Create a Tool](../../../how-to/gen-ai/create-tool)
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/agent-studio/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,123 +0,0 @@
----
-title: Agent Studio in Gen AI Builder
-description: Understand and manage Assistants, Structures, Rulesets, Tools, and Threads — the core building blocks for interactive and task-based AI applications in Gen AI Builder.
----
-
-## What is Agent Studio
-
-**Agent Studio** in Gen AI Builder is where you create and manage the key components that bring your AI Factory applications to life.
-
-Agent Studio enables you to:
-
-- Build **Assistants** that interact with users.
-- Package **Structures** that perform complex AI workflows.
-- Define **Rulesets** to control Assistant behavior.
-- Provide **Tools** to extend Assistant and Structure capabilities.
-- Manage **Threads** to analyze conversation history and maintain state.
-
-In short:
-**Agent Studio is where you assemble, govern, and analyze the intelligent behaviors of your AI-driven applications.**
-
-## Why use Agent Studio
-
-- To create rich, interactive AI applications.
-- To apply business logic and organizational rules to AI interactions.
-- To integrate AI with external data and systems.
-- To ensure controlled, compliant, and high-quality AI behavior.
-- To gain visibility into conversations and continuously improve AI performance.
-
-Agent Studio is the **workbench** for designing production-ready AI experiences.
-
-## Core components
-
-### [Assistants](assistants)
-
-**AI agents** that converse with users, powered by LLMs, Knowledge Bases, Tools, Rulesets, and Memory.
-
-Use Assistants to:
-
-- Build chatbots and copilots.
-- Implement internal and external virtual agents.
-- Connect users with AI-driven business logic.
-
-* Learn more:
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Create an Assistant](../../../how-to/gen-ai/create-assistant)
-- Reference (placeholder): `gen-ai/reference/assistants`
-
----
-
-### [Structures](structures)
-
-**Griptape-powered agents, pipelines, and workflows** that encapsulate business logic for AI-driven tasks.
-
-Use Structures to:
-
-- Perform data transformations.
-- Implement multi-step reasoning.
-- Provide Tools and Data Source pipelines.
-
-* Learn more:
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Create a Structure](../../../how-to/gen-ai/create-structure)
-- Reference (placeholder): `gen-ai/reference/structures`
-
----
-
-### [Rulesets](rulesets)
-
-**Collections of behavioral rules** that guide how Assistants interact with users.
-
-Use Rulesets to:
-
-- Enforce compliance.
-- Maintain brand tone.
-- Implement conversational policies.
-
-* Learn more:
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Create a Ruleset](../../../how-to/gen-ai/create-ruleset)
-- Reference (placeholder): `gen-ai/reference/rulesets`
-
----
-
-### [Tools](tools)
-
-**Griptape Tools** that Assistants and Structures can call to perform external actions.
-
-Use Tools to:
-
-- Query APIs.
-- Perform calculations.
-- Fetch live data.
-- Automate workflows.
-
-* Learn more:
-- [Tools explained](../../../learn/explained/tools-explained)
-- [Create a Tool](../../../how-to/gen-ai/create-tool)
-- Reference (placeholder): `gen-ai/reference/tools`
-
----
-
-### [Threads](threads)
-
-**Conversation history and context state** for each interaction with an Assistant.
-
-Use Threads to:
-
-- Audit Assistant behavior.
-- Debug conversations.
-- Analyze user interaction patterns.
-- Manage conversation memory.
-
-* Learn more:
-- [Threads explained](../../../learn/explained/threads-explained)
-- [View and manage Threads](../../../how-to/gen-ai/view-threads)
-- Reference (placeholder): `gen-ai/reference/threads`
-
----
-
-## Related concepts
-
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/data-lake/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/data-lake/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/data-lake/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/data-lake/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,68 +0,0 @@
----
-title: Working with Data Lake in Gen AI Builder
-description: Understand what the Data Lake is in Gen AI Builder, why it matters, and how it fits into the AI Factory content pipeline.
----
-
-## What is the Data Lake
-
-The **Data Lake** is the foundational object storage layer for Gen AI Builder and the AI Factory.
-It is where Griptape-powered services store:
-
-- Uploaded files from Data Sources
-- Indexed data and embeddings
-- Griptape Structures and Tools
-- Temporary artifacts used in AI workflows
-
-Without a properly configured Data Lake, core features of Gen AI Builder — including Libraries, Knowledge Bases, and Retrievers — cannot function.
-
-For a deep dive, see: [Data Lake explained](../../../learn/explained/data-lake-explained).
-
-## Why use a dedicated Data Lake
-
-- **Separation of concerns:** Keeps Griptape data isolated from other object storage.
-- **Performance:** Enables optimized patterns for AI workloads.
-- **Security:** Simplifies permission scoping and CORS configuration.
-- **Reliability:** Ensures the AI indexing and retrieval pipelines have guaranteed storage availability.
-
-**Best practice:**
-Use a dedicated bucket or container **only for Gen AI Builder / Griptape**.
-
-## When to configure the Data Lake
-
-- When setting up a new Griptape deployment.
-- When connecting an external storage backend to Gen AI Builder.
-- Before adding Data Sources or creating Knowledge Bases.
-
-**Important:**
-The Data Lake must be configured **before using Libraries or Knowledge Bases**.
-
-## How does the Data Lake fit into the content pipeline
-
-The Data Lake supports the entire AI Factory pipeline:
-
-Data Sources → Data Lake → Libraries → Knowledge Bases → Retrievers → Assistants → AI Applications
-
-
-The Data Lake is the persistent storage backend that powers:
-
-- Data Sources of type **Data Lake**
-- Content staging during Library and Knowledge Base creation
-- Storage for Griptape Structures and Tools
-
-## Getting started
-
-To configure the Data Lake:
-
-- Provision an object storage bucket (S3-compatible or GCS).
-- Configure CORS to enable UI interaction.
-- Obtain and provide required credentials to your deployment.
-
-See [Configure the Data Lake](../../../how-to/gen-ai/configure-datalake) for the full step-by-step guide.
-
-## Related topics
-
-- [Data Lake explained](../../../learn/explained/data-lake-explained)
-- [Configure the Data Lake](../../../how-to/gen-ai/configure-datalake)
-- [Configure Data Sources](../data-sources)
-- [Configure Knowledge Bases](../knowledge-bases)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/data-sources.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/data-sources.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/data-sources.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/data-sources.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,97 +0,0 @@
----
-title: Configure Data Sources in Gen AI Builder
-description: Learn how to configure and manage Data Sources in Gen AI Builder to bring external and internal content into your Knowledge Bases.
----
-
-## Who is this for
-
-Platform users building Knowledge Bases and AI applications using Gen AI Builder.
-This includes developers, AI engineers, content managers, and system integrators.
-
-If you are responsible for bringing external or internal content into your AI workflows—whether from documents, web pages, cloud storage, or proprietary systems—this set of guides will show you how.
-
-## What you will accomplish
-
-You will learn how to configure and manage different types of Data Sources in Gen AI Builder.
-These sources provide the raw content that populates Libraries and Knowledge Bases for your AI applications.
-
-## Why use Data Sources
-
-- The quality and relevance of your AI applications depend heavily on the content you ingest.
-- Gen AI Builder allows you to connect to a variety of content sources and manage the ingestion pipeline securely and efficiently.
-- Using Data Sources ensures your Knowledge Bases stay fresh, accurate, and aligned with your organization’s needs.
-
-## About Data Sources in Gen AI Builder
-
-Data Sources are the first step in bringing content into the system.
-The content from Data Sources flows into **Libraries**, and then into **Knowledge Bases**, where it is indexed and made available for AI workloads.
-
-Common types of Data Sources include:
-
-- Web pages
-- Data Lakes
-- Amazon S3 buckets
-- Google Drive folders
-- Atlassian Confluence spaces
-- Custom systems via PG.AI Structures
-
-**Key features:**
-
-- Secure connection to data
-- Scheduled refresh to keep content up to date
-- Optional transformation with PG.AI Structures
-- Integration with downstream Libraries and Knowledge Bases
-
-For foundational concepts, see:
-
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Structures explained](../../../learn/explained/structures)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-
-For guidance on using this content in AI applications:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-
-To see how Data Sources are used within **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## How-to Guides
-
-Use the following guides to configure each type of Data Source:
-
-- [Configure a Web Page data source](data-source-webpage)
-- [Configure a Data Lake data source](data-source-data-lake)
-- [Configure an Amazon S3 data source](data-source-s3)
-- [Configure a Google Drive data source](data-source-google-drive)
-- [Configure an Atlassian Confluence data source](data-source-confluence)
-- [Configure a Custom data source](data-source-custom)
-
-Each guide provides:
-
-- Who it is for
-- What you will accomplish
-- Why to use this source
-- Detailed configuration steps
-- Key considerations and best practices
-- Troubleshooting tips
-- Example scenarios
-
-## Next steps
-
-Once you have configured Data Sources, proceed to:
-
-- Configure Libraries to organize your content.
-- Build Knowledge Bases to enable semantic search and RAG applications.
-- Connect your Knowledge Bases to AI agents and workflows using Gen AI Builder and Structures.
-
-For more advanced integrations, see:
-
-- [Building AI Workflows with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Structures explained](../../../learn/explained/structures)
-
----
-
-By thoughtfully configuring Data Sources, you will lay a strong foundation for high-quality, context-aware AI applications in your organization.
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,123 +0,0 @@
----
-title: Managing Data with Libraries in Gen AI Builder
-description: Understand what Libraries are in Gen AI Builder, why they matter, and how they power AI applications through Data Sources, Knowledge Bases, and Retrievers.
----
-
-## What are Libraries
-
-Libraries are a fundamental component of Gen AI Builder.
-They enable you to connect, organize, and prepare your data for use in AI-powered applications.
-
-Libraries serve as the **primary mechanism for managing knowledge sources** that your Griptape Structures — including Agents, Pipelines, and Assistants — leverage for Retrieval Augmented Generation (RAG) tasks.
-
-In short:
-**Libraries are the curated, organized collections of information your AI can "know" and retrieve.**
-
-## Why use Libraries
-
-Libraries allow you to:
-
-- **Centralize data** for AI: Aggregate diverse data sources into unified, accessible knowledge pools.
-- **Power Retrieval Augmented Generation (RAG):** Provide contextual data to AI structures for accurate, grounded responses.
-- **Fuel Assistants and other Structures:** Make relevant content queryable and available to Griptape-based Agents and Pipelines.
-- **Simplify data management:** Manage the full data lifecycle — connection, processing, indexing — through an intuitive UI.
-- **Support Knowledge Bases:** Libraries feed content into the Knowledge Bases that your AI Assistants ultimately query.
-
-## How Libraries fit into Gen AI Builder
-
-The content pipeline in Gen AI Builder looks like this:
-
-Data Sources → Libraries → Knowledge Bases → Retrievers → Assistants → AI Applications
-
-
-- **Data Sources:** Raw content (web pages, documents, databases, cloud storage, APIs)
-- **Libraries:** Organize and index content from Data Sources
-- **Knowledge Bases:** Searchable layer built from Libraries
-- **Retrievers:** Define how Assistants query Knowledge Bases
-- **Assistants:** Use retrieved content to generate grounded AI responses
-
-For background on key concepts, see:
-
-- [AI Factory Concepts](../../learn/explained/ai-factory-concepts)
-- [Knowledge Bases explained](../../learn/explained/knowledge-bases)
-- [Retrievers explained](../../learn/explained/retrievers-explained)
-- [Retrieval Augmented Generation (RAG)](../../learn/explained/rag)
-
-To see how Libraries are used in **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Components of Libraries
-
-### Data Sources
-
-Libraries connect to one or more **Data Sources**, including:
-
-- Web Pages
-- PG.AI Data Lake
-- Amazon S3
-- Google Drive
-- Atlassian Confluence
-- Custom Data Sources (via Griptape Structures)
-
-Learn how to configure Data Sources:
-
-- [Configure Data Sources](./data-sources)
-
-### Knowledge Bases
-
-Libraries feed content into **Knowledge Bases**, which index and optimize content for fast, semantic search.
-
-Learn how to configure Knowledge Bases:
-
-- [Configure Knowledge Bases](./knowledge-bases)
-
-### Retrievers
-
-Retrievers define how Assistants query Knowledge Bases — selecting and formatting retrieved content for AI use.
-
-Learn how to configure Retrievers:
-
-- [Configure Retrievers](./retrievers)
-
-## When to use Libraries
-
-Use Libraries whenever:
-
-- You want to manage content pipelines for AI applications.
-- You need to organize content from multiple Data Sources for use in Knowledge Bases.
-- You are building Assistants that require grounded responses based on your content.
-
-**Common examples:**
-
-- Centralizing product documentation for support chatbots.
-- Building corporate policy Libraries to power HR Agents.
-- Creating Libraries of financial data to enable advanced RAG-based analytics.
-
-## Best practices for Libraries
-
-- **Focused content:** Create Libraries with a clear, specific scope.
-- **Data quality:** Ensure source data is accurate, current, and well structured.
-- **Incremental additions:** Start small, expand incrementally, test at each step.
-- **Security:** Follow the principle of least privilege for Data Sources.
-- **Regular review:** Periodically review and update Libraries to maintain relevance and quality.
-
-## Next steps
-
-To start working with Libraries:
-
-1. [Configure Data Sources](./data-sources)
-2. [Configure Knowledge Bases](./knowledge-bases)
-3. [Configure Retrievers](./retrievers)
-4. Connect Libraries and Retrievers to Assistants to enable Retrieval Augmented Generation.
-
-For procedural steps, see the How-to Guides in this section.
-
-## Related topics
-
-- [AI Factory Concepts](../../learn/explained/ai-factory-concepts)
-- [Knowledge Bases explained](../../learn/explained/knowledge-bases)
-- [Retrievers explained](../../learn/explained/retrievers-explained)
-- [Retrieval Augmented Generation (RAG)](../../learn/explained/rag)
-- [Structures explained](../../learn/explained/structures)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/knowledge-bases.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/knowledge-bases.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/knowledge-bases.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/knowledge-bases.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,65 +0,0 @@
----
-title: Working with Knowledge Bases in Gen AI Builder
-description: Learn how to create and manage Knowledge Bases in Gen AI Builder to organize and make your content available to AI applications.
----
-
-## Who is this for
-
-Platform users building AI applications with context-aware responses powered by Knowledge Bases.
-Typical users include developers, AI architects, data engineers, and business owners managing critical content.
-
-## What you will accomplish
-
-You will learn how to create, view, and manage Knowledge Bases in Gen AI Builder.
-Knowledge Bases enable you to organize content from Libraries and make it queryable by AI agents, powering semantic search and Retrieval-Augmented Generation (RAG).
-
-## Why use Knowledge Bases
-
-- Knowledge Bases enable AI models to access your organization's specific knowledge.
-- They provide high-quality, context-rich answers by grounding LLMs in your data.
-- They simplify the management and updating of content pipelines through integration with Libraries.
-
-For foundational concepts, see:
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases) <!-- We will generate this explainer -->
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Structures explained](../../../learn/explained/structures)
-
-To see how Knowledge Bases are used within **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## How-to Guides
-
-Use the following guides to configure and manage your Knowledge Bases:
-
-- [Create a Knowledge Base](../how-to/create-knowledge-base)
-- [Manage Knowledge Bases](../how-to/manage-knowledge-base)
-- [Best practices for Hybrid Knowledge Bases](../how-to/hybrid-kb-best-practices)
-
-Each guide provides:
-
-- Who it is for
-- What you will accomplish
-- Why to use this approach
-- Detailed step-by-step instructions
-- Best practices and troubleshooting tips
-
-## Next steps
-
-Once your Knowledge Bases are configured:
-
-- Connect AI agents and applications to your Knowledge Bases using Gen AI Builder.
-- Design hybrid Knowledge Bases to combine structured and unstructured data.
-- Implement RAG workflows with your Knowledge Bases.
-
-Explore:
-
-- [Building AI Workflows with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Structures explained](../../../learn/explained/structures)
-
----
-
-By thoughtfully managing Knowledge Bases, you will enable your AI applications to deliver accurate, context-aware results grounded in your organization’s knowledge.
Index: advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/retrivers.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/retrivers.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/retrivers.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/gen-ai/builder/libraries/retrivers.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,92 +0,0 @@
----
-title: Working with Retrievers in Gen AI Builder
-description: Understand what Retrievers are, when and why to use them, and how they power AI Assistants in Gen AI Builder.
----
-
-## What is a Retriever
-
-A **Retriever** in Gen AI Builder is a specialized **Griptape Structure** that defines how your AI Assistants retrieve information from Knowledge Bases.
-Retrievers enable precise, customizable information retrieval — the core of **Retrieval-Augmented Generation (RAG)** workflows.
-
-At query time, your Assistant uses its configured Retriever(s) to:
-
-- Search one or more Knowledge Bases
-- Select the most relevant results ("chunks") based on semantic similarity and other filters
-- Feed that context into the Large Language Model (LLM) to generate an informed response
-
-**In short:**
-A Retriever connects user queries → Knowledge Base content → AI answers.
-
----
-
-For a deep dive, see [Retrievers explained](../../../learn/explained/retrievers-explained).
-For step-by-step configuration, see [Create a Retriever](../../../how-to/gen-ai/create-retriever).
-
----
-
-## Why use Retrievers
-
-- **Ground AI responses:** Retrievers ensure that Assistants provide factually grounded answers based on your organization’s Knowledge Bases.
-- **Support explainability:** Retrieved content can be referenced in the Assistant’s responses, enabling transparent and auditable AI.
-- **Control search behavior:** You can configure which Knowledge Bases are queried, control token limits, and supply advanced metadata.
-- **Enable multi-source RAG:** Retrievers can target multiple Knowledge Bases simultaneously — useful for complex Assistants with broad knowledge scope.
-
-## When to use Retrievers
-
-- Whenever you are building an AI Assistant that needs to answer questions based on internal content.
-- Whenever your content is stored in **Knowledge Bases** and you want dynamic, query-time retrieval.
-- When you want your AI application to support **semantic search** and not rely solely on the LLM’s internal training.
-
-**Examples:**
-
-- Support chatbots answering questions from product documentation
-- Financial advisors surfacing policies and legal disclaimers on demand
-- Sales tools providing quick access to internal pricing documents and competitive intelligence
-
-## How do Retrievers work
-
-At runtime, the process is:
-
-1. User asks a question.
-2. Assistant forwards the query to its assigned Retriever(s).
-3. Retriever runs semantic + optional structured search across configured Knowledge Bases.
-4. Retriever returns the most relevant results.
-5. Assistant passes this context to the LLM.
-6. LLM generates a response incorporating the retrieved information.
-
-**Under the hood:**
-A Retriever is implemented as a Griptape Structure, giving you flexibility in how it performs retrieval.
-
-See [Griptape concepts](../../../learn/explained/ai-factory-concepts) for more on Structures.
-
-## Key components of a Retriever
-
-- **Knowledge Bases targeted:** Determines the scope of retrieval.
-- **Max Tokens:** Controls how much content is retrieved per query.
-- **Metadata:** Optional structured data for advanced configuration or tagging.
-- **Associated Assistants:** Controls which Assistants can use this Retriever.
-
-## Getting started
-
-### Pre-requisites
-
-- One or more **Knowledge Bases** created and populated in Gen AI Builder.
-
-### First steps
-
-- Create a Retriever and configure it to target your Knowledge Bases.
-- Associate the Retriever with an AI Assistant.
-- Test the Assistant to verify that retrieved content is correctly influencing responses.
-
-See [Create a Retriever](../../../how-to/gen-ai/create-retriever) for the full step-by-step guide.
-
----
-
-## Related topics
-
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Create a Retriever](../../../how-to/gen-ai/create-retriever)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Structures explained](../../../learn/explained/structures)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/ai-factory-concepts.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/ai-factory-concepts.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/ai-factory-concepts.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/ai-factory-concepts.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,149 +0,0 @@
----
-title: AI Factory concepts
-navTitle: Concepts
-description: EDB’s vision, strategy, and technologies for delivering AI Factory capabilities in Postgres and Hybrid Manager environments.
----
-
-AI Factory concepts explain how EDB integrates modern AI capabilities across its platform — including Postgres®, Hybrid Manager (HM), and the AI Factory services.
-
-**Why it matters:**
-AI is transforming data infrastructure, applications, and developer workflows. EDB’s AI Factory is designed to help you:
-
-- Embed AI in Postgres and database-driven apps
-- Build scalable AI-powered applications
-- Operate AI workloads on modern cloud and hybrid infrastructure
-- Simplify AI model serving and governance
-- Support key patterns like Retrieval-Augmented Generation (RAG) and semantic search
-
----
-
-## EDB’s vision for AI with Postgres and Hybrid Manager
-
-EDB enables AI workloads in several complementary ways:
-
-- **AI in databases:** Use AIDB to perform vector search, embeddings, and ML functions directly in Postgres.
-- **AI-powered apps:** Build apps using Gen AI Builder (Griptape-based) integrated with Knowledge Bases and model serving.
-- **AI infrastructure:** Operate models on cloud-native infrastructure via KServe with optimized GPU resources.
-- **AI management:** Leverage Hybrid Manager for unified model lifecycle management and observability.
-
-**Goal:** Make Postgres a first-class component of the modern AI stack, and provide a production-ready control plane for AI-driven applications.
-
-[Related: AI Factory generic concepts](./generic-concepts)
-
----
-
-## Core AI patterns supported
-
-### Vector search and semantic search
-
-Use vector databases and pgvector in Postgres to support:
-
-- Semantic search
-- Product recommendations
-- Anomaly detection
-- RAG pipelines
-
-**Mapped to:**
-[Vector search in AIDB](../hybrid-manager/ai-factory/vector_search) *(planned)*
-[Vector search How-To guides](../hybrid-manager/ai-factory/learn/how-to/index) *(planned)*
-
----
-
-### Retrieval-Augmented Generation (RAG)
-
-Combine vector search with LLMs to enable:
-
-- More accurate and grounded AI responses
-- Domain-specific LLM applications
-- Compliance-aware AI apps
-
-**Mapped to:**
-[Building RAG pipelines with HCP AI Factory](../hybrid-manager/ai-factory/rag_architecture) *(planned)*
-[Deploying LLMs for RAG with KServe](../hybrid-manager/ai-factory/llm_serving) *(planned)*
-
----
-
-### AI in database (In-DB ML and vector search)
-
-AIDB enables:
-
-- Vector similarity search inside Postgres
-- Embedding pipelines integrated with Postgres data
-- Future: in-database model scoring and ML operations
-
-**Mapped to:**
-[AIDB concepts](../hybrid-manager/ai-factory/aidb_concepts) *(planned)*
-[AIDB How-To guides](../hybrid-manager/ai-factory/learn/how-to/index) *(planned)*
-
----
-
-### Model serving and inference with KServe
-
-Run AI models as scalable inference services:
-
-- LLMs, embedding models, vision models
-- GPU-accelerated inference
-- KServe-backed InferenceServices with optional Transformers and Explainers
-
-**Mapped to:**
-[KServe model serving in HCP AI Factory](../hybrid-manager/ai-factory/kserve_concepts) *(planned)*
-[Deploying models with KServe](../hybrid-manager/ai-factory/learn/how-to/deploy_kserve_model) *(planned)*
-
----
-
-### Gen AI Builder (Griptape-based app development)
-
-Rapidly build AI applications that:
-
-- Use LLMs with tools, memory, and control
-- Integrate with AIDB Knowledge Bases
-- Call external APIs and databases
-- Support modular and maintainable AI workflows
-
-**Mapped to:**
-[Gen AI Builder concepts](../hybrid-manager/ai-factory/genai_builder_concepts) *(planned)*
-[Building apps with Gen AI Builder](../hybrid-manager/ai-factory/learn/how-to/build_genai_app) *(planned)*
-
----
-
-## Key architectural principles
-
-### Modular architecture
-
-AI Factory is designed to:
-
-- Run on hybrid infrastructure (on-prem, cloud, multi-cloud)
-- Integrate with Postgres and cloud-native services
-- Provide composable building blocks for AI workloads
-
-### Open standards and interoperability
-
-- **LLM support:** KServe with open model formats
-- **Vector search:** pgvector and Postgres, or external vector stores
-- **Framework flexibility:** Griptape enables LLM-agnostic app development
-- **Data portability:** Support for RAG patterns, LLM fine-tuning, and model swapping
-
----
-
-## Summary of capabilities and mapped components
-
-| Capability | EDB AI Factory Component | Planned HM Spoke Page |
-|------------|--------------------------|----------------------|
-| Vector search | AIDB (pgvector) | ../hybrid-manager/ai-factory/vector_search |
-| RAG pipelines | AIDB + KServe + Griptape | ../hybrid-manager/ai-factory/rag_architecture |
-| AI in database | AIDB | ../hybrid-manager/ai-factory/aidb_concepts |
-| Model serving | KServe | ../hybrid-manager/ai-factory/kserve_concepts |
-| Gen AI app dev | Gen AI Builder (Griptape) | ../hybrid-manager/ai-factory/genai_builder_concepts |
-| AI-driven infra | GPUs + Kubernetes + KServe | ../hybrid-manager/ai-factory/infra_concepts |
-
----
-
-## Next steps
-
-- [AI Factory terminology](./terminology) — Definitions of key terms and components
-- [AI Factory generic concepts](./generic-concepts) — Industry concepts that inform the AI Factory design
-- [AI Factory learning guide](../learn/index) *(planned)*
-- [AI Factory in Hybrid Manager](../hybrid-manager/ai-factory/index) *(planned spoke root)*
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/assistants-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/assistants-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/assistants-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/assistants-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,154 +0,0 @@
----
-title: Assistants explained
-description: Understand the role of Assistants in Gen AI Builder and how they power AI-driven conversational applications.
----
-
-## What is an Assistant
-
-An **Assistant** in Gen AI Builder is an AI-powered agent designed to interact with users, perform tasks, access knowledge, and follow predefined behavioral guidelines.
-
-Assistants unify:
-
-- **Natural language interaction** via LLMs
-- **Knowledge retrieval** via Knowledge Bases and Retrievers
-- **Behavior control** via Rulesets
-- **Task execution** via Tools
-- **Conversation memory** to maintain context
-
-In short:
-**Assistants turn AI Factory components into interactive, useful applications.**
-
-Assistants are the primary **deployment target** for most conversational and task-based AI use cases in Gen AI Builder.
-
-## Why use Assistants
-
-- To build **AI-driven user experiences** — chatbots, agents, copilots.
-- To combine **retrieved knowledge** with **controlled behavior**.
-- To enable **multi-step reasoning** and **task execution**.
-- To enforce **compliance**, **brand tone**, and **organizational policies**.
-- To provide **consistent memory and context** across conversations.
-
-Without Assistants, your AI Factory content (Knowledge Bases, Rulesets, Tools) would remain static — Assistants activate these components.
-
-## How Assistants work
-
-At runtime:
-
-1. The user submits input (text or other supported input).
-2. The Assistant:
-- Applies its assigned **Rulesets**.
-- Retrieves relevant content from **Knowledge Bases** (via its Retriever).
-- Applies **conversation memory**.
-- Executes **Tools** if needed.
-- Generates a response using its assigned **LLM**.
-
-3. The response is returned to the user.
-
-Assistant architecture:
-
-```User Input → Rulesets → Retriever → Knowledge Bases → Retrieved Content
-→ Tools → Action Execution
-→ Memory → Context Maintenance
-→ LLM → Response → User Output
-```
-
-
-
-## Core components of an Assistant
-
-### LLM
-
-- The Large Language Model that powers language understanding and generation.
-- Example choices:
-- GPT-4
-- Claude 3 Sonnet
-- Gemini Pro
-- Custom models via KServe
-
-### Knowledge Bases + Retriever
-
-- Retrieve relevant content using **Retrieval-Augmented Generation (RAG)**.
-- Retriever defines search behavior.
-- Knowledge Bases provide indexed, trusted content.
-
-### Rulesets
-
-- Define **how** the Assistant should behave.
-- Enforce tone, style, policy, and compliance.
-
-### Tools
-
-- Extend Assistant capabilities beyond language generation.
-- Example Tools:
-- API calls
-- Calculators
-- Internal system lookups
-- External data retrieval
-
-### Memory
-
-- Controls **conversation context**:
-- How many turns to remember
-- How to summarize or truncate history
-- Enables multi-turn coherent interactions.
-
-## When to use Assistants
-
-Use Assistants when you want to expose AI Factory capabilities through:
-
-- **Conversational interfaces** (chatbots, support bots, advisors)
-- **Internal productivity tools** (compliance checkers, knowledge agents)
-- **Copilot-style assistants** (embedded in applications)
-- **Custom API endpoints** driven by AI logic
-
-Assistants are the **primary vehicle for production AI applications** in Gen AI Builder.
-
-## Patterns of use
-
-### Single Assistant per use case
-
-- One Assistant → one primary interaction pattern.
-- Example: `PG Financial Client Support AI`.
-
-### Persona-based Assistants
-
-- Different Assistants for different **personas** or roles.
-- Example:
-- `Wealth Management Advisor`
-- `Marketing Copy Generator`
-- `Internal Policy Checker`
-
-### Shared Knowledge + Ruleset Assistants
-
-- Multiple Assistants sharing Knowledge Bases and Rulesets.
-- Useful for enforcing consistent behavior across applications.
-
-### Memory-driven Assistants
-
-- Assistants configured with advanced Memory settings.
-- Useful for:
-- Long-term user relationships.
-- Complex multi-turn workflows.
-
-## Best practices
-
-- Always combine **Knowledge Bases** + **Rulesets** → for grounded, controlled behavior.
-- Use Tools only where needed — avoid unnecessary Tool complexity.
-- Start simple → iterate on:
-- System prompt
-- Rulesets
-- Retriever tuning
-- Memory configuration
-- Test Assistants extensively before production.
-- Monitor Assistant performance and refine over time.
-
-## Related topics
-
-- [Create an Assistant](../../../how-to/gen-ai/create-assistant)
-- [Working with Assistants](../../../builder/agent-studio/assistants)
-- [Rulesets explained](rulesets-explained)
-- [Knowledge Bases explained](knowledge-bases)
-- [Retrievers explained](retrievers-explained)
-- [Structures explained](structures-explained)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/build)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/create-assistant.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/create-assistant.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/create-assistant.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/create-assistant.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,180 +0,0 @@
----
-title: Create an Assistant in Gen AI Builder
-description: How to create an Assistant in Gen AI Builder and configure it to deliver AI-powered conversational experiences.
----
-
-## Who is this for
-
-Platform users building AI-driven Assistants in Gen AI Builder.
-Typical users include developers, AI architects, conversational designers, and content owners.
-
-## What you will accomplish
-
-You will create an **Assistant** — an AI-powered agent that can:
-
-- Interact with users
-- Retrieve knowledge
-- Follow behavioral Rulesets
-- Optionally perform tasks via Tools
-
-You will configure the core components of an Assistant and prepare it for use.
-
-## Why create an Assistant
-
-- Assistants provide the **interactive layer** for your AI Factory content and capabilities.
-- They unify:
-- Large Language Models (LLMs)
-- Knowledge Bases
-- Rulesets
-- Tools
-- They enable end users to interact with your AI in a guided, controlled, and useful way.
-
-For background, see:
-
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate
-- **Estimated time**: 10–15 minutes
-
-## Pre-requisites
-
-- At least one Knowledge Base (optional but recommended).
-- One or more Rulesets (optional but recommended).
-- (Optional) Tools configured and available.
-
-## How to create an Assistant
-
-### 1. Navigate to Assistants
-
-- Go to **Assistants** in the Gen AI Builder UI.
-This may appear under **APPS** or a similar category.
-
-### 2. Create a new Assistant
-
-- Click **Create Assistant**.
-
-### 3. Configure Assistant fields
-
-#### Name (required)
-
-- A unique and descriptive name.
-Example: `PG Financial Client Support AI`
-
-#### Description (optional)
-
-- Purpose, audience, or key capabilities.
-Example: `Virtual support agent for PG Financial clients.`
-
-#### LLM Model Selection (required)
-
-- Select the primary Large Language Model (LLM) to power the Assistant.
-- Example options:
-- `GPT-4`
-- `Claude 3 Sonnet`
-- `Gemini Pro`
-- Custom models served via KServe
-
-#### System Prompt / Instructions (optional)
-
-- High-level instructions or persona definition.
-Example: `You are a helpful and friendly customer support agent for PG Financial.`
-
-#### Knowledge Bases (optional)
-
-- Select one or more existing Knowledge Bases.
-- Enables **Retrieval-Augmented Generation (RAG)**.
-- Example: `PG Financial Product FAQs KB`, `PG Financial Account Services KB`
-
-#### Rulesets (optional)
-
-- Select one or more existing Rulesets.
-- Defines behavioral guidelines for the Assistant.
-- Example: `Polite Support Tone`, `Financial Advice Disclaimer`
-
-#### Tools (optional)
-
-- Select Tools the Assistant can use (if Tools are configured).
-- Example Tools:
-- Calculator
-- Calendar lookup
-- Custom API connector
-
-#### Memory Configuration (optional, advanced)
-
-- Configure how the Assistant remembers conversational context.
-- Typical options:
-- Number of turns to remember
-- Summarization strategy
-
-#### Other Advanced Settings (optional)
-
-- Temperature (controls response creativity).
-- Top_p (controls nucleus sampling).
-- Tool usage settings.
-- RAG behavior settings.
-
-### 4. Finalize creation
-
-- Click **Create** to save the Assistant.
-- The new Assistant will appear in the list of Assistants.
-
-## Example: Create a Support Assistant
-
-1. Navigate to **Assistants**.
-2. Click **Create Assistant**.
-3. Configure fields:
-- Name: `PG Financial Client Support AI`
-- Description: `Virtual agent providing customer support for PG Financial.`
-- LLM Model: `GPT-4`
-- System Prompt: `You are a polite and helpful customer support agent for PG Financial.`
-- Knowledge Bases:
-- `PG Financial Product FAQs KB`
-- `PG Financial Account Services KB`
-- Rulesets:
-- `Polite Support Tone`
-- `Financial Advice Disclaimer`
-- Tools:
-- `Account Balance Lookup API` (example)
-4. Click **Create** to finalize the Assistant.
-
-5. Next steps:
-- Test the Assistant using the built-in UI.
-- Refine Rulesets, Tools, and Memory settings as needed.
-
-## Troubleshooting
-
-### Assistant not responding as expected
-
-- Verify that:
-- Knowledge Bases contain relevant content.
-- Appropriate Rulesets are assigned.
-- System Prompt aligns with desired behavior.
-
-### RAG behavior inconsistent
-
-- Check Retriever configuration and Knowledge Base indexing.
-- Verify that Tools are configured correctly if used.
-
-### Assistant memory not working as expected
-
-- Review Memory settings.
-- Adjust summarization parameters if applicable.
-
-### Tools not functioning
-
-- Verify Tool configuration and API availability.
-- Confirm that the Assistant has the Tool assigned.
-
-## Related topics
-
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Working with Assistants](../../../builder/agent-studio/assistants)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Structures explained](../../../learn/explained/structures)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/data-lake-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/data-lake-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/data-lake-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/data-lake-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,79 +0,0 @@
----
-title: Data Lake explained
-description: Understand the role of the Data Lake in Gen AI Builder and how it supports AI Factory content pipelines.
----
-
-## What is the Data Lake
-
-The **Data Lake** in Gen AI Builder is the foundational object storage layer that supports the entire AI Factory content pipeline.
-It provides persistent, scalable storage for:
-
-- Uploaded files from Data Sources
-- Indexed data and embeddings used in Libraries and Knowledge Bases
-- Griptape Structures and Tools
-- Temporary artifacts used in AI workflows
-
-In short:
-The Data Lake is where your AI system’s knowledge and operational data live.
-
-## Why is the Data Lake required
-
-Griptape-powered services rely on object storage to function correctly:
-
-- **Data Sources** of type Data Lake store and retrieve files here.
-- **Libraries** use the Data Lake to store processed and transformed content.
-- **Knowledge Bases** are built from content staged in the Data Lake.
-- **Structures and Tools** required by AI Agents and Pipelines are stored in the Data Lake.
-
-Without a Data Lake:
-
-- Data ingestion would fail.
-- Libraries and Knowledge Bases could not be built.
-- Retrieval-Augmented Generation (RAG) pipelines would be non-functional.
-- Structures and Tools would be unavailable to Agents and Assistants.
-
-The Data Lake is a **core dependency** of AI Factory.
-
-## How does the Data Lake fit into the content pipeline
-
-The AI Factory pipeline flows through the Data Lake:
-
-```Data Sources → Data Lake → Libraries → Knowledge Bases → Retrievers → Assistants → AI Applications ```
-
-- **Data Sources** → Files are ingested into the Data Lake.
-- **Libraries** → Processed content is staged and indexed via the Data Lake.
-- **Knowledge Bases** → Built from the indexed content stored in the Data Lake.
-- **Retrievers / Assistants** → Retrieve content ultimately powered by Data Lake-backed pipelines.
-
-## When do you configure the Data Lake
-
-- When deploying a new Griptape / Gen AI Builder instance.
-- When connecting an external object storage backend.
-- Before adding Data Sources or creating Knowledge Bases.
-- Whenever you change your storage provider or bucket.
-
-**Important:** The Data Lake must be configured **before using Libraries or Knowledge Bases**.
-
-## Patterns of use
-
-- **Dedicated bucket:** Best practice is to provision a dedicated object storage bucket solely for Griptape and Gen AI Builder.
-- **Isolated permissions:** Object storage credentials should be scoped to this bucket only.
-- **CORS configured:** Cross-Origin Resource Sharing (CORS) must be set to allow UI interaction with the Data Lake.
-- **S3-compatible:** The Data Lake can be AWS S3, GCS with S3 interoperability, or any other compatible object storage service.
-
-## Best practices
-
-- Use a **dedicated bucket** per Gen AI Builder deployment.
-- Follow the **principle of least privilege** for storage credentials.
-- Configure **CORS** correctly to enable portal-based interactions.
-- Regularly review bucket permissions and audit access.
-- Monitor storage utilization and cost.
-
-## Related topics
-
-- [Configure the Data Lake](../../../how-to/gen-ai/configure-datalake)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Structures explained](structures)
-- [Configure Data Sources](../../../builder/libraries/data-sources)
-- [Configure Knowledge Bases](../../../builder/libraries/knowledge-bases)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/generic-concepts.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/generic-concepts.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/generic-concepts.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/generic-concepts.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,167 +0,0 @@
----
-title: AI Factory generic concepts
-navTitle: Generic concepts
-description: Industry concepts that underpin modern AI workloads, data architectures, and AI Factory capabilities.
----
-
-AI Factory leverages modern AI concepts, architectures, and technologies to enable advanced data-driven and intelligent applications.
-This page explains **generic AI concepts** that form the foundation of AI Factory’s design and capabilities.
-
-For EDB-specific implementations, see [AI Factory concepts](../.ai-factory-concepts).
-For definitions of key terms, see [AI Factory terminology](./terminology).
-
-## Core AI concepts
-
-### Machine learning (ML)
-
-The ability of systems to learn from data and improve performance on specific tasks without explicit programming.
-
-Common approaches:
-
-- **Supervised learning** — Learn from labeled data (predict sales based on past sales data)
-- **Unsupervised learning** — Find patterns in unlabeled data (customer segmentation)
-- **Reinforcement learning** — Learn through interaction and rewards (training an AI to play a game)
-
-**Infrastructure needs:** large data storage, compute for training (CPUs, GPUs), and scalable environments for inference.
-
-### Deep learning (DL) & neural networks
-
-A subfield of ML using deep neural networks with multiple layers.
-
-Strengths:
-
-- Excels at complex data types (images, audio, video, text)
-- Powers applications such as image recognition, speech recognition, text generation
-
-**Infrastructure needs:**
-High-performance compute (GPUs or TPUs), optimized data pipelines, efficient serving infrastructure.
-
-### Natural language processing (NLP)
-
-AI techniques to understand and generate human language.
-
-Common tasks:
-
-- Text classification
-- Sentiment analysis
-- Machine translation
-- Question answering
-- Conversational AI interfaces
-
-**Infrastructure needs:**
-Powerful compute for large models, low-latency serving for real-time interactions.
-
-### Large language models (LLMs)
-
-Advanced NLP models based on deep learning architectures such as Transformers.
-
-Key characteristics:
-
-- Trained on massive text corpora
-- Capable of generating coherent, context-aware text
-- Support advanced applications such as chatbots, content generation, code generation
-
-**Infrastructure needs:**
-Extremely large compute and storage resources for training; optimized hardware (GPUs) for inference.
-
-### Embeddings and vector databases
-
-**Embeddings** are dense vector representations of data — capturing semantic meaning in a numerical space.
-
-Applications:
-
-- Semantic search
-- Recommendation systems
-- Anomaly detection
-- Retrieval-augmented generation (RAG) for LLMs
-
-**Infrastructure needs:**
-Specialized vector databases with efficient approximate nearest neighbor (ANN) search algorithms.
-
-See also: [Vector databases and semantic search](#vector-databases-and-semantic-search)
-
-## AI for databases
-
-### Intelligent database management
-
-Use of ML to automate and optimize database operations:
-
-- Automated performance tuning
-- Intelligent query optimization
-- Proactive resource management
-- Predictive maintenance and self-healing capabilities
-
-Goal: self-driving, highly optimized database infrastructure.
-
-### In-database machine learning
-
-Perform ML model training and inference **directly within the database**, reducing data movement and enabling:
-
-- Faster insights
-- Simplified architectures
-- Real-time prediction capabilities
-- Streamlined MLOps workflows
-
-Example: using SQL to invoke ML models inside Postgres.
-
-### Vector databases and semantic search
-
-Databases optimized to store and query embeddings (vector representations of data).
-
-Key features:
-
-- Fast similarity search (approximate nearest neighbor)
-- Support for high-dimensional data
-- Scalability to billions of vectors
-
-Common use cases:
-
-- Semantic search
-- Personalized recommendations
-- Enhanced LLM-based applications (RAG)
-- Anomaly detection
-
-## AI for infrastructure
-
-### AI-accelerated hardware (GPUs, TPUs)
-
-Specialized hardware optimized for AI workloads.
-
-- **GPUs** — General-purpose acceleration for training and inference
-- **TPUs** — Custom ASICs optimized for deep learning
-- **Other accelerators** — FPGAs, inference chips
-
-Benefits:
-
-- Orders-of-magnitude speedup for AI model training
-- Low-latency, high-throughput inference
-- Efficient resource utilization in AI clusters
-
-### Leveraging cloud compute for AI workloads
-
-Modern AI workloads rely heavily on cloud compute capabilities:
-
-- Access to GPU and TPU instances on demand
-- Managed Kubernetes services for scalable training and serving
-- Optimized software stacks (CUDA, TensorFlow, PyTorch, JAX)
-- Advanced networking for distributed model training
-- Flexible hybrid and multi-cloud strategies
-
-Key goals:
-
-- Minimize cost
-- Maximize flexibility
-- Ensure portability of data and models across environments
-
-## Related concepts
-
-- [Vectorized query engines](../../analytics/learn/explained/generic-concepts#vectorized-query-engines)
-- [Data lakehouse architectures](../../analytics/learn/explained/generic-concepts#data-lakehouse)
-- [Separation of storage and compute](../../analytics/learn/explained/generic-concepts#separation-of-storage-and-compute)
-
-## Next steps
-
-To understand how these concepts power **EDB’s AI Factory**, see:
-
-- [AI Factory concepts](../../ai-factory-concepts)
-- [AI Factory terminology](./terminology)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/knowledge-bases-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/knowledge-bases-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/knowledge-bases-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/knowledge-bases-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,92 +0,0 @@
----
-title: Working with Retrievers in Gen AI Builder
-description: Understand what Retrievers are, when and why to use them, and how they power AI Assistants in Gen AI Builder.
----
-
-## What is a Retriever
-
-A **Retriever** in Gen AI Builder is a specialized **Griptape Structure** that defines how your AI Assistants retrieve information from Knowledge Bases.
-Retrievers enable precise, customizable information retrieval — the core of **Retrieval-Augmented Generation (RAG)** workflows.
-
-At query time, your Assistant uses its configured Retriever(s) to:
-
-- Search one or more Knowledge Bases
-- Select the most relevant results ("chunks") based on semantic similarity and other filters
-- Feed that context into the Large Language Model (LLM) to generate an informed response
-
-**In short:**
-A Retriever connects user queries → Knowledge Base content → AI answers.
-
----
-
-For a deep dive, see [Retrievers explained](../../../learn/explained/retrievers-explained).
-For step-by-step configuration, see [Create a Retriever](../../../how-to/gen-ai/create-retriever).
-
----
-
-## Why use Retrievers
-
-- **Ground AI responses:** Retrievers ensure that Assistants provide factually grounded answers based on your organization’s Knowledge Bases.
-- **Support explainability:** Retrieved content can be referenced in the Assistant’s responses, enabling transparent and auditable AI.
-- **Control search behavior:** You can configure which Knowledge Bases are queried, control token limits, and supply advanced metadata.
-- **Enable multi-source RAG:** Retrievers can target multiple Knowledge Bases simultaneously — useful for complex Assistants with broad knowledge scope.
-
-## When to use Retrievers
-
-- Whenever you are building an AI Assistant that needs to answer questions based on internal content.
-- Whenever your content is stored in **Knowledge Bases** and you want dynamic, query-time retrieval.
-- When you want your AI application to support **semantic search** and not rely solely on the LLM’s internal training.
-
-**Examples:**
-
-- Support chatbots answering questions from product documentation
-- Financial advisors surfacing policies and legal disclaimers on demand
-- Sales tools providing quick access to internal pricing documents and competitive intelligence
-
-## How do Retrievers work
-
-At runtime, the process is:
-
-1. User asks a question.
-2. Assistant forwards the query to its assigned Retriever(s).
-3. Retriever runs semantic + optional structured search across configured Knowledge Bases.
-4. Retriever returns the most relevant results.
-5. Assistant passes this context to the LLM.
-6. LLM generates a response incorporating the retrieved information.
-
-**Under the hood:**
-A Retriever is implemented as a Griptape Structure, giving you flexibility in how it performs retrieval.
-
-See [Griptape concepts](../../../learn/explained/ai-factory-concepts) for more on Structures.
-
-## Key components of a Retriever
-
-- **Knowledge Bases targeted:** Determines the scope of retrieval.
-- **Max Tokens:** Controls how much content is retrieved per query.
-- **Metadata:** Optional structured data for advanced configuration or tagging.
-- **Associated Assistants:** Controls which Assistants can use this Retriever.
-
-## Getting started
-
-### Pre-requisites
-
-- One or more **Knowledge Bases** created and populated in Gen AI Builder.
-
-### First steps
-
-- Create a Retriever and configure it to target your Knowledge Bases.
-- Associate the Retriever with an AI Assistant.
-- Test the Assistant to verify that retrieved content is correctly influencing responses.
-
-See [Create a Retriever](../../../how-to/gen-ai/create-retriever) for the full step-by-step guide.
-
----
-
-## Related topics
-
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Create a Retriever](../../../how-to/gen-ai/create-retriever)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Structures explained](../../../learn/explained/structures)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-library-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-library-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-library-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-library-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,106 +0,0 @@
----
-title: AI Factory Model Library Explained
-navTitle: Model Library Explained
-description: Understand the Model Library in AI Factory, how it works, and how it is powered by the Hybrid Control Plane Image and Model Library.
----
-
-# AI Factory Model Library Explained
-
-The **Model Library** in AI Factory is your central interface for discovering, managing, and deploying AI models for Model Serving within the Hybrid Control Plane (HCP) environment.
-
-It is powered by the core **Image and Model Library** of HCP.
-The Model Library presents a curated AI-focused view of model images from that broader Image Library.
-
-## How does the Model Library work?
-
-The Model Library:
-
-- Surfaces a filtered view of **AI model images** available in the Image Library.
-- Allows AI Factory users to deploy supported models to the **Model Serving (KServe)** layer.
-- Ensures all model images pass through platform-wide governance and security policies.
-
-It does not provide a separate registry or upload flow:
-
-- All model images must flow through the Image Library.
-- The Model Library reflects images that are marked as valid for AI model serving.
-
-## Architecture flow
-
-Container Registry → Image and Model Library → Model Library → Model Serving (KServe) → AI Factory Workloads
-
-| Layer | Role |
-|-------|------|
-| Container Registry | Stores model container images (ex: NVIDIA NIM) |
-| Image and Model Library | Single source of truth for all images |
-| Model Library | Curated AI-focused UI for model images |
-| Model Serving (KServe) | Runs deployed model instances |
-| AI Factory Workloads | Gen AI Builder, Knowledge Base pipelines, Assistants |
-
-## What does the Model Library provide?
-
-- Browse available model images.
-
-- View supported tags and versions.
-
-- Deploy models to Model Serving infrastructure.
-
-- Manage lifecycle of model deployments.
-
-- Understand which models are powering your AI Factory experiences:
-
-- Knowledge Bases (via AIDB).
-- Gen AI Builder (for Assistants and pipelines).
-
-## Why is it powered by Image Library?
-
-- **Governance:** All images pass through the same governance flow.
-- **Security:** Registry integration, tag selection, and auditability is unified.
-- **Consistency:** Database and AI models share the same image management system.
-- **Flexibility:** You can add your own model images (via Image Library path).
-
-→ For a deeper understanding, see: [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
-
-## Supported model types
-
-In current AI Factory versions:
-
-| Model Type | Example Image |
-|------------|---------------|
-| Text Completion | llama-3.3-nemotron-super-49b |
-| Text Embedding | arctic-embed-l |
-| Image Embedding | nvclip |
-| OCR | paddleocr |
-| Text Reranker | llama-3.2-nv-rerankqa-1b-v2 |
-
-Future versions will expand supported model types and custom model support.
-
-## Model Serving integration
-
-Models deployed from the Model Library are served via **KServe**.
-
-The Model Library provides a guided workflow to deploy models:
-
-1. Select a model image and tag.
-2. Configure model runtime settings (replicas, resources, etc.).
-3. Deploy to KServe → creates an InferenceService in the HCP Kubernetes environment.
-4. The model is now available for:
- - AIDB Knowledge Base ingestion.
- - Gen AI Builder usage.
- - Other AI Factory capabilities.
-
-## Summary
-
-- The Model Library is your entry point for managing AI models in AI Factory.
-- It is powered by the core Image and Model Library of Hybrid Control Plane.
-- All model images pass through this unified governance flow.
-- Model Serving uses these images via KServe to power your AI workloads.
-
-## Related Explained Pages
-
-- [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
-
-## Related How-Tos
-
-- [Deploy AI Models](../../how-to/deploy-ai-models)
-- [Integrate Private Registry](../../how-to/integrate-private-registry)
-- [Define Repository Rules](../../how-to/define-repository-rules)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-concepts.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-concepts.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-concepts.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-concepts.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,120 +0,0 @@
-give me copy pastable block, in markdown like you have before
-
-
----
-title: KServe Concepts
-navTitle: KServe Concepts
-description: Learn how KServe enables Kubernetes-native, scalable model serving in AI Factory.
----
-
-# KServe Concepts: Kubernetes-Native Model Serving
-
-Use this guide to understand the core concepts of KServe, a standard for serverless inferencing on Kubernetes. Understanding these concepts helps you deploy, manage, and scale machine learning models within AI Factory.
-
-## What is KServe?
-
-KServe is an open-source, Kubernetes-native platform that provides a standardized, scalable, and simplified way to deploy and manage machine learning models in production. It builds on Kubernetes, Knative (optional, for serverless scaling), and Istio (optional, for advanced traffic management).
-
-## Purpose in model serving
-
-KServe simplifies the process of taking trained machine learning models and making them available as reliable, scalable inference services. It provides a consistent deployment experience across different model frameworks.
-
-AI Factory uses KServe as the model serving engine for Model Serving, enabling scalable, GPU-accelerated model inferencing on Kubernetes.
-
-## Core architectural principles
-
-### Standardization
-
-KServe defines a standard InferenceService Custom Resource Definition (CRD), providing a consistent interface for deploying models.
-
-### Scalability
-
-KServe leverages Kubernetes autoscaling and optionally Knative for serverless scaling. Models scale automatically based on demand.
-
-### Serverless inferencing
-
-KServe can scale models to zero when idle and back up when needed (with Knative integration). This optimizes resource usage.
-
-### Pluggable and extensible
-
-KServe supports a wide range of model servers and allows for adding pre-processing and post-processing steps through Transformers, and model explainability through Explainers.
-
-## Key components and Custom Resource Definitions (CRDs)
-
-### InferenceService
-
-The primary CRD for deploying a model. It includes:
-
-- **Predictor**: The core model serving component.
-- **Transformer** (optional): Pre-processing or post-processing service.
-- **Explainer** (optional): Generates model explanations.
-
-### Predictor
-
-Runs the model server and handles inference requests. It specifies:
-
-- Model server to use (e.g., NVIDIA Triton, MLServer).
-- Model location (e.g., S3, GCS, container image).
-- Resource allocations (CPU, memory, GPU).
-- Autoscaling parameters.
-
-### Transformer (optional)
-
-Handles input transformations before inference and output transformations after inference.
-
-### Explainer (optional)
-
-Provides explanations of model predictions using explainability tools.
-
-### ServingRuntime / ClusterServingRuntime
-
-Define templates for model server pods:
-
-- **ServingRuntime**: Namespace-scoped.
-- **ClusterServingRuntime**: Cluster-scoped, reusable across namespaces.
-
-They define the container image, environment variables, resource limits, and supported model formats.
-
-### Model server
-
-The software that loads the model and exposes the inference endpoint (e.g., NVIDIA Triton, TorchServe, TensorFlow Serving).
-
-## How KServe works
-
-1. You apply an InferenceService manifest to the Kubernetes cluster.
-2. The KServe controller creates the necessary Kubernetes resources:
-- Pods, Services, and autoscaling resources.
-3. The model server pod loads the model and serves inference requests.
-4. A client sends inference requests to the service URL.
-5. Requests optionally pass through a Transformer and/or Explainer before returning results.
-
-## Key features and capabilities
-
-- **Multi-framework support**: TensorFlow, PyTorch, scikit-learn, XGBoost, ONNX, TensorRT, and more.
-- **GPU acceleration**: Native support for NVIDIA GPUs and Kubernetes accelerators.
-- **Autoscaling**: Including scale-to-zero with Knative.
-- **Standard inference protocols**: V2 Inference Protocol (gRPC/HTTP).
-- **Canary rollouts**: Traffic shifting for safe model updates.
-- **Observability**:
-- Prometheus metrics.
-- Logging via Kubernetes.
-- Integration with tracing systems.
-- **Batching**: Improves throughput with server-side batching.
-- **Model explainability**: Supports various explainability tools.
-
-## Role in AI Factory
-
-In AI Factory:
-
-- KServe serves as the core model serving layer.
-- AI Factory simplifies KServe deployment and management.
-- The HCP Model Library integrates with KServe for model discovery and deployment.
-- GPU resource management is integrated with KServe deployments.
-- KServe services power Gen AI Builder agents and AIDB Knowledge Bases.
-- Centralized observability is provided through AI Factory observability tooling.
-
-## Further reading
-
-- [KServe Official Documentation](https://kserve.github.io/website/)
-- [KServe GitHub Repository](https://github.com/kserve/kserve)
-- [AI Factory Model Serving Quickstart](../model-serving/quickstart)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/model-serving-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,102 +0,0 @@
----
-title: Model Serving Explained
-navTitle: Model Serving Explained
-description: Understand how Model Serving works in AI Factory, using KServe to provide scalable, production-ready model inference.
----
-
-# Model Serving Explained
-
-Model Serving in AI Factory allows you to deploy AI models as scalable, production-grade inference services.
-
-It provides a standardized serving architecture based on Kubernetes and KServe, giving your models the ability to serve predictions and embeddings over network-accessible APIs.
-
-AI Factory Model Serving is optimized to support enterprise-class AI workloads with:
-
-- GPU-accelerated infrastructure
-- Flexible scaling
-- Monitoring and observability
-- Seamless integration with other AI Factory capabilities
-
-## Key Concepts
-
-### KServe in AI Factory
-
-Model Serving is powered by **KServe**, a popular open-source model serving engine for Kubernetes.
-KServe enables:
-
-- Serving models as standardized Kubernetes resources called `InferenceService`.
-- Auto-scaling and resource management.
-- REST and gRPC APIs for model consumption.
-- Support for a wide range of model formats.
-
-In AI Factory version 1.2, Model Serving focuses on streamlined support for **NVIDIA NIM** containers.
-Future releases will add expanded support for additional model types and formats.
-
-### Model Serving Stack
-
-| Layer | Purpose |
-|-------|---------|
-| AI Factory | Provides infrastructure and Model Serving APIs |
-| Hybrid Manager Kubernetes Cluster | Hosts model-serving workloads |
-| KServe | Manages model serving lifecycle and APIs |
-| NVIDIA NIM containers | Containerized models (LLM, embedding, reranker, OCR) |
-| User applications | Call model endpoints for inference |
-
-### Supported Models
-
-AI Factory Model Serving currently supports **NVIDIA NIM** containers for:
-
-- Text Completion
-- Text Embeddings
-- Text Reranking
-- Image Embeddings
-- Image OCR
-
-See the [Supported Models index](../../models/supported-models/index) for full details.
-
-### Infrastructure and Scaling
-
-- Models run on GPU-enabled Kubernetes nodes provided by Hybrid Manager (HCP).
-- AI Factory automatically manages GPU node groups and KServe runtimes.
-- You control model deployment and updates via:
-  - `InferenceService` resources
-  - `ClusterServingRuntime` definitions
-  - GPU resource tuning
-
-### Consumption Model
-
-- Applications interact with models via:
-  - REST APIs
-  - gRPC APIs
-- Example use cases:
-  - AI Assistants using embedding models
-  - RAG pipelines calling text rerankers
-  - Image processing pipelines using OCR
-
-## Deployment Architecture
-```Applications → Model Endpoints (REST/gRPC) → KServe → GPU-enabled Kubernetes → Model Containers
-```
-
-- Each model is isolated in its own KServe `InferenceService`.
-- KServe manages model lifecycle, scaling, and routing.
-- Monitoring is integrated with Prometheus-compatible scrapers.
-
-## Summary
-
-Model Serving in AI Factory provides a robust, scalable architecture for serving production AI models:
-
-✅ Kubernetes-native serving with KServe
-✅ GPU acceleration
-✅ Enterprise observability
-✅ Seamless integration with AI Factory pipelines and applications
-
-**Next steps:**
-
-- [Model Serving Quickstart](../model-serving/quickstart)
-- [How Model Serving Deployment Works](../../model/serving/deployment/index)
-- [Deploy NVIDIA NIM Containers](../../model/serving/deployment/deploying-nim)
-- [Supported Models](../../models/supported-models/index)
-
----
-
-For background on KServe and how it fits into AI Factory architecture, see [AI Factory Concepts](./ai-factory-concepts#model-serving-kserve).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/retrievers-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/retrievers-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/retrievers-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/retrievers-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,138 +0,0 @@
----
-title: Retrievers explained
-description: Understand the role of Retrievers in Gen AI Builder and how they power Retrieval-Augmented Generation (RAG) and semantic search.
----
-
-## What is a Retriever
-
-A **Retriever** in Gen AI Builder is a specialized **Griptape Structure** that defines how an AI Assistant retrieves relevant information from one or more Knowledge Bases at query time.
-
-Retrievers form the core of Retrieval-Augmented Generation (RAG) pipelines:
-
-- They take the user’s question or Assistant task as input.
-- They run semantic and optional structured search across configured Knowledge Bases.
-- They return the most relevant "chunks" or documents.
-- The Assistant passes this retrieved content to the Large Language Model (LLM) as part of its input context.
-
-**In short:**
-Retrievers connect your Assistant to your Knowledge Bases — grounding the AI in your organization’s content.
-
-## Why use Retrievers
-
-- **Ground AI responses:** Prevent hallucination by anchoring responses in trusted, current content.
-- **Enable explainability:** Retrieved content can be shown or cited in AI responses.
-- **Power multi-KB search:** Assistants can target multiple Knowledge Bases via a single Retriever.
-- **Customize retrieval behavior:** Max Tokens, metadata, and Griptape Structure logic enable fine-tuning.
-
-Without Retrievers, Assistants can only rely on their base model knowledge — which is static and cannot reflect your organization's current content.
-
-## How do Retrievers work
-
-Under the hood:
-
-1. User asks a question.
-2. Assistant sends this query to its assigned Retriever(s).
-3. Retriever (Griptape Structure) runs:
-- **Vector search** on unstructured content (using embeddings).
-- **Structured filtering** (if Hybrid KB is used and filters are specified).
-- **Ranking and selection** of relevant results.
-4. Retriever returns results to the Assistant:
-- Content is formatted as context for the LLM.
-- Optionally, references and metadata can be passed along.
-5. Assistant generates its response using the retrieved content.
-
-This entire flow happens **at query time** — ensuring responses reflect the most current content in your Knowledge Bases.
-
-## The role of Griptape Structures
-
-Retrievers are implemented as **Griptape Structures** — making them highly customizable.
-
-The core structure typically defines:
-
-- Query preprocessing logic (if needed)
-- Search logic (vector search + filters)
-- Result post-processing and formatting
-- Optional metadata injection
-
-This architecture allows organizations to:
-
-- Implement custom ranking strategies.
-- Apply domain-specific filters.
-- Enrich retrieved results with contextual tags.
-
-See [Structures explained](structures) for more on Griptape Structures.
-
-## Tuning and configuration points
-
-### Knowledge Bases selection
-
-- Select one or more Knowledge Bases to target.
-- A Retriever can search across **multiple KBs** — useful for broad Assistants.
-
-### Max Tokens
-
-- Controls the total size of content passed to the LLM.
-- Tuning guidance:
-- Too low → insufficient context for high-quality answers.
-- Too high → risk of LLM truncating the prompt or blowing past token limits.
-- Typical values: 1000–3000 depending on use case and LLM in use.
-
-### Metadata
-
-- Optional JSON object.
-- Use for:
-- Internal tracking and versioning
-- Configuring advanced search behavior (if supported by your Retriever Structure)
-- Passing hints to Assistants
-
-### Filters (Hybrid KB)
-
-- If targeting a Hybrid Knowledge Base, structured filters can be applied (e.g., region, product, price).
-- The Retriever Structure must support applying these filters.
-
-## Patterns of use
-
-### Single-KB Retriever
-
-- Simple pattern — target one Knowledge Base.
-- Example: `HR Policy Retriever` → queries `HR Policy KB`.
-
-### Multi-KB Retriever
-
-- Targets multiple Knowledge Bases.
-- Useful when an Assistant covers broad domains.
-- Example: `Support Assistant Retriever` → queries `Product Docs KB`, `Pricing Policies KB`, `Support FAQs KB`.
-
-### Hybrid KB Retriever
-
-- Targets Hybrid Knowledge Base.
-- Supports structured filtering + semantic search.
-- Example: `Product Catalog Retriever` → allows user to search `Product Catalog Hybrid KB` with filters for `Category`, `Price`, etc.
-
-### Cross-domain Retriever
-
-- Designed for Assistants that switch "personas" or tasks.
-- Can be implemented as:
-- Single Retriever with dynamic KB selection
-- Multiple Retrievers assigned per task or Assistant persona
-
-## Common tuning scenarios
-
-| Goal                                 | Tuning action                  |
-|--------------------------------------|-------------------------------|
-| Limit long irrelevant documents      | Tune Max Tokens downward      |
-| Retrieve rich multi-paragraph context| Tune Max Tokens upward        |
-| Filter by structured fields          | Use Hybrid KB with filters    |
-| Improve ranking quality              | Customize Griptape Structure |
-| Enable multi-source RAG              | Target multiple Knowledge Bases |
-
-## Related topics
-
-- [Retrievers in Gen AI Builder](../../ai-factory/gen-ai/builder/libraries/retrievers/)
-- [Create a Retriever](../../how-to/gen-ai/create-retriever)
-- [Knowledge Bases explained](knowledge-bases)
-- [Retrieval Augmented Generation (RAG)](rag)
-- [Embeddings explained](embeddings)
-- [Structures explained](structures)
-- [Griptape concepts](ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/rulesets-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/rulesets-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/rulesets-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/rulesets-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,100 +0,0 @@
----
-title: Rulesets explained
-description: Understand the role of Rulesets in Gen AI Builder and how they guide the behavior of Assistants and Structures.
----
-
-## What is a Ruleset
-
-A **Ruleset** in Gen AI Builder is a collection of natural language Rules that guide the behavior of Assistants and Structures.
-Rulesets define *how* an AI should behave when generating responses — complementing the *what* that comes from Knowledge Bases and Retrievers.
-
-Rulesets are written in natural language and provide clear instructions such as:
-
-- Maintain a polite and professional tone.
-- Do not mention competitors.
-- Always include legal disclaimers when discussing financial advice.
-
-In short:
-**Rulesets shape the "voice" and compliance of your AI — ensuring consistent, brand-aligned, and policy-compliant behavior.**
-
-## Why use Rulesets
-
-- **Control tone and style:** Ensure Assistants match your organizational voice.
-- **Enforce compliance:** Apply mandatory legal, ethical, or policy-driven behavior.
-- **Support multiple personas:** Define distinct behavioral patterns for different Assistants.
-- **Complement RAG:** Provide behavioral context that works alongside retrieved knowledge.
-
-Without Rulesets, Assistants rely solely on base model behavior — which may not align with your organizational requirements.
-
-## How Rulesets work
-
-At runtime, the process is:
-
-1. The user submits a query to the Assistant.
-2. The Assistant forwards the query to its Retriever (if configured) and gathers relevant content.
-3. The Assistant also applies its assigned **Ruleset**, which is passed to the underlying Griptape Structure that governs response generation.
-4. The LLM receives:
-- The user query
-- Retrieved content (context)
-- The **Ruleset** as behavioral instructions
-5. The LLM generates a response **that adheres to the Ruleset** and is informed by the retrieved content.
-
-Rulesets are implemented internally as **Griptape Structures**:
-
-- Each Rule is a natural language instruction.
-- The Ruleset defines which Rules apply to an Assistant.
-- The Griptape runtime injects Ruleset instructions into the system prompt or preamble for the LLM.
-
-See [Structures explained](structures) for more on Griptape Structures.
-
-## When to use Rulesets
-
-- **Always** when deploying Assistants in production.
-- Whenever tone, style, or compliance matter.
-- When building Assistants for different audiences:
-- Support Agents
-- Internal Knowledge Agents
-- Sales and Marketing Assistants
-- Executive Assistants
-- When using different **personas** or Assistant "voices."
-
-## Patterns of use
-
-### Single Ruleset → One Assistant
-
-- Simple pattern.
-- One Ruleset defines the tone and policy for one Assistant.
-- Example: `Polite Support Tone` Ruleset → assigned to `Customer Support Assistant`.
-
-### Shared Ruleset → Multiple Assistants
-
-- One Ruleset reused across multiple Assistants.
-- Useful for enforcing organization-wide policies (e.g., legal disclaimers).
-- Example: `Financial Advice Disclaimer` Ruleset → used by both `Wealth Management Bot` and `Internal Compliance Checker`.
-
-### Layered persona Rulesets
-
-- Use multiple Rulesets or composite Rulesets to create layered personas:
-- Global compliance Ruleset
-- Brand tone Ruleset
-- Assistant-specific Ruleset
-- Example:
-- `Brand Tone` + `Legal Compliance` + `Executive Assistant Voice`
-
-## Best practices
-
-- Write Rules in **clear, unambiguous natural language**.
-- Avoid conflicting or overlapping Rules.
-- Test Assistants with Rulesets applied — adjust as needed.
-- Use **Metadata** and **Alias** fields to support versioning and structured management.
-- Keep global / reusable Rulesets modular (do not duplicate content across many Rulesets).
-- Maintain an **audit trail** of Ruleset updates in regulated environments.
-
-## Related topics
-
-- [Create a Ruleset](../../../how-to/gen-ai/create-ruleset)
-- [Working with Rulesets](../../../builder/agent-studio/rulesets)
-- [Structures explained](structures)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/structures-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/structures-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/structures-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/structures-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,131 +0,0 @@
----
-title: Structures explained
-description: Understand the role of Structures in Gen AI Builder and how they enable advanced AI workflows, tooling, and data transformations.
----
-
-## What is a Structure
-
-A **Structure** in Gen AI Builder is a Griptape-powered agent, pipeline, or workflow that encapsulates advanced AI logic and business processes.
-Structures are the building blocks that enable you to extend the capabilities of your AI Factory beyond simple conversation.
-
-Structures can:
-
-- Execute **complex AI workflows**.
-- Transform data for Data Sources and Knowledge Bases.
-- Act as **Tools** for Assistants.
-- Implement **custom Data Sources** and **custom Retrievers**.
-- Integrate external APIs and systems.
-
-In short:
-**Structures bring *custom AI logic* and *integration* to your AI Factory applications.**
-
-Without Structures, your Assistants and RAG pipelines would be limited to basic retrieval + generation.
-
-## Why use Structures
-
-- To define **organization-specific business logic**.
-- To enable **multi-step reasoning** and **workflows**.
-- To support **task execution** via Tools.
-- To enrich and transform incoming data.
-- To implement **custom Retrieval** pipelines.
-- To integrate AI with **external systems** (APIs, databases, services).
-
-Structures make your AI applications **deeply extensible and production-capable**.
-
-## How Structures work
-
-A Structure is implemented as a Griptape agent, pipeline, or workflow:
-
-- **Griptape agent** → conversational AI logic.
-- **Griptape pipeline** → multi-step data transformation.
-- **Griptape workflow** → complex orchestration across agents and tools.
-
-Structures are:
-
-- Packaged as Zip files or deployed from GitHub.
-- Configured with:
-- Structure Config file (YAML / Python)
-- Environment variables (API keys, secrets, parameters)
-- Executed:
-- On demand via the Web Console.
-- Programmatically via API.
-- As Tools called by Assistants.
-- In Data Source pipelines.
-
-## Where Structures fit in the AI Factory pipeline
-```Data Sources → Data Lake → Libraries → Knowledge Bases → Retrievers → Assistants
-```
-
-Structures can:
-- Provide custom Data Sources
-- Transform Data Sources
-- Act as Tools in Assistants
-
-
-## When to use Structures
-
-Use Structures when you need:
-
-- Advanced AI capabilities beyond basic chat.
-- Data transformations (e.g., parsing documents, summarizing content, enriching data).
-- Business workflows (e.g., categorizing transactions, routing requests).
-- Task execution (e.g., API calls, system integrations).
-- Specialized retrieval and ranking logic.
-
-Typical use cases:
-
-- **Transaction categorization** pipeline.
-- **Data anonymization** for privacy.
-- **Product catalog enrichment** workflow.
-- **API calling agents** that fetch live data.
-- **Complex summarization** agents.
-- **Custom search** with ranking logic.
-
-## Patterns of use
-
-### Standalone Structure
-
-- Structure is executed on demand via UI or API.
-- Example: `Data Anonymizer`
-
-### Structure as a Tool
-
-- Structure is published as a Tool.
-- Assistant can call Structure at runtime.
-- Example: `Product Inventory Lookup` Tool.
-
-### Structure as Data Transformer
-
-- Structure used in **Data Source pipeline**.
-- Transforms incoming data before indexing.
-- Example: `PDF Parser + Metadata Enricher`.
-
-### Structure as Custom Data Source
-
-- Structure implements a **custom Retriever** or **custom Data Source**.
-- Example: `Real-Time Pricing Retriever`.
-
-## Best practices
-
-- Package Structures cleanly:
-- Clear Structure Config.
-- Minimal external dependencies.
-- Externalize configuration via **environment variables**.
-- Implement **robust error handling**.
-- Validate that Structures perform well in your production pipeline.
-- Use **versioned Data Lake paths** to support Structure versioning.
-- Monitor Structure execution performance.
-- Follow Griptape best practices:
-- Modular pipelines.
-- Stateless agents where possible.
-- Clear separation of transformation and orchestration logic.
-
-## Related topics
-
-- [Create a Structure](../../../how-to/gen-ai/create-structure)
-- [Working with Structures](../../../builder/agent-studio/structures)
-- [Create an Assistant](../../../how-to/gen-ai/create-assistant)
-- [Rulesets explained](rulesets-explained)
-- [Knowledge Bases explained](knowledge-bases)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/terminology.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/terminology.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/terminology.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/terminology.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,342 +0,0 @@
----
-title: AI Factory terminology
-navTitle: Terminology
-description: Definitions of key concepts, technologies, and architecture patterns used in the AI Factory and Hybrid Manager AI Factory.
----
-
-AI Factory terminology defines key concepts and technologies used across the **EDB AI Factory**, **Hybrid Manager AI Factory**, and related components.
-
-This page complements conceptual explanations in:
-
-- [AI Factory concepts](./ai-factory-concepts)
-- [Generic concepts](./generic-concepts)
-
-**Why it matters:**
-The AI Factory integrates cutting-edge AI capabilities with database services and cloud-native infrastructure. Understanding these terms will help you build effective AI-powered applications and data pipelines within the EDB ecosystem.
-
----
-
-## Core AI concepts
-
-### Machine learning (ML)
-
-**What:** Algorithms that learn from data and improve performance on tasks.
-**Why:** Enables predictive analytics, automation, and decision-making.
-
-Types:
-
-- Supervised learning (uses labeled data)
-- Unsupervised learning (finds patterns in unlabeled data)
-- Reinforcement learning (learns via trial and error)
-
-[Learn more](https://en.wikipedia.org/wiki/Machine_learning)
-
-### Deep learning (DL)
-
-**What:** ML using neural networks with many layers.
-**Why:** Powers state-of-the-art AI for image recognition, speech, text, and more.
-
-- Requires large datasets and high compute
-- Commonly used for LLMs, computer vision, audio processing
-
-[Learn more](https://en.wikipedia.org/wiki/Deep_learning)
-
-### Natural language processing (NLP)
-
-**What:** AI that enables computers to understand and generate human language.
-**Why:** Drives chatbots, semantic search, text summarization, and more.
-
-Applications:
-
-- Text classification
-- Sentiment analysis
-- Question answering
-- Conversational AI
-
-[Learn more](https://en.wikipedia.org/wiki/Natural_language_processing)
-
-### Large language models (LLMs)
-
-**What:** Deep learning models trained on large text corpora.
-**Why:** Power chatbots, text generation, coding assistants, translation.
-
-Examples:
-
-- OpenAI GPT
-- Anthropic Claude
-- Meta LLaMA
-
-[Learn more](https://huggingface.co/blog/llm)
-
-### Embeddings
-
-**What:** Dense vector representations of data (text, images, users) in multi-dimensional space.
-**Why:** Allow semantic comparisons for search, recommendations, and RAG.
-
-[Learn more](https://sebastianraschka.com/blog/2023/llm-embeddings.html)
-
-### Vector databases
-
-**What:** Databases optimized for storing and querying embeddings.
-**Why:** Power semantic search and LLM-based apps (RAG pipelines).
-
-Popular options:
-
-- pgvector (Postgres extension)
-- Pinecone
-- Weaviate
-- Milvus
-
-[Learn more](https://vectorsearch.dev/)
-
-### Retrieval-augmented generation (RAG)
-
-**What:** Technique where LLMs retrieve documents from vector stores to ground their responses.
-**Why:** Makes LLM output more accurate, current, and domain-specific.
-
-[Intro to RAG](https://huggingface.co/blog/rag)
-
----
-
-## AI for databases
-
-### Intelligent database management
-
-**What:** ML applied to optimize database operations.
-**Why:** Automates tuning and improves performance.
-
-Examples:
-
-- Automated performance tuning
-- Predictive scaling
-- Anomaly detection
-
-### In-database machine learning (In-DB ML)
-
-**What:** Running ML inside the database.
-**Why:** Eliminates ETL, enables real-time ML with SQL.
-
-Benefits:
-
-- Reduced data movement
-- Simplified architecture
-- Real-time insights
-
-### Vector search in Postgres
-
-**What:** Perform similarity search on embeddings inside Postgres.
-**Why:** Power AI use cases without leaving the database.
-
-- Uses [pgvector](https://github.com/pgvector/pgvector)
-- Integrated in AIDB
-
-### AIDB
-
-**What:** AI-in-Database for HCP-managed Postgres.
-**Why:** Brings vector search, embedding handling, and ML into Postgres.
-
-- Foundation for semantic search and RAG inside Postgres
-
-### Natural language interfaces to databases
-
-**What:** Query databases using natural language (instead of SQL).
-**Why:** Democratizes data access for non-technical users.
-
-- Powered by LLMs and NLP
-- Used for data exploration and reporting
-
----
-
-## AI infrastructure
-
-### AI-accelerated hardware
-
-**What:** Specialized processors for AI:
-
-- GPUs (primary for training + inference)
-- TPUs (Google)
-- FPGAs (custom acceleration)
-
-**Why:** Essential for large models (LLMs, computer vision, etc.)
-
-[More](https://developer.nvidia.com/cuda-gpus)
-
-### KServe
-
-**What:** Kubernetes-native model serving platform.
-**Why:** Standard for serving ML models in production.
-
-- InferenceService CRDs
-- Autoscaling
-- Serverless option
-- GPU-aware scheduling
-
-[KServe documentation](https://kserve.github.io/website/)
-
-### Gen AI Builder
-
-**What:** Application builder powered by Griptape, integrated with AI Factory.
-**Why:** Rapidly build complex AI apps with LLMs, tools, and structured workflows.
-
-Uses:
-
-- KServe-hosted models
-- AIDB Knowledge Bases
-- Datalake object storage
-
----
-
-## Griptape concepts
-
-### Structures
-
-**What:** Orchestrate AI workflows:
-
-- **Agent** — autonomous planning + tool use
-- **Pipeline** — linear sequence of tasks
-- **Workflow** — general orchestration with branching
-
-### Tasks
-
-**What:** Units of work:
-
-- **PromptTask** — interact with LLM
-- **ToolTask** — run a tool
-- **QueryTask** — query vector DB or RAG store
-- **CodeExecutionTask** — run code in sandbox
-
-[Griptape documentation](https://griptape.ai/)
-
-### Tools
-
-**What:** External capabilities accessible to Griptape structures.
-
-- Web search
-- API calls
-- DB queries
-- Enterprise systems
-
-### Drivers
-
-**What:** Connect to services:
-
-- LLM drivers — KServe-hosted models
-- Embedding drivers — KServe embedding models
-- Vector store drivers — pgvector, Pinecone, etc.
-- Storage drivers — persist memory to Datalake
-
-### Memory
-
-**What:** Manages state and context for AI apps.
-
-- Conversation memory (short-term)
-- Task memory
-- Long-term memory in object storage (Datalake)
-
-### Rulesets and rules
-
-**What:** Guide AI agent behavior.
-
-- Rules — constraints or instructions (tone, safety, behavior)
-- Rulesets — collections of rules applied to agents or structures
-
----
-
-## KServe concepts
-
-### InferenceService
-
-**What:** Primary KServe CRD for model deployment.
-
-Components:
-
-- **Predictor** — runs model server
-- **Transformer** — pre/post-processing
-- **Explainer** — optional model explainability
-
-### ServingRuntime / ClusterServingRuntime
-
-**What:** Blueprint for model server pods.
-
-- Container image
-- Resources
-- Model formats supported
-
-### Model server
-
-**What:** Software that loads the model and serves it via API.
-
-Options:
-
-- NVIDIA Triton
-- TorchServe
-- TensorFlow Serving
-- MLServer
-- Custom servers
-
----
-
-## Image and Model Library terms
-
-### AI model image
-
-**What:** Container image packaging an AI model + serving stack.
-
-- Deployed via KServe in AI Factory
-
-### Database image (PG image)
-
-**What:** Container image configured to run Postgres.
-
-### Container registry
-
-**What:** Stores and delivers container images.
-
-Examples:
-
-- Docker Hub
-- AWS ECR
-- GCP GAR
-- Private registries
-
-### EDB-provided registry
-
-**What:** Registry managed by EDB hosting official Postgres and AI model images.
-
-### Private registry
-
-**What:** Customer-managed registry integrated with AI Factory.
-
-### Image tag (version tag)
-
-**What:** Label identifying image version.
-
-### Repository rule
-
-**What:** Pattern used to select images/tags from private registry.
-
-### NVIDIA Inference Microservice (NIM)
-
-**What:** Optimized AI model images from NVIDIA for high-performance inference.
-
-### SHA digest
-
-**What:** Unique identifier (hash) for image content.
-
----
-
-## Summary
-
-**AI Factory terminology** spans:
-
-- AI concepts and patterns
-- AI-for-database patterns (AIDB, vector search, In-DB ML)
-- AI infrastructure (KServe, GPUs, model serving)
-- AI application development (Griptape, Gen AI Builder)
-- Container-based delivery (Images and Model Library)
-
-**Next:**
-To explore these terms in depth, see:
-
-- [AI Factory concepts](./ai-factory-concepts)
-- [Generic concepts](./generic-concepts)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/threads-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/threads-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/threads-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/threads-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,126 +0,0 @@
----
-title: Threads explained
-description: Understand the role of Threads in Gen AI Builder and how they provide conversation history, state persistence, and insights into Assistant behavior.
----
-
-## What is a Thread
-
-A **Thread** in Gen AI Builder represents a **single conversation history** — a sequence of interactions (messages) between a user and an AI Assistant.
-
-Threads allow you to:
-
-- Review entire conversations.
-- Debug and analyze Assistant behavior.
-- Persist and manage conversation context.
-- Support compliance and audit requirements.
-
-Each Thread stores:
-
-- The full message flow.
-- Tool invocations and results.
-- Memory state (if applicable).
-- System-level metadata.
-
-In short:
-**Threads are the *source of truth* for understanding what happened in an AI-powered conversation.**
-
-## Why use Threads
-
-- To **review conversation history** and verify behavior.
-- To **analyze user interaction patterns** and improve Assistants.
-- To **debug issues** reported by users.
-- To support **compliance auditing** with conversation logs.
-- To track and manage **conversation memory**.
-- To enable **stateful experiences** that span multiple sessions.
-
-Threads are essential for **production AI systems** where quality, traceability, and state persistence are required.
-
-## How Threads work
-
-At runtime:
-
-1. User initiates a conversation → new **Thread** is created.
-2. Each user input and Assistant response is added to the Thread.
-3. If **Tools** are invoked, their results are recorded in the Thread.
-4. If **memory** is enabled, context state is updated in the Thread.
-5. The Thread persists across turns → enabling coherent multi-turn conversations.
-
-Data flow:
-
-```User Input → Assistant → LLM → Response
-→ Message added to Thread → Memory updated → Tool results recorded
-→ Next user input → repeat
-```
-
-Every Thread captures:
-
-- Message sequence (user → Assistant → user ...).
-- Tool calls and responses.
-- Memory state across turns.
-- Timestamps and metadata.
-
-## When to use Threads
-
-Use Threads when you need to:
-
-- Perform **QA** on Assistant behavior.
-- Debug reported issues.
-- Analyze conversation flows to improve Assistants.
-- Verify that **Rulesets** and **Knowledge Bases** are applied correctly.
-- Support **compliance** with auditable conversation records.
-- Implement stateful Assistants with long-term memory.
-
-## Patterns of use
-
-### Conversation QA
-
-- Regularly review Threads for key Assistants.
-- Spot tone issues, factual errors, Rule violations.
-
-### Debugging
-
-- Investigate specific user reports:
-- Find the Thread.
-- Trace conversation step-by-step.
-
-### Compliance auditing
-
-- Export Threads for regulated Assistants.
-- Provide conversation logs for external audits.
-
-### Conversation analytics
-
-- Analyze common user questions.
-- Identify gaps in Knowledge Bases.
-- Inform content strategy.
-
-### Stateful Assistants
-
-- Implement Assistants with memory.
-- Use Threads to maintain and review conversation state.
-
-## Best practices
-
-- Regularly **review Threads** as part of QA processes.
-- Implement **naming conventions** or metadata to make Threads easily searchable.
-- Use **Thread search and filtering** to quickly locate relevant conversations.
-- Monitor **Tool usage** inside Threads — validate Tool call success and outputs.
-- For **memory-enabled Assistants**, use Threads to monitor:
-- Context flow.
-- Summary accuracy.
-- Memory window settings.
-- For **compliance-sensitive Assistants**, implement automated export or archiving of Threads.
-- Retain Threads according to **data retention policies**:
-- Privacy requirements.
-- Audit needs.
-- Storage cost considerations.
-
-## Related topics
-
-- [View and manage Threads](../../../how-to/gen-ai/view-threads)
-- [Working with Threads](../../../builder/agent-studio/threads)
-- [Assistants explained](assistants-explained)
-- [Structures explained](structures-explained)
-- [Rulesets explained](rulesets-explained)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/tools-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/tools-explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/tools-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/tools-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,146 +0,0 @@
----
-title: Tools explained
-description: Understand the role of Tools in Gen AI Builder and how they extend the capabilities of Assistants and Structures.
----
-
-## What is a Tool
-
-A **Tool** in Gen AI Builder is a Griptape-powered component that extends the capabilities of AI Assistants and Structures by enabling them to perform actions beyond simple text generation.
-
-Tools act as callable functions or plugins that can:
-
-- Query external systems.
-- Perform calculations.
-- Retrieve live data.
-- Interact with APIs.
-- Execute business logic.
-
-In short:
-**Tools make your AI *active*, not just reactive — enabling it to take meaningful actions.**
-
-Without Tools, Assistants and Structures are limited to generating responses based on static knowledge.
-
-## Why use Tools
-
-- To enable AI-driven applications that can **interact with external systems**.
-- To enrich conversations with **live data**.
-- To automate **business processes**.
-- To implement **organization-specific logic** as reusable components.
-- To allow LLMs to answer questions they otherwise could not — by giving them access to Tools.
-
-Tools transform Assistants from static Q&A bots into **true interactive agents**.
-
-## How Tools work
-
-Tools are implemented as **Griptape Tools** — Python classes that define one or more callable activities.
-
-At runtime:
-
-1. User submits input to an Assistant.
-2. The LLM determines that a Tool should be used.
-3. The Assistant calls the Tool, passing required parameters.
-4. The Tool executes:
-- Calls an API.
-- Performs a calculation.
-- Fetches data.
-- Runs any defined business logic.
-5. The Tool returns results to the Assistant.
-6. The Assistant incorporates the Tool result into its response.
-
-The flow:
-```User Input → Assistant → LLM decides → Tool Invocation
-→ Tool executes → Tool result → LLM generates final response → User Output
-```
-
-
-Tool configuration:
-
-- Packaged as a Zip file or deployed from GitHub.
-- Includes:
-- Griptape Tool class (Python).
-- Tool Config file.
-- Environment variables (e.g., API keys).
-
-Tools can also be used:
-
-- Within **Structures** → as components of pipelines.
-- In **Data Source pipelines** → for data transformation.
-
-## When to use Tools
-
-Use Tools when you need:
-
-- **Live data retrieval**:
-- Stock prices
-- Weather
-- Exchange rates
-- News headlines
-- **API integration**:
-- Internal databases
-- CRM systems
-- Custom applications
-- **Business logic execution**:
-- Complex calculations
-- Rule-based processing
-- **Process automation**:
-- Multi-step workflows triggered by user input
-
-If you want an Assistant to answer:
-
-*"What’s the current EUR to USD exchange rate?"*
-→ You need a Tool.
-
-If you want an Assistant to:
-
-*"Summarize all open Jira tickets for my team."*
-→ You need a Tool.
-
-## Patterns of use
-
-### API Connector Tool
-
-- Calls an external API and returns data.
-- Example: `FX Rate Tool`, `Weather Lookup Tool`.
-
-### Calculator Tool
-
-- Performs internal calculations and returns result.
-- Example: `Mortgage Calculator Tool`.
-
-### Data Fetcher Tool
-
-- Retrieves data from an internal system.
-- Example: `Customer Profile Lookup Tool`.
-
-### Data Transformer Tool
-
-- Preprocesses or transforms data.
-- Example: `Document Summarizer Tool`.
-
-### Process Automation Tool
-
-- Triggers multi-step processes or workflows.
-- Example: `Order Processing Tool`.
-
-## Best practices
-
-- Implement Tools as **modular Griptape Tools**:
-- Use @activity decorators for activities.
-- Define clear input/output schemas.
-- Write **robust error handling** — handle API failures gracefully.
-- Externalize secrets via **environment variables** — never hardcode credentials.
-- Keep Tools focused — one Tool → one purpose.
-- Use **Tool metadata and versioning** to manage lifecycle.
-- Test Tools independently before integrating with Assistants.
-- Monitor Tool usage and performance.
-- Document Tool behavior clearly for users and developers.
-
-## Related topics
-
-- [Create a Tool](../../../how-to/gen-ai/create-tool)
-- [Working with Tools](../../../builder/agent-studio/tools)
-- [Structures explained](structures-explained)
-- [Assistants explained](assistants-explained)
-- [Rulesets explained](rulesets-explained)
-- [AI Factory Concepts](ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/configure-datalake.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/configure-datalake.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/configure-datalake.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/configure-datalake.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,168 +0,0 @@
----
-title: Configure the Data Lake in Gen AI Builder
-description: How to configure the Data Lake in Gen AI Builder to support content storage and AI pipelines.
----
-
-## Who is this for
-
-Platform users setting up Gen AI Builder or Hybrid Manager with Griptape.
-This includes platform administrators, DevOps engineers, and AI builders configuring backend storage for AI Factory.
-
-## What you will accomplish
-
-You will configure a **Data Lake** — the object storage backend required for Griptape services to function.
-You will create and configure a storage bucket, set CORS policies, and provide required credentials.
-
-## Why configure the Data Lake
-
-- The Data Lake is used to store:
-- Uploaded files from Data Sources
-- Indexed data and embeddings
-- Griptape Structures and Tools
-- Temporary artifacts used by Griptape-powered services
-- Without a Data Lake, Libraries, Knowledge Bases, and AI Assistants will not function.
-- The Data Lake must be configured **before adding Data Sources or creating Knowledge Bases**.
-
-For background, see:
-
-- [Data Lake explained](../../../learn/explained/data-lake-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate (object storage + configuration)
-- **Estimated time**: 10–20 minutes
-
-## Pre-requisites
-
-- Access to an S3-compatible object storage provider:
-- AWS S3
-- Google Cloud Storage (GCS) with S3 interoperability enabled
-- Other compatible services (MinIO, Ceph, etc.)
-- Permissions to create buckets and manage CORS.
-- Ability to obtain object storage credentials.
-
-## How to configure the Data Lake
-
-### 1. Create a dedicated bucket
-
-Provision a new bucket **dedicated for Griptape / Gen AI Builder**.
-
-**Best practice:** Do not reuse a general-purpose bucket.
-
-#### AWS S3 or compatible
-
-- Create a bucket (example: `docs-temp-hcp`).
-
-#### Google Cloud Storage (GCS)
-
-- Create a bucket.
-- Enable **S3 interoperability** in GCS settings.
-- Assign required roles:
-- Storage Admin
-- Service Account Token Creator
-- Generate HMAC keys (used as Access Key ID and Secret Access Key).
-
-### 2. Configure CORS policy
-
-You must configure CORS (Cross-Origin Resource Sharing) to allow UI interactions with the Data Lake.
-
-#### Example CORS for S3-compatible storage
-
-```json
-[
-    {
-        "AllowedHeaders": ["*"],
-        "AllowedMethods": ["PUT", "POST", "DELETE", "GET", "HEAD"],
-        "AllowedOrigins": ["https://<PORTAL_DOMAIN_NAME>"],
-        "ExposeHeaders": []
-    }
-]
-```
-Example CORS for GCS
-1. Create cors-config.json:
-
-```[
-  {
-    "origin": ["https://<PORTAL_DOMAIN_NAME>"],
-    "method": ["GET", "PUT", "POST", "DELETE", "HEAD"],
-    "responseHeader": ["*"],
-    "maxAgeSeconds": 3600
-  }
-]
-```
-2. Apply policy:
-```
-gsutil cors set cors-config.json gs://<your-gcs-bucket-name>
-```
-3. Obtain credentials
-You will need the following for your Griptape deployment:
-
-Bucket name
-
-S3 Endpoint URL
-
-Access Key ID
-
-Secret Access Key
-
-Region (if applicable)
-
-For GCS: Use HMAC keys as your Access Key ID and Secret Access Key.
-
-4. Provide credentials to deployment
-Configure your Kubernetes deployment or backend:
-
-Environment variables
-
-Configuration files / secrets
-
-Typical environment variables:
-
-
-```DATA_LAKE_BUCKET_NAME=docs-temp-hcp
-DATA_LAKE_ENDPOINT_URL=https://<your-endpoint>
-DATA_LAKE_ACCESS_KEY_ID=<access-key-id>
-DATA_LAKE_SECRET_ACCESS_KEY=<secret-access-key>
-DATA_LAKE_REGION=<region>  # If applicable
-```
-
-5. Verify connectivity
-In Gen AI Builder, navigate to Data Lake.
-
-You should see your configured bucket listed.
-
-You can create folders or upload test files to verify permissions.
-
-Example: docs-temp-hcp → Create Folder → Upload File
-
-Troubleshooting
-Bucket not visible in Gen AI Builder
-Verify that credentials are correct and permissions are granted.
-
-Check that CORS policy allows required methods.
-
-Uploads or UI interactions fail
-Confirm CORS settings.
-
-Verify that bucket is dedicated to Griptape — avoid conflicting bucket policies.
-
-Access denied errors
-Ensure that the Access Key ID / Secret Access Key has the required permissions:
-
-S3 or Storage Object Admin
-
-Read / write / delete permissions on the bucket
-
-Related topics
-Data Lake explained
-
-AI Factory Concepts
-
-Configure Data Sources
-
-Configure Knowledge Bases
-
-Hybrid Manager: Using Gen AI Builder
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-knowledge-base.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-knowledge-base.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-knowledge-base.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-knowledge-base.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,159 +0,0 @@
----
-title: Create a Knowledge Base in Gen AI Builder
-description: How to create a Knowledge Base in Gen AI Builder to organize content from your Libraries for AI applications.
----
-
-## Who is this for
-
-Platform users who want to create Knowledge Bases to support AI applications that deliver context-aware responses.
-Typical users include developers, AI architects, and business owners managing Knowledge Bases in production environments.
-
-## What you will accomplish
-
-You will create a Knowledge Base in Gen AI Builder by selecting one or more Libraries as its content source.
-You can choose between fully managed vector stores, self-managed databases, or hybrid Knowledge Bases that combine structured and unstructured data.
-
-## Why use Knowledge Bases
-
-- Knowledge Bases provide a consistent, queryable foundation for AI applications.
-- They power semantic search and Retrieval-Augmented Generation (RAG) by organizing embedded content.
-- You can manage updates to Knowledge Bases by refreshing source Libraries and re-syncing Knowledge Bases.
-- Hybrid Knowledge Bases let you combine structured metadata with rich unstructured text, supporting advanced AI use cases.
-
-For background, see:
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Structures explained](../../../learn/explained/structures)
-
-To see how Knowledge Bases are used in **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low to moderate (depends on Knowledge Base type selected).
-- **Estimated time**: 5–15 minutes.
-
-## How to create a Knowledge Base
-
-1. Navigate to **Knowledge Bases** in the Gen AI Builder UI.
-2. Select **Create Knowledge Base**.
-
-### Common fields
-
-- **Name**: Provide a clear, unique name. (Required.)
-Example: `Company Policy KB`
-- **Description**: Optional text to describe the Knowledge Base.
-
-### Select the type of Knowledge Base
-
-Gen AI Builder supports three Knowledge Base types:
-
-### 1. PG.AI - Fully-Managed Vector Store
-
-Use this option to create a vector-based Knowledge Base with a fully managed backend.
-
-- **Data Sources**: Select one or more Libraries to populate the Knowledge Base.
-- Example Libraries: `HR Policy Documents Library`, `Employee Handbook Library`
-
-This is ideal for unstructured text content and general-purpose semantic search.
-
-#### Example
-
-- Name: `Company Policy KB`
-- Libraries selected: `HR Policy Documents Library`, `Employee Handbook Library`
-
-### 2. PG.AI Database - Self-Managed
-
-Use this option to connect to an existing PostgreSQL-compatible database with AI/vector extensions.
-
-- **Connection String**: Enter the full connection string for your database.
-- Example: `postgresql://user:password@host:port/dbname`
-- **Password**: Enter the password for the database user.
-- **Data Sources**: Select one or more Libraries to populate the Knowledge Base.
-
-This is suitable if you want to control your own database infrastructure, typically an HCP-managed Postgres instance with AIDB or pgvector.
-
-#### Example
-
-- Name: `Tech Docs Self-Managed KB`
-- Connection String: `postgresql://user:password@host:5432/techdocs`
-- Password: `**********`
-- Library selected: `API Documentation Library`
-
-### 3. PG.AI - Hybrid Database (for Structured/Unstructured Data)
-
-Use this option when your source data is structured (CSV, Google Sheet) and you want to combine structured filters with unstructured semantic search.
-
-- **Data Sources**: Select one or more Libraries (usually from structured sources).
-- **Structured Columns**:
-- Add one or more structured columns.
-- Provide the column name, optional description, and select data type (Text, Number, Date, Boolean).
-- **Unstructured Columns**:
-- Add one or more unstructured columns (text fields to embed for semantic search).
-- Provide the column name and optional description.
-
-This is ideal for product catalogs, customer profiles, or any hybrid data scenario.
-
-#### Example
-
-- Name: `Product Catalog Hybrid KB`
-- Library selected: `Product CSV Library`
-- Structured Columns:
-- `SKU` (Text)
-- `Price` (Number)
-- Unstructured Columns:
-- `ProductName`
-- `Features`
-
-## Finalizing creation
-
-After configuring the Knowledge Base:
-
-- Select **Create**.
-- The system will build the Knowledge Base and begin populating it from your selected Libraries.
-- You can monitor progress in the Knowledge Bases list view.
-
-## Troubleshooting
-
-### Missing Libraries
-
-- Ensure that the source Libraries you want to use have been created and are populated.
-
-### Connection errors (Self-Managed)
-
-- Verify that the database connection string and password are correct.
-- Check that your database is configured with required extensions (pgvector, AIDB) and is reachable from Gen AI Builder.
-
-### Incomplete indexing (Hybrid KB)
-
-- Check that the correct columns were selected for structured and unstructured indexing.
-- Verify column names match exactly with the source Library schema.
-
-## Example scenario
-
-You want to create a unified Knowledge Base for your product catalog.
-
-Example configuration:
-
-- Name: `Product Catalog Hybrid KB`
-- Library selected: `Product CSV Library`
-- Structured Columns:
-- `SKU` (Text)
-- `Price` (Number)
-- Unstructured Columns:
-- `ProductName`
-- `Features`
-
-## Related topics
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Structures explained](../../../learn/explained/structures)
-- [Manage Knowledge Bases](manage-knowledge-base)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-retriever.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-retriever.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-retriever.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-retriever.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,131 +0,0 @@
----
-title: Create a Retriever in Gen AI Builder
-description: How to create a Retriever in Gen AI Builder to control how your Assistants retrieve information from Knowledge Bases.
----
-
-## Who is this for
-
-Platform users who want to configure how their AI Assistants retrieve information from Knowledge Bases in Gen AI Builder.
-Typical users include developers, AI architects, and conversational designers.
-
-## What you will accomplish
-
-You will create a **Retriever** in Gen AI Builder and configure it to target one or more Knowledge Bases.
-You can then associate this Retriever with an Assistant to enable grounded, context-aware AI responses.
-
-## Why create a Retriever
-
-- Retrievers define **which Knowledge Bases** your Assistants search.
-- They enable **semantic search** and **Retrieval-Augmented Generation (RAG)**.
-- They allow you to configure **token limits**, **metadata**, and more.
-- Without a Retriever, an Assistant cannot access external content for grounding its responses.
-
-For background, see:
-
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-
-To see how Retrievers are used in **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low to moderate.
-- **Estimated time**: 5–10 minutes.
-
-## Pre-requisites
-
-- One or more populated **Knowledge Bases** created in Gen AI Builder.
-
-## How to create a Retriever
-
-1. Navigate to **Retrievers** in the Gen AI Builder UI.
-2. Select **Create Retriever**.
-
-3. Configure the following fields:
-
-### Name (required)
-
-- Label: **Name your retriever**
-- Action: Enter a clear, unique name.
-Example: `Product Docs Retriever`
-
-### Description (optional)
-
-- Label: **Describe your retriever**
-- Action: Add optional context about the Retriever’s purpose.
-Example: `Retriever for product documentation and customer FAQs`
-
-### Knowledge Bases (required)
-
-- Label: **Select the knowledge bases that will be queried by this retriever.**
-- Action: Select one or more existing Knowledge Bases.
-Example: `PG Financial Product FAQs KB`, `PG Financial Account Services KB`
-
-### Max Tokens (optional)
-
-- Label: **Max Tokens**
-- Action: Set a token limit for results returned by the Retriever.
-Example: `2000`
-
-**Note:**
-Max Tokens controls how much content is passed to the LLM as context.
-
-### Metadata (optional)
-
-- Label: **Metadata should be a valid JSON object.**
-- Action: Provide optional structured metadata in JSON format.
-Example: `{"version": "1.0", "team": "AI Engineering"}`
-
-## Finalizing creation
-
-- Select **Create**.
-- The Retriever will appear in the Retrievers list and is now available to assign to an Assistant.
-
-## Associating a Retriever with an Assistant
-
-- When configuring an Assistant, go to **Advanced Options**.
-- Select one or more Retrievers to associate with the Assistant.
-- This controls how the Assistant queries Knowledge Bases during operation.
-
-## Example scenario
-
-You want to create a Retriever for an Assistant that answers questions about product features and pricing.
-
-Example configuration:
-
-- Name: `Product Support Retriever`
-- Description: `Fetches product specs and pricing from KBs`
-- Knowledge Bases:
-- `Product Catalog Hybrid KB`
-- `Pricing Policies KB`
-- Max Tokens: `2000`
-- Metadata: `{"team": "Support AI"}`
-
-## Troubleshooting
-
-### Retriever not returning expected results
-
-- Verify that the target Knowledge Bases contain relevant content.
-- Check that your Max Tokens setting is appropriate (not too low).
-- Review Assistant configuration — ensure the correct Retriever is assigned.
-
-### Retriever not visible when configuring Assistant
-
-- Confirm that the Retriever was created successfully.
-- Ensure the target Knowledge Bases are accessible and indexed.
-
-### Max Tokens impact
-
-- Setting too high may exceed LLM context window and cause truncation.
-- Setting too low may result in incomplete context.
-
-## Related topics
-
-- [Retrievers explained](../../../learn/explained/retrievers-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Structures explained](../../../learn/explained/structures)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-ruleset.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-ruleset.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-ruleset.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-ruleset.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,148 +0,0 @@
----
-title: Create a Ruleset in Gen AI Builder
-description: How to create a Ruleset in Gen AI Builder and define Rules to guide Assistant behavior.
----
-
-## Who is this for
-
-Platform users building Assistants or Structures in Gen AI Builder who want to define and enforce consistent behavior and style.
-Typical users include developers, AI architects, content designers, and compliance owners.
-
-## What you will accomplish
-
-You will create a **Ruleset** — a collection of Rules written in natural language that guide your Assistant’s behavior.
-You will add one or more Rules and associate the Ruleset with an Assistant.
-
-## Why create a Ruleset
-
-- To define **behavioral guidelines** for Assistants and Structures.
-- To enforce tone, style, and policy compliance.
-- To support multiple personas with different behavioral patterns.
-- To complement Knowledge Bases and Retrievers in the AI pipeline.
-
-For background, see:
-
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low
-- **Estimated time**: 5–10 minutes
-
-## Pre-requisites
-
-- Gen AI Builder deployed with Assistant features enabled.
-
-## How to create a Ruleset
-
-### 1. Navigate to Rulesets
-
-- Go to **Rulesets** in the Gen AI Builder UI.
-This may appear under **TOOLS** or a similar category.
-
-### 2. Create a new Ruleset
-
-- Click **Create Ruleset**.
-
-### 3. Configure Ruleset fields
-
-#### Name (required)
-
-- Label: **Your Ruleset Name**
-- Action: Enter a clear, unique name.
-Example: `Customer Service Tone Rules`
-
-#### Description (optional)
-
-- Label: **Describe your Ruleset**
-- Action: Provide context about the Ruleset’s purpose or scope.
-Example: `Guidelines for polite, professional responses in customer interactions.`
-
-#### Rules (required)
-
-- Label: **Select the Rules that will be used to populate this Ruleset.**
-- Action: Select one or more existing Rules, or create new Rules directly in this dialog.
-
-##### Creating a new Rule (if needed)
-
-- In the Rules field, click **(Create new rule)**.
-
-In the **Create New Rule** modal:
-
-- **Name** (required):
-Example: `Maintain Polite Tone`
-
-- **Rule text** (required):
-Example: `Always address the user respectfully. Use polite greetings and sign-offs.`
-
-- Click **Create** to save the Rule.
-
-You can repeat this process to add multiple Rules to the Ruleset.
-
-#### Alias (optional)
-
-- Label: **Your Ruleset Alias**
-- Action: Provide an optional, user-friendly alias.
-Example: `polite-support`
-
-#### Metadata (optional)
-
-- Label: **Metadata should be a valid JSON object.**
-- Action: Provide optional metadata in JSON format.
-Example: `{ "version": "1.0", "author": "support_team" }`
-
-### 4. Finalize Ruleset creation
-
-- Click **Create** to save the Ruleset.
-- The new Ruleset will appear in the list of Rulesets.
-
-## Example: Create a "Polite Support Agent" Ruleset
-
-1. Navigate to **Rulesets**.
-2. Click **Create Ruleset**.
-3. Fill out Ruleset fields:
-- Name: `Polite Support Tone`
-- Description: `Ensures the assistant maintains a polite and professional tone during customer interactions.`
-- Alias: `polite-support`
-- Metadata: `{ "version": "1.0", "author": "support_team" }`
-4. Add Rules:
-- Click **(Create new rule)**.
-- Rule 1:
-- Name: `Use Formal Salutations`
-- Rule: `Always begin responses with a polite greeting, such as "Hello," or "Good day,". Address the user respectfully if their name is known.`
-- Click **(Create new rule)**.
-- Rule 2:
-- Name: `Offer Assistance Clearly`
-- Rule: `Clearly state how you can assist the user. For example, "How may I help you today?" or "I can assist you with..."`
-
-5. Click **Create** to finalize the Ruleset.
-
-6. Next steps:
-- Associate this Ruleset with an Assistant in its configuration.
-
-## Troubleshooting
-
-### Ruleset not visible when configuring an Assistant
-
-- Ensure that the Ruleset has at least one Rule.
-- Verify that the Ruleset was created successfully.
-
-### Conflicting or ambiguous Rules
-
-- Review Rule text carefully — avoid ambiguous phrasing.
-- Test Assistants with the Ruleset applied and adjust Rules as needed.
-
-### Ruleset updates not reflected
-
-- If you update a Rule or Ruleset, verify that the Assistant has the latest version assigned.
-- Some Assistants may require a re-sync or refresh to pick up updated Rules.
-
-## Related topics
-
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Working with Rulesets](../../../builder/agent-studio/rulesets)
-- [Structures explained](../../../learn/explained/structures)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-structure.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-structure.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-structure.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-structure.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,177 +0,0 @@
----
-title: Create a Structure in Gen AI Builder
-description: How to create a Structure in Gen AI Builder by deploying a Griptape-powered agent, pipeline, or workflow from a Zip file.
----
-
-## Who is this for
-
-Platform users building advanced AI workflows in Gen AI Builder using Griptape Structures.
-Typical users include developers, AI architects, and data engineers.
-
-## What you will accomplish
-
-You will create a **Structure** in Gen AI Builder by deploying a Griptape-powered agent, pipeline, or workflow.
-You will configure the Structure and prepare it for execution.
-
-## Why create a Structure
-
-- Structures enable **advanced AI workflows** beyond basic chat.
-- They allow you to:
-- Transform data in Data Sources.
-- Act as Tools callable by Assistants.
-- Implement custom Data Sources and Retrievers.
-- Package and reuse business logic in AI Factory.
-
-For background, see:
-
-- [Structures explained](../../../learn/explained/structures-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate
-- **Estimated time**: 10–20 minutes
-
-## Pre-requisites
-
-- Griptape Structure packaged as a Zip file:
-- Includes Structure code + dependencies.
-- Includes Structure configuration (typically YAML).
-- Structure Zip file uploaded to Data Lake.
-
-## How to create a Structure
-
-### 1. Navigate to Structures
-
-- Go to **Structures** in the Gen AI Builder UI.
-This may appear under **APPS**, **TOOLS**, or a dedicated category.
-
-### 2. Create a new Structure
-
-- Click **Create Structure**.
-
-### 3. Select creation method
-
-- Use **Create Structure from Zip file**.
-
-### 4. Configure Structure fields
-
-#### Name (required)
-
-- Label: **Name of your Structure**
-- Action: Enter a unique and descriptive name.
-Example: `PGFT_TransactionCategorizer`
-
-#### Description (optional)
-
-- Label: **Description of your Structure**
-- Action: Describe what the Structure does.
-Example: `Categorizes financial transactions using PGFT business logic.`
-
-### 5. Configure Data Lake parameters
-
-#### Bucket Id (required)
-
-- Label: **Select a PG.AI Bucket**
-- Action: Select the Data Lake bucket where your Structure Zip is stored.
-
-#### Asset Path (required)
-
-- Label: **your-folder/structure-code.zip**
-- Action: Enter the full path to your Zip file inside the bucket.
-Example: `griptape-structures/transaction_categorizer.zip`
-
-### 6. Configure Structure Config parameters
-
-#### Structure Config File (recommended)
-
-- Label: **Path to the structure configuration file in your zip file**
-- Action: Enter relative path inside the Zip to the Structure Config file.
-Example: `structure_config.yaml`
-
-### 7. Configure environment variables (optional)
-
-#### Add individual variables
-
-- Instruction: Set environment variables that will be passed to the Structure.
-
-For each variable:
-
-- **Type:** Variable (or Secret if supported).
-- **Name:** e.g. `API_KEY`
-- **Value:** e.g. `your_actual_api_key_here`
-
-#### Import .env file
-
-- Optionally paste `.env` file contents to bulk import variables.
-
-### 8. Finalize creation
-
-- Click **Create** to deploy the Structure.
-
-Deployment steps:
-
-- System unzips the file.
-- Sets up runtime environment.
-- Makes the Structure available for execution.
-
-### 9. Verify deployment
-
-- The new Structure will appear in the Structures list.
-- Monitor deployment **Status**.
-- You can:
-- Run Structure directly.
-- Use it as a Tool in an Assistant.
-- Use it in a Data Source pipeline.
-
-## Example: Deploy a Transaction Categorizer Structure
-
-1. Package Griptape Structure `TransactionCategorizer`.
-2. Upload to Data Lake:
-- Bucket: `pgai-structures`
-- Path: `transaction_categorizer/transaction_categorizer.zip`
-3. Create Structure:
-- Name: `PGFT_TransactionCategorizer`
-- Description: `Categorizes financial transactions.`
-- Bucket Id: `pgai-structures`
-- Asset Path: `transaction_categorizer/transaction_categorizer.zip`
-- Structure Config File: `structure_config.yaml`
-4. Add env variables:
-- Name: `MODEL_API_KEY`
-- Value: `your_key_here`
-5. Click **Create**.
-
-Next steps:
-
-- Run Structure via UI or API.
-- Assign Structure as Tool to an Assistant.
-
-## Troubleshooting
-
-### Structure deploy fails
-
-- Verify Zip file contents:
-- Structure code is correct.
-- Structure Config file is present and correctly referenced.
-- Check Data Lake path is correct and accessible.
-
-### Environment variables not working
-
-- Confirm variables are added correctly.
-- If using Secret type, verify secrets are configured properly.
-
-### Structure not appearing as Tool
-
-- Ensure the Structure defines an appropriate Tool interface.
-- Check that it is published and Tool-compatible.
-
-## Related topics
-
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Working with Structures](../../../builder/agent-studio/structures)
-- [Create an Assistant](../../../how-to/gen-ai/create-assistant)
-- [Rulesets explained](../../../learn/explained/rulesets-explained)
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-tool.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-tool.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-tool.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/create-tool.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,176 +0,0 @@
----
-title: Create a Tool in Gen AI Builder
-description: How to create a Tool in Gen AI Builder by deploying a Griptape-powered Tool from a Zip file.
----
-
-## Who is this for
-
-Platform users building advanced AI-powered applications in Gen AI Builder using Griptape Tools.
-Typical users include developers, AI architects, and system integrators.
-
-## What you will accomplish
-
-You will create a **Tool** in Gen AI Builder by deploying a Griptape-powered component that enables Assistants or Structures to perform specific actions.
-
-## Why create a Tool
-
-- Tools enable Assistants and Structures to:
-- Query external APIs.
-- Perform calculations.
-- Fetch live data.
-- Interact with databases and internal services.
-- Tools extend AI Factory capabilities beyond simple text generation.
-
-For background, see:
-
-- [Tools explained](../../../learn/explained/tools-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate
-- **Estimated time**: 10–20 minutes
-
-## Pre-requisites
-
-- Griptape Tool packaged as a Zip file:
-- Includes Tool code + dependencies.
-- Includes Tool configuration (typically YAML or Python).
-- Tool Zip file uploaded to Data Lake.
-
-## How to create a Tool
-
-### 1. Navigate to Tools
-
-- Go to **Tools** in the Gen AI Builder UI.
-This may appear under **TOOLS**, **APPS**, or a dedicated category.
-
-### 2. Create a new Tool
-
-- Click **Create Tool**.
-
-### 3. Select creation method
-
-- Use **Griptape Tool from Data Lake**.
-- Click **Select**.
-
-### 4. Configure Tool fields
-
-#### Name (required)
-
-- Label: **Name of your Tool**
-- Action: Enter a unique and descriptive name.
-Example: `PG_Financial_Stock_Ticker`
-
-#### Description (optional)
-
-- Label: **Description of your Tool**
-- Action: Describe what the Tool does.
-Example: `Fetches live stock ticker prices from external API.`
-
-### 5. Configure Data Lake parameters
-
-#### Bucket Id (required)
-
-- Label: **Select a PG.AI Bucket**
-- Action: Select the Data Lake bucket where your Tool Zip is stored.
-
-#### Asset Path (required)
-
-- Label: **your-folder/tool-code.zip**
-- Action: Enter the full path to your Zip file inside the bucket.
-Example: `tools/stock_ticker_tool.zip`
-
-### 6. Configure Tool Config parameters
-
-#### Tool Config File (recommended)
-
-- Label: **Path to the tool configuration file in your zip file**
-- Action: Enter relative path inside the Zip to the Tool Config file.
-Example: `tool_config.yaml`
-
-### 7. Configure environment variables (optional)
-
-#### Add individual variables
-
-- Instruction: Set environment variables that will be passed to the Tool.
-
-For each variable:
-
-- **Type:** Variable (or Secret if supported).
-- **Name:** e.g. `STOCK_API_KEY`
-- **Value:** e.g. `your_actual_api_key_here`
-
-#### Import .env file
-
-- Optionally paste `.env` file contents to bulk import variables.
-
-### 8. Finalize creation
-
-- Click **Create** to deploy the Tool.
-
-Deployment steps:
-
-- System unzips the file.
-- Sets up runtime environment.
-- Makes the Tool available for execution.
-
-### 9. Verify deployment
-
-- The new Tool will appear in the Tools list.
-- Monitor deployment **Status**.
-- You can:
-- Assign the Tool to Assistants.
-- Use the Tool in a Structure.
-- Test the Tool if a direct interface is provided.
-
-## Example: Deploy a PG Financial FX Rate Tool
-
-1. Package Griptape Tool `FxRateTool`.
-2. Upload to Data Lake:
-- Bucket: `pgai-tools`
-- Path: `fx_rate_tool/fx_rate_tool.zip`
-3. Create Tool:
-- Name: `PG_Financial_FX_Rate_Tool`
-- Description: `Fetches live FX rates from external API.`
-- Bucket Id: `pgai-tools`
-- Asset Path: `fx_rate_tool/fx_rate_tool.zip`
-- Tool Config File: `tool_config.yaml`
-4. Add env variables:
-- Name: `FX_API_KEY`
-- Value: `your_key_here`
-5. Click **Create**.
-
-Next steps:
-
-- Test Tool functionality.
-- Assign Tool to Assistants where needed.
-
-## Troubleshooting
-
-### Tool deploy fails
-
-- Verify Zip file contents:
-- Tool code is correct.
-- Tool Config file is present and correctly referenced.
-- Check Data Lake path is correct and accessible.
-
-### Environment variables not working
-
-- Confirm variables are added correctly.
-- If using Secret type, verify secrets are configured properly.
-
-### Tool not appearing for Assistant selection
-
-- Ensure Tool implements proper Griptape Tool class and is published.
-- Verify Tool status is active.
-
-## Related topics
-
-- [Tools explained](../../../learn/explained/tools-explained)
-- [Working with Tools](../../../builder/agent-studio/tools)
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-confluence.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-confluence.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-confluence.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-confluence.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,125 +0,0 @@
----
-title: Configure an Atlassian Confluence data source
-description: How to configure an Atlassian Confluence data source in Gen AI Builder to ingest content from Confluence spaces and pages.
----
-
-## Who is this for
-
-Platform users who want to ingest content from Atlassian Confluence into Gen AI Builder for use in Knowledge Bases and Retrieval-Augmented Generation (RAG) workflows.
-Typical users include developers, technical writers, and knowledge managers who maintain institutional knowledge in Confluence.
-
-## What you will accomplish
-
-You will configure an Atlassian Confluence data source in Gen AI Builder to ingest pages from one or more Confluence spaces or URLs. The ingested content will then be available for transformation and indexing into Libraries and Knowledge Bases.
-
-## Why use a Confluence data source
-
-- Confluence is a common platform for internal technical documentation, policies, engineering notes, and project knowledge.
-- Bringing Confluence content into Gen AI Builder makes this valuable institutional knowledge searchable and usable in AI workflows.
-- Targeted ingestion of selected spaces or pages provides fine-grained control.
-
-For background on how this content powers downstream AI use cases, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate. You need to obtain an Atlassian API Token and understand your Confluence space structure.
-- **Estimated time**: 10–15 minutes if API token is already generated.
-
-## Key considerations
-
-### Data relevance and quality
-
-- Ingest trusted spaces or specific pages with high-quality documentation.
-- Ensure content is well structured and uses semantic page layouts to improve AI processing.
-
-### Scope of ingestion
-
-- Use space-specific or page-specific URLs when possible to avoid ingesting broad and irrelevant content.
-- If you want to ingest multiple spaces, configure multiple data source entries.
-
-### Credentials security
-
-- Use an API Token with read-only access when possible.
-- Do not use administrator-level tokens unless necessary.
-
-## How to configure an Atlassian Confluence data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Confluence** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `Engineering Wiki Confluence`.
-- **Description** (optional): Add descriptive context for future reference.
-
-4. Connect to Confluence:
-
-- **Confluence Site URL**: Enter your Confluence site URL.
-- Example: `https://mycompany.atlassian.net` or `https://mycompany.atlassian.net/wiki/spaces/DEVKB`
-- **API Token**: Enter a valid Atlassian API Token.
-- **Atlassian Email**: Enter the email address associated with the API token.
-
-5. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply a PG.AI Structure to transform the data during ingestion.
-- Select an existing Structure from the list.
-
-6. Select **Create** to add the Confluence data source.
-
-## Supported content
-
-- Textual data from Confluence pages.
-- Content from supported macros (if rendered as text).
-- Content from page hierarchies.
-
-For more about how content is indexed and retrieved, see [Embeddings explained](../../../learn/explained/embeddings).
-
-## Managing and refreshing the data source
-
-Once created, the Confluence data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### Authentication failure
-
-- Verify that the Confluence Site URL, API Token, and Email are correct.
-- Confirm that the API Token has sufficient read permissions.
-- Check the data job history for specific errors.
-
-### Space or page not found
-
-- Double-check the Confluence URL and verify that your account has access to the target spaces or pages.
-- If using a page-specific URL, confirm that the page is not restricted.
-
-## Example scenario
-
-You want to index content from the "Developer Knowledge Base" space in your company's Confluence instance.
-
-Example configuration:
-
-- **Name**: `Developer KB Confluence Space`
-- **Confluence Site URL**: `https://mycompany.atlassian.net/wiki/spaces/DEVKB`
-- **API Token**: `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`
-- **Atlassian Email**: `developer@example.com`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Apply a Structure to strip headers and footers and extract core content.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-custom.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-custom.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-custom.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-custom.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,123 +0,0 @@
----
-title: Configure a Custom data source
-description: How to configure a Custom data source in Gen AI Builder using a PG.AI Structure to ingest data from proprietary systems.
----
-
-## Who is this for
-
-Platform users who need to ingest data from proprietary systems, internal databases, or APIs not supported by built-in data source types.
-Typical users include developers, system integrators, and advanced data engineers who build custom pipelines and Structures.
-
-## What you will accomplish
-
-You will configure a Custom data source in Gen AI Builder by selecting a pre-configured PG.AI Structure that acts as a data provider. The output of this Structure will be ingested and made available for indexing into Libraries and Knowledge Bases.
-
-## Why use a Custom data source
-
-- Internal data often resides in proprietary databases, APIs, or specialized data stores.
-- The Custom data source allows you to connect these systems by writing a PG.AI Structure that performs the data retrieval.
-- This offers maximum flexibility while ensuring the resulting content flows through Gen AI Builder’s ingestion and transformation pipeline.
-
-For background on how this content powers downstream AI use cases, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-For concepts related to Structures, see:
-
-- [Structures explained](../../../learn/explained/structures)
-
-## Complexity and time to complete
-
-- **Complexity**: High. You must write or have access to a PG.AI Structure that correctly implements data retrieval logic.
-- **Estimated time**: 15–60 minutes depending on Structure complexity and target system integration.
-
-## Key considerations
-
-### Structure design
-
-- The selected PG.AI Structure must be implemented and tested to act as a data source (data provider Structure).
-- The Structure should return data in a format suitable for downstream processing (typically textual).
-- The Structure should handle retries, error conditions, and paging if applicable.
-
-### Transform logic
-
-- You can optionally apply a separate "Transform your data" Structure to further refine, clean, or enrich the content before indexing.
-
-### Security and compliance
-
-- If the Structure connects to sensitive systems, ensure appropriate access controls, authentication, and logging are implemented.
-- Be mindful of privacy regulations and data handling requirements.
-
-## How to configure a Custom data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Custom Data Source** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `Legacy CRM Connector`.
-- **Description** (optional): Add descriptive context for future reference.
-
-4. Connect to Custom Data Source:
-
-- **PG.AI Structure**: Select the pre-configured PG.AI Structure that implements your data retrieval logic.
-- Example: `InternalProductAPIReader`
-
-5. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply an additional PG.AI Structure to transform the data during ingestion.
-- Select an existing Structure from the list.
-
-6. Select **Create** to add the Custom data source.
-
-## Supported content
-
-- Determined by the output of the selected PG.AI Structure.
-- Typically textual data suitable for AI processing.
-
-## Managing and refreshing the data source
-
-Once created, the Custom data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### Structure not listed
-
-- Verify that your PG.AI Structure is correctly registered and marked as a data source (data provider Structure).
-- Ensure the Structure is accessible in your current project or environment.
-
-### Fetching errors or incomplete data
-
-- Check the Structure’s implementation for errors, authentication issues, or unexpected API changes.
-- Review the data job history for detailed error messages.
-
-## Example scenario
-
-You want to ingest product catalog data from an internal REST API into Gen AI Builder.
-
-Example configuration:
-
-- **Name**: `Product Catalog via Custom API`
-- **PG.AI Structure**: `InternalProductAPIReader`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Apply a Structure that extracts and formats product descriptions.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Structures explained](../../../learn/explained/structures)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-data-lake.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-data-lake.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-data-lake.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-data-lake.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,125 +0,0 @@
----
-title: Configure a Data Lake data source
-description: How to configure a Data Lake data source in Gen AI Builder to ingest files from your platform-managed Data Lake.
----
-
-## Who is this for
-
-Platform users who want to ingest file-based content from the platform’s managed Data Lake into Gen AI Builder for use in Knowledge Bases and Retrieval-Augmented Generation (RAG) workflows.
-This is typically used by developers, data engineers, and business owners building AI applications that require internal structured or unstructured documents.
-
-## What you will accomplish
-
-You will configure a Data Lake data source in Gen AI Builder to ingest files from a selected Data Lake bucket and (optionally) specific asset paths. The content will then be available for indexing into Libraries and Knowledge Bases.
-
-## Why use a Data Lake data source
-
-- Many enterprise documents (PDFs, CSVs, Markdown, text files) already reside in internal Data Lake storage.
-- The Data Lake source provides a highly reliable, cost-effective way to bring this content into AI workflows.
-- Unlike public web content, Data Lake files are typically higher-trust, internal documents.
-- The Data Lake source allows for precise targeting (specific folders or files), making it efficient and scalable.
-
-For background on how this content powers downstream AI use cases, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low to moderate. You need familiarity with your Data Lake structure (bucket names and paths).
-- **Estimated time**: 5–10 minutes if paths are known and bucket access is already configured.
-
-## Key considerations
-
-### Data relevance and quality
-
-- Ingest only trusted, high-quality files that are useful for your AI applications.
-- Well-structured files (consistent formatting, well-written text) lead to better AI results.
-
-### Scope of ingestion
-
-- Be specific about which files or folders you ingest. Use targeted paths instead of entire buckets where possible.
-- Ingesting too broad a scope (e.g., entire raw data buckets) can lead to large processing costs and irrelevant content.
-
-### Permissions and access
-
-- You must have appropriate permissions to the selected Data Lake bucket and paths.
-- If no specific asset paths are provided, all files in the selected bucket will be ingested.
-
-## How to configure a Data Lake data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Data Lake** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `Q1 Product Specs - Data Lake`.
-- **Description** (optional): Add descriptive context for future reference.
-
-4. Connect to Data Lake:
-
-- **Bucket**: Select the Data Lake bucket from the dropdown list.
-- **Asset Paths**:
-- Enter one or more specific paths to files or folders. Example: `manuals/pdf/current-version/`
-- Select **Add Asset Path** to add more paths.
-- If no paths are provided, the system will ingest all files in the bucket.
-
-5. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply a PG.AI Structure to transform the data during ingestion.
-- Select an existing Structure from the list.
-
-6. Select **Create** to add the Data Lake data source.
-
-## Supported file types
-
-- PDF
-- CSV
-- Markdown
-- Most text-based file types
-
-For more about how content is indexed and retrieved from these files, see [Embeddings explained](../../../learn/explained/embeddings).
-
-## Managing and refreshing the data source
-
-Once created, the Data Lake data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### Files not found
-
-- Verify that the bucket and asset paths are correct.
-- Check the data job history for errors.
-
-### Access issues
-
-- Ensure your user or system has the necessary permissions to access the selected bucket and paths.
-
-## Example scenario
-
-You want to index a set of PDF product manuals stored in your Data Lake.
-
-Example configuration:
-
-- **Name**: `Product Manuals Archive`
-- **Bucket**: `technical-documentation-prd`
-- **Asset Path**: `manuals/pdf/current-version/`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Apply a Structure to extract specific sections from PDFs.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-google-drive.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-google-drive.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-google-drive.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-google-drive.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,125 +0,0 @@
----
-title: Configure a Google Drive data source
-description: How to configure a Google Drive data source in Gen AI Builder to ingest files from Google Drive folders into your Knowledge Bases.
----
-
-## Who is this for
-
-Platform users who want to ingest files stored in Google Drive into Gen AI Builder for use in Knowledge Bases and Retrieval-Augmented Generation (RAG) workflows.
-Typical users include developers, data engineers, and business stakeholders who collaborate on project documentation within Google Workspace.
-
-## What you will accomplish
-
-You will configure a Google Drive data source in Gen AI Builder to ingest files from selected Google Drive folders. The content will then be available for transformation and indexing into Libraries and Knowledge Bases.
-
-## Why use a Google Drive data source
-
-- Many project documents live in collaborative Google Drive folders.
-- Google Docs, Slides, and Sheets are frequently used for dynamic team content that is not available elsewhere.
-- Directly connecting to Google Drive allows you to capture this valuable knowledge without manual export steps.
-
-For background on how this content powers downstream AI use cases, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate. You must complete an OAuth flow and understand your Google Drive folder structure.
-- **Estimated time**: 5–10 minutes.
-
-## Key considerations
-
-### Data relevance and quality
-
-- Ingest curated folders that contain useful, trusted documents.
-- Well-written content in Google Docs and well-formatted Google Slides or Sheets contribute to more effective AI responses.
-
-### Scope of ingestion
-
-- Select folders carefully during the OAuth authorization flow.
-- Avoid ingesting broad folders (such as entire My Drive or Shared Drive roots) unless intentional.
-
-### Permissions scope
-
-- Review the permissions requested during the OAuth process.
-- If using Google Workspace, confirm that your organization allows third-party app access as required.
-
-## How to configure a Google Drive data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Google Drive** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `Project Alpha GDrive Docs`.
-- **Description** (optional): Add descriptive context for future reference.
-
-4. Connect to Google Drive:
-
-- Select **Connect With Google** to initiate the OAuth 2.0 authorization flow.
-- Sign in to your Google Account and grant the requested permissions.
-- Follow on-screen prompts to select the files or folders to ingest.
-
-5. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply a PG.AI Structure to transform the data during ingestion.
-- Select an existing Structure from the list.
-
-6. Select **Create** to add the Google Drive data source.
-
-## Supported file types
-
-- Google Workspace files: Docs, Sheets, Slides (converted to text).
-- PDF
-- CSV
-- Markdown
-- Most text-based file types.
-
-For more about how content is indexed and retrieved from these files, see [Embeddings explained](../../../learn/explained/embeddings).
-
-## Managing and refreshing the data source
-
-Once created, the Google Drive data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### Authentication failed
-
-- Check your Google Account and try the OAuth flow again.
-- Verify that your organization allows third-party app access.
-- Check browser settings and pop-up blockers if the OAuth flow does not complete.
-
-### Files or folders not visible
-
-- Verify that you have access to the target files or folders in Google Drive.
-- If using a Shared Drive, ensure that the selected files are visible and accessible to your account.
-
-## Example scenario
-
-You want to index project planning documents stored in a shared Google Drive folder.
-
-Example configuration:
-
-- **Name**: `Project Plans & Presentations GDrive`
-- **OAuth flow**: Completed successfully.
-- **Selected folder**: `Active Projects Q2`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Apply a Structure to summarize Google Docs content and extract key sections.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-s3.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-s3.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-s3.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-s3.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,131 +0,0 @@
----
-title: Configure an Amazon S3 data source
-description: How to configure an Amazon S3 data source in Gen AI Builder to ingest files from S3 buckets into your Knowledge Bases.
----
-
-## Who is this for
-
-Platform users who want to ingest files stored in Amazon S3 buckets into Gen AI Builder for use in Knowledge Bases and Retrieval-Augmented Generation (RAG) workflows.
-Typical users include developers, data engineers, and cloud architects managing enterprise data pipelines.
-
-## What you will accomplish
-
-You will configure an Amazon S3 data source in Gen AI Builder to ingest files from one or more S3 URIs. The ingested content will be available for transformation and indexing into Libraries and Knowledge Bases.
-
-## Why use an Amazon S3 data source
-
-- S3 is a common storage platform for enterprise content such as PDFs, CSVs, and documents.
-- Directly connecting to S3 allows you to leverage existing cloud storage without duplicating data.
-- Precise targeting of folders or files ensures high efficiency and control over the ingested content.
-
-For background on how this content powers downstream AI use cases, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate. You need to securely configure AWS credentials and understand your S3 URI structure.
-- **Estimated time**: 10–15 minutes if IAM permissions are already configured.
-
-## Key considerations
-
-### Data relevance and quality
-
-- Ingest trusted, curated files that contribute meaningfully to your AI application.
-- Well-structured, clearly written documents lead to better downstream AI results.
-
-### Scope of ingestion
-
-- Use targeted S3 URIs to ingest only the content you need.
-- Avoid specifying root-level prefixes that could ingest unnecessary or sensitive content.
-
-### Credentials security
-
-- Use IAM credentials with **least privilege**—grant read-only access only to the specific S3 resources Gen AI Builder needs.
-- Do not use over-permissioned root or admin keys.
-
-### Cost awareness
-
-- Be mindful of S3 data transfer and retrieval costs, especially for large ingestion jobs or frequent refresh schedules.
-
-## How to configure an Amazon S3 data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Amazon S3** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `Marketing Assets S3`.
-- **Description** (optional): Add descriptive context for future reference.
-
-4. Connect to S3:
-
-- **AWS Access Key**: Enter your AWS Access Key ID.
-- **AWS Secret Key**: Enter your AWS Secret Access Key.
-- **URIs**: Provide one or more S3 URIs for objects or prefixes.
-- Example: `s3://bucket-name/key_name`
-- Select **Add URI** to add more URIs.
-
-5. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply a PG.AI Structure to transform the data during ingestion.
-- Select an existing Structure from the list.
-
-6. Select **Create** to add the Amazon S3 data source.
-
-## Supported file types
-
-- PDF
-- CSV
-- Markdown
-- Most text-based file types
-
-For more about how content is indexed and retrieved from these files, see [Embeddings explained](../../../learn/explained/embeddings).
-
-## Managing and refreshing the data source
-
-Once created, the Amazon S3 data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### Access denied
-
-- Verify that your IAM user or role has the necessary `s3:GetObject` and (if applicable) `s3:ListBucket` permissions.
-- Review your bucket policies and S3 URIs for correctness.
-- Check the data job history for specific permission-related errors.
-
-### Bucket or key not found
-
-- Double-check S3 URIs for correctness.
-- Confirm that the specified keys or prefixes exist and are accessible to your IAM user or role.
-
-## Example scenario
-
-You want to index quarterly financial reports stored in an S3 folder.
-
-Example configuration:
-
-- **Name**: `Financial Reports S3 Archive`
-- **AWS Access Key**: `AKIAxxxxxxxxxxxxxxxx`
-- **AWS Secret Key**: `xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`
-- **URI**: `s3://my-company-financials/quarterly-reports/pdf/`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Apply a Structure to summarize PDFs and extract key financial figures.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-webpage.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-webpage.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-webpage.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/data-source-webpage.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,109 +0,0 @@
----
-title: Configure a Web Page data source
-description: How to configure a Web Page data source in Gen AI Builder to ingest content from publicly accessible websites.
----
-
-## Who is this for
-
-Platform users who want to bring web content into Gen AI Builder for use in Knowledge Bases and Retrieval-Augmented Generation (RAG) workflows. Typical users include developers, AI architects, and business owners working on contextual AI applications.
-
-## What you will accomplish
-
-You will configure a Web Page data source in Gen AI Builder to ingest selected web pages. The content will then be available for transformation and indexing into downstream Libraries and Knowledge Bases.
-
-## Why use a Web Page data source
-
-- Web pages often contain valuable, curated, and public-facing information that complements internal data sources.
-- Adding relevant web content can enhance AI answers, RAG performance, and semantic search by expanding your Knowledge Base.
-- Targeted ingestion of specific web pages ensures the system uses only high-quality and relevant external content.
-
-For guidance on how Knowledge Bases and RAG applications use this content, see:
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Hybrid Manager Gen AI Builder overview](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Key considerations
-
-### Data relevance and quality
-
-- Ingest web pages that contain useful, high-quality, and relevant content.
-- Pages with structured and clearly written HTML are the most effective.
-- Avoid dynamic pages that require JavaScript for key content.
-
-### Scope of ingestion
-
-- Target specific pages.
-- Avoid adding entire websites or root-level URLs unless necessary, as this can result in large volumes of irrelevant data and higher processing costs.
-
-### Compliance and terms of service
-
-- Always respect the website's robots.txt file and terms of service.
-- Ensure you are authorized to ingest the content for your intended use.
-
-## How to configure a Web Page data source
-
-1. In the interface where you manage data sources, select **+ Add New Data Source**.
-2. Choose **Web Page** from the available data source types.
-3. Configure the following fields:
-
-- **Name**: Provide a clear and unique name. Example: `EDB Main Site FAQs`.
-- **Description** (optional): Add descriptive context for future reference.
-- **URLs**: Enter one or more full URLs of the web pages to ingest.
-- Example: `https://www.enterprisedb.com/`
-- Select **Add URL** to provide additional URLs.
-
-4. (Optional) Configure advanced options:
-
-- **Scheduled refresh**: Enable this option to automatically refresh the data on a schedule.
-- Provide a cron expression. Example: `0 2 * * *` (daily at 2 AM).
-- **Transform your data**: Enable this option to apply a PG.AI Structure that transforms the data during ingestion.
-- Select an existing Structure from the list.
-
-5. Select **Create** to add the Web Page data source.
-
-## Supported content
-
-- Text content from HTML pages.
-- Limited support for client-side rendered content or heavily dynamic pages.
-
-## Managing and refreshing the data source
-
-Once created, the Web Page data source can be viewed and managed through the Data Sources interface.
-
-Actions available:
-
-- **Edit**: Modify the data source configuration.
-- **Refresh**: Manually trigger a data ingestion job.
-- **Delete**: Remove the data source.
-
-You can also review the data job history to monitor ingestion performance and troubleshoot issues.
-
-## Troubleshooting
-
-### URL not accessible
-
-- Check the URL for typos.
-- Verify the page is public and not protected by login or access controls.
-
-### No or incomplete content
-
-- The page may rely on JavaScript for key content, which is not fully supported.
-- Anti-scraping protections may block ingestion.
-- Review the data job history. If 0 bytes were ingested, dynamic rendering may be the cause.
-
-## Example scenario
-
-You want to enrich your Knowledge Base with relevant articles from the public EDB website.
-
-Example configuration:
-
-- **Name**: `EDB Blog - Latest GenAI Articles`
-- **URL**: `https://www.enterprisedb.com/blog/category/genai`
-- **Scheduled refresh**: `0 2 * * *` (daily at 2 AM)
-- **Transform your data**: Use a Structure to strip unwanted boilerplate such as author bios.
-
-## Related topics
-
-- [Knowledge Bases overview](../../../pipeline/knowledge_base/index)
-- [Building Knowledge Bases with Gen AI Builder](../../../learn/how-to/gen-ai/knowledge-base-setup) <!-- Placeholder link -->
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/hybrid-kb-best-practices.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/hybrid-kb-best-practices.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/hybrid-kb-best-practices.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/hybrid-kb-best-practices.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,175 +0,0 @@
----
-title: Best practices for Hybrid Knowledge Bases
-description: How to design Hybrid Knowledge Bases in Gen AI Builder using structured and unstructured columns effectively.
----
-
-## Who is this for
-
-Platform users creating Hybrid Knowledge Bases to power advanced AI applications combining structured and unstructured data.
-Typical users include developers, AI architects, data engineers, and content managers working with product catalogs, customer profiles, or mixed datasets.
-
-## What you will accomplish
-
-You will learn best practices for designing Hybrid Knowledge Bases in Gen AI Builder:
-
-- How to select structured and unstructured columns
-- How to design a schema that supports powerful AI queries
-- How to avoid common pitfalls in Hybrid Knowledge Base design
-
-## Why design Hybrid Knowledge Bases carefully
-
-- Hybrid Knowledge Bases allow you to combine **structured filters** (exact matches, ranges, facets) with **semantic search** (vector-based search).
-- A well-designed Hybrid Knowledge Base enables more accurate, flexible, and explainable AI responses.
-- Poor schema design can lead to inefficient queries, incomplete search results, or wasted processing.
-
-For background, see:
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-
-To see how Hybrid Knowledge Bases are used in **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Moderate — schema design requires understanding your data and use cases.
-- **Estimated time**: 15–30 minutes per Hybrid Knowledge Base.
-
-## Designing Hybrid Knowledge Bases
-
-### Understand your use case
-
-Start by understanding what types of queries you want your AI application to support.
-
-Examples:
-
-- "Show products under $500 in category 'Outdoor' and rank by relevance to search terms."
-- "Find policies for region 'EU' that mention data retention."
-
-### Selecting structured columns
-
-**Structured columns** support:
-
-- Exact match filters (e.g., `region = 'EU'`)
-- Range queries (e.g., `price BETWEEN 100 AND 500`)
-- Faceted search (e.g., list all `categories`)
-
-Best practices:
-
-- Use **short, unambiguous column names** that match your Library schema exactly.
-- Choose data types carefully:
-- `Text` for identifiers and categories (SKU, region, category)
-- `Number` for numeric ranges (price, rating)
-- `Date` for date-based filtering (release date, expiration date)
-- `Boolean` for simple yes/no filters (isActive, inStock)
-- Do not include freeform text fields as structured columns — they should go in unstructured columns.
-
-### Selecting unstructured columns
-
-**Unstructured columns** support:
-
-- Full-text semantic search
-- Retrieval-Augmented Generation (RAG) queries
-
-Best practices:
-
-- Select columns containing rich text that will benefit from embedding:
-- Descriptions
-- Reviews
-- Policies
-- FAQ answers
-- Detailed notes
-- Avoid columns with redundant or trivial text — this can degrade search quality.
-- Be intentional about **how many** unstructured columns you define:
-- Too many dilute search results.
-- Too few limit context richness.
-
-### Example schema design
-
-#### Product catalog Hybrid KB
-
-**Structured columns:**
-
-- `SKU` (Text)
-- `Category` (Text)
-- `Price` (Number)
-- `AvailableSince` (Date)
-- `InStock` (Boolean)
-
-**Unstructured columns:**
-
-- `ProductName`
-- `Features`
-- `DetailedDescription`
-
-### Common pitfalls
-
-#### Mismatched column names
-
-- The column names you define **must match exactly** the column names in your source Library (CSV headers, Google Sheet columns).
-- Misspelled names or casing differences will cause ingestion errors.
-
-#### Poor type selection
-
-- Do not mark numeric fields as Text or vice versa.
-- Use Date types correctly to support proper date queries.
-
-#### Overloading structured columns
-
-- Do not add long freeform text columns as structured — this will degrade filtering performance.
-
-#### Underusing unstructured columns
-
-- If your unstructured text is spread across multiple columns (e.g., FAQ question, FAQ answer), consider combining them with a Structure before ingestion, or clearly select both as unstructured.
-
-## Example scenario
-
-You are designing a Hybrid Knowledge Base for your **Product Catalog**.
-
-Example configuration:
-
-- **Structured Columns**:
-- `SKU` (Text)
-- `Category` (Text)
-- `Price` (Number)
-- `AvailableSince` (Date)
-- **Unstructured Columns**:
-- `ProductName`
-- `DetailedDescription`
-
-Resulting queries supported:
-
-- Filter by `Category` and `Price` range.
-- Sort by `AvailableSince`.
-- Perform semantic search across product names and descriptions.
-
-## Troubleshooting
-
-### Incomplete search results
-
-- Verify that unstructured columns were selected and indexed correctly.
-- Check that structured filters in queries are not too restrictive.
-
-### Filter not working
-
-- Verify that the column was marked as structured and with the correct data type.
-- Check that the Library schema and column names match exactly.
-
-### Ingestion errors
-
-- Check that all column names match your source Library exactly.
-- Verify that the data types of columns align with your intended schema.
-
-## Related topics
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Structures explained](../../../learn/explained/structures)
-- [Create a Knowledge Base](create-knowledge-base)
-- [Manage Knowledge Bases](manage-knowledge-base)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,54 +0,0 @@
----
-title: Gen AI How-To Guides
-navTitle: Gen AI How-To
-description: Practical guides for configuring and managing Gen AI features in AI Factory — including Assistants, Structures, Tools, Knowledge Bases, and more.
----
-
-Use these guides to build and manage AI-powered applications in AI Factory’s Gen AI capabilities.
-
-These how-to guides cover day-to-day tasks for configuring your AI Agents (Assistants and Structures), integrating external data sources, managing Rulesets, and more.
-
----
-
-## Where to start
-
-- [Configure your Data Lake](./configure-datalake)
-- [Create a Knowledge Base](./create-knowledge-base)
-- [Create a Retriever](./create-retriever)
-- [Create a Ruleset](./create-ruleset)
-- [Create a Structure](./create-structure)
-- [Create a Tool](./create-tool)
-
----
-
-## Working with Data Sources
-
-- [Connect a Confluence data source](./data-source-confluence)
-- [Connect a custom data source](./data-source-custom)
-- [Connect the Data Lake](./data-source-data-lake)
-- [Connect a Google Drive data source](./data-source-google-drive)
-- [Connect an S3 data source](./data-source-s3)
-- [Connect a Web Page data source](./data-source-webpage)
-
----
-
-## Best practices and management
-
-- [Hybrid Knowledge Base best practices](./hybrid-kb-best-practices)
-- [Manage a Knowledge Base](./manage-knowledge-base)
-- [View and manage Threads](./view-threads)
-
----
-
-## Related concepts
-
-- [Gen AI Concepts](../../explained/ai-factory-concepts)
-- [Structures Explained](../../explained/structures-explained)
-- [Tools Explained](../../explained/tools-explained)
-- [Knowledge Bases Explained](../../explained/knowledge-bases-explained)
-- [Retrievers Explained](../../explained/retrievers-explained)
-- [Rulesets Explained](../../explained/rulesets-explained)
-- [Threads Explained](../../explained/threads-explained)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/manage-knowledge-base.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/manage-knowledge-base.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/manage-knowledge-base.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/manage-knowledge-base.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,136 +0,0 @@
----
-title: Manage Knowledge Bases in Gen AI Builder
-description: How to view, edit, refresh, and delete Knowledge Bases in Gen AI Builder.
----
-
-## Who is this for
-
-Platform users managing Knowledge Bases for AI applications in Gen AI Builder.
-Typical users include developers, AI architects, data engineers, and content managers.
-
-## What you will accomplish
-
-You will learn how to manage existing Knowledge Bases in Gen AI Builder.
-You will perform common management tasks:
-
-- Viewing existing Knowledge Bases
-- Editing Knowledge Base settings
-- Refreshing or re-syncing Knowledge Bases
-- Deleting Knowledge Bases
-- Viewing Knowledge Base details
-
-## Why manage Knowledge Bases
-
-- Keeping your Knowledge Bases up to date ensures AI applications deliver current and accurate responses.
-- Managing your Knowledge Bases helps control storage and cost by cleaning up unused or obsolete Knowledge Bases.
-- Monitoring Knowledge Base status and content improves reliability and performance in production AI workflows.
-
-For background, see:
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-
-To see how Knowledge Bases are used in **Hybrid Manager deployments**, visit:
-
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low.
-- **Estimated time**: 2–5 minutes per Knowledge Base.
-
-## Viewing existing Knowledge Bases
-
-1. Navigate to **Knowledge Bases** in the Gen AI Builder UI.
-2. The Knowledge Bases list view displays:
-
-- **Name**: User-defined name (example: `docs-temp-hcp`).
-- **Created**: Creation date and time.
-- **Updated**: Last updated date and time.
-- **Status**: Operational status (example: `Succeeded`).
-- **Type**: Backend type (example: `GenAI Builder Vector` or `vector`).
-
-3. Use pagination controls to navigate large lists (example: `Rows per page: 5 1-5 of 8 < >`).
-
-## Viewing Knowledge Base details
-
-1. Click on a Knowledge Base name or **View Details** icon to open its detail page.
-2. The detail view typically shows:
-
-- **Name** and **Description**
-- **Created** and **Updated** timestamps
-- **Status** and backend **Type**
-- **Linked Libraries** or **Data Sources**
-- **Indexing or sync status**
-- Recent activity or job history
-
-## Editing a Knowledge Base
-
-> **Note:** Edit capabilities may be limited after Knowledge Base creation, depending on type.
-
-1. In the Knowledge Bases list view, select the **Edit** action (typically from an actions menu or edit icon).
-2. You may be able to edit:
-
-- **Description**
-- In some cases, the set of Libraries or Data Sources (depends on type)
-- Configuration for Hybrid KB structured/unstructured columns (if supported)
-
-3. Save changes.
-
-## Refreshing or re-syncing a Knowledge Base
-
-1. In the Knowledge Bases list view, select the **Refresh** or **Re-sync** action.
-2. The system will:
-
-- Pull the latest data from linked Libraries.
-- Re-index the Knowledge Base if necessary.
-
-> Use this action when source Libraries have been updated and you want to propagate changes to the Knowledge Base.
-
-## Deleting a Knowledge Base
-
-> **Warning:** Deleting a Knowledge Base is irreversible. This will remove the Knowledge Base and its content from AI applications.
-
-1. In the Knowledge Bases list view, select the **Delete** action (typically from an actions menu or delete icon).
-2. Confirm the deletion when prompted.
-
-> This will not delete the source Libraries — only the Knowledge Base.
-
-## Troubleshooting
-
-### Knowledge Base status shows "Error" or "Failed"
-
-- Click **View Details** and check the activity log or job history.
-- If the Knowledge Base is linked to an external database (Self-Managed PG.AI Database), verify that the database connection is healthy.
-
-### Refresh operation does not update expected content
-
-- Verify that the linked Libraries have been refreshed and contain the latest data.
-- Re-run the Refresh / Re-sync action.
-- Check that Libraries are correctly configured with expected Data Sources and Transformations.
-
-### Cannot edit certain fields
-
-- Some Knowledge Base types (especially Fully-Managed Vector Stores and Self-Managed DBs) restrict editing of certain configuration fields after creation.
-- To change these fields, you may need to create a new Knowledge Base.
-
-## Example scenario
-
-You want to refresh and check the status of a Knowledge Base used in production:
-
-1. Navigate to **Knowledge Bases**.
-2. Locate `Company Policy KB`.
-3. Click **View Details** → confirm recent Libraries were updated.
-4. Click **Refresh** → re-sync the Knowledge Base.
-5. Monitor status until **Succeeded**.
-
-## Related topics
-
-- [Knowledge Bases explained](../../../learn/explained/knowledge-bases)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Retrieval Augmented Generation (RAG)](../../../learn/explained/rag)
-- [Embeddings explained](../../../learn/explained/embeddings)
-- [Structures explained](../../../learn/explained/structures)
-- [Create a Knowledge Base](create-knowledge-base)
-- [Hybrid Manager: Using Gen AI Builder](../../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/view-threads.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/view-threads.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/view-threads.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/gen-ai/view-threads.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,180 +0,0 @@
----
-title: View and manage Threads in Gen AI Builder
-description: How to view and manage Threads in Gen AI Builder to analyze conversation history and Assistant behavior.
----
-
-## Who is this for
-
-Platform users who need to review and analyze **conversation history** for QA, compliance, debugging, or Assistant improvement.
-Typical users include developers, QA teams, AI architects, and compliance officers.
-
-## What you will accomplish
-
-You will:
-
-- Navigate to the **Threads** section.
-- View existing Threads.
-- Open individual Threads.
-- Inspect message flow and Assistant behavior.
-- Understand how to analyze and manage Threads.
-
-## Why view Threads
-
-- Threads provide the **complete conversation history** between users and Assistants.
-- They allow you to:
-- Verify Assistant behavior and tone.
-- Audit conversations for compliance.
-- Debug unexpected issues.
-- Analyze user interaction patterns.
-- Review Tool usage and memory state.
-
-For background, see:
-
-- [Threads explained](../../../learn/explained/threads-explained)
-- [AI Factory Concepts](../../../learn/explained/ai-factory-concepts)
-- [Hybrid Manager: Using Gen AI Builder](../../../../hybrid-manager/ai-factory/gen-ai/builder/index)
-
-## Complexity and time to complete
-
-- **Complexity**: Low
-- **Estimated time**: 5–10 minutes
-
-## Pre-requisites
-
-- At least one existing Thread generated by an Assistant conversation.
-
-## How to view Threads
-
-### 1. Navigate to Threads
-
-- Go to **Threads** in the Gen AI Builder UI.
-This may appear under **TOOLS**, **APPS**, or a dedicated category.
-
-### 2. View Threads list
-
-You will see a list of existing Threads.
-Each Thread displays:
-
-- **Name:** User-defined or auto-generated.
-- **Alias:** Unique Thread ID.
-- **Created:** When Thread started.
-- **Updated:** Last message timestamp.
-
-Other UI elements:
-
-- **Search bar:** Find Threads by name or alias.
-- **Create Thread:** (Optional) Start new Thread if supported.
-- **Pagination controls:** Navigate large lists.
-
-### 3. Search for relevant Thread
-
-- Use **Search** to locate a specific Thread.
-- Common searches:
-- User ID
-- Assistant name
-- Keywords from initial message
-- Thread alias
-
-### 4. Open a Thread
-
-- Click on a Thread **Name** or **Alias** to open it.
-
-### 5. Inspect Thread details
-
-#### Thread header
-
-- **Thread Name / Alias**
-- **ID:** Copyable unique identifier.
-- **Created / Updated** timestamps.
-
-#### Actions menu
-
-- Options may include:
-- Edit Thread properties.
-- Delete Thread.
-- Export Thread.
-
-#### Message list (left pane)
-
-- Sequence of messages:
-- Message ID
-- Created / Updated timestamps
-- Message index (order in Thread)
-
-#### Message details (right pane)
-
-- Select a message to view:
-- Full content (user message or Assistant response)
-- Message metadata
-- Tool invocations (if any)
-- Memory state impact (if applicable)
-
-### 6. Analyze conversation flow
-
-- Step through messages to verify:
-- **Intent understanding** → Did the Assistant correctly understand the user?
-- **Knowledge accuracy** → Was information correct and well-sourced?
-- **Tone and compliance** → Did the Assistant follow Rulesets?
-- **Tool usage** → Were Tools invoked correctly and results incorporated?
-- **Memory handling** → If using memory, was conversation state maintained appropriately?
-
-### 7. Take action
-
-- Use **Actions** menu as needed:
-- Edit or rename Thread.
-- Delete unwanted or test Threads.
-- Export Thread for QA or audit.
-
-## Example: Review a PG Financial Support Thread
-
-Scenario:
-
-- User asked about opening a new savings account with **PG Financial Support Assistant**.
-
-Steps:
-
-1. Navigate to **Threads**.
-2. Search: `savings account` or `PG Financial Support Assistant`.
-3. Open Thread: `"Inquiry about new savings account"`.
-4. Review messages:
-- Confirm initial intent: User asked about savings account.
-- Check Assistant responses:
-- Provided correct product details.
-- Maintained polite tone per Rulesets.
-- Retrieved accurate content from Knowledge Base.
-5. Verify memory handling:
-- User asked follow-up questions → Assistant responded appropriately.
-6. Document any findings:
-- Gaps in Knowledge Base.
-- Rule violations.
-- Improvement opportunities.
-
-## Troubleshooting
-
-### Thread not found
-
-- Check that conversation was completed (may take a moment to appear in list).
-- Verify search terms.
-- Confirm you have appropriate project / Assistant scope.
-
-### Messages missing
-
-- Verify that Thread was not truncated or deleted.
-- Confirm Tool outputs were logged if expected.
-
-### Assistant behavior incorrect
-
-- Use Thread analysis to:
-- Identify Ruleset gaps.
-- Refine Retriever configuration.
-- Improve Knowledge Base coverage.
-- Debug Tool behavior.
-
-## Related topics
-
-- [Threads explained](../../../learn/explained/threads-explained)
-- [Working with Threads](../../../builder/agent-studio/threads)
-- [Assistants explained](../../../learn/explained/assistants-explained)
-- [Structures explained](../../../learn/explained/structures-explained)
-- [Rulesets explained](../../../learn/explained/rulesets-expla)
-next
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/define-repository-rules.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/define-repository-rules.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/define-repository-rules.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/define-repository-rules.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,167 +0,0 @@
----
-title: Define Repository Rules for Model Library
-navTitle: Define Repository Rules
-description: How to define and manage repository rules to control which model images appear in the Model Library.
----
-
-# Define Repository Rules for Model Library
-
-This guide explains how to define **Repository Rules** to control which model image repositories and tags from your connected registries appear in the AI Factory **Model Library**.
-
-Repository Rules help you manage:
-
-- **Scope** of image discovery
-- **Governance** over which images are exposed
-- **Version control** by controlling which tags are visible
-
-## Who should use this guide?
-
-- AI platform admins integrating private registries
-- DevOps engineers managing container registry integrations
-- Security/compliance teams governing AI model image usage
-
-## What this enables
-
-- You can precisely control which image repositories HCP syncs.
-- You can filter which tags within each repository are exposed.
-- You can govern which images are available in the Model Library for deployment.
-
-## Estimated time to complete
-
-5–10 minutes per Repository Rule.
-
-## Prerequisites
-
-- You must have already [Integrated your Private Registry](integrate-private-registry).
-- Your registry must appear in **AI Factory > Model Library > Manage Repositories**.
-
-## Understanding Repository Rules
-
-Repository Rules are attached to each configured registry.
-
-They allow you to define:
-
-- **Which repositories** to discover
-- **Which tags** within those repositories to expose
-
-This prevents accidental exposure of test images, unvetted model builds, or entire registry contents.
-
-Repository Rules are **evaluated periodically** — newly pushed images matching the rule will appear in Model Library on the next sync.
-
-## How Repository Rules work
-
-Each rule typically includes:
-
-| Field | Description |
-|-------|-------------|
-| Repository Name | Name of the repository to include (exact match or pattern) |
-| Tag Filter | (Optional) Filter for tag names (exact match, prefix, regex) |
-| Rule Enabled | Whether this rule is currently active |
-
-Example:
-
-| Repository Name | Tag Filter |
-|-----------------|------------|
-| `my-company/llama3-models` | `^release-.*` |
-
-→ Only tags starting with `release-` in `my-company/llama3-models` will appear.
-
-## Defining Repository Rules
-
-### 1. Navigate to Repository Rules
-
-- Go to:
-
-`AI Factory > Model Library > Manage Repositories`
-
-- Click **Manage Rules** for your desired registry.
-
-### 2. Add a Repository Rule
-
-- Click **Add Rule**.
-- Fill in:
-
-  - **Repository Name** — required
-  - **Tag Filter** — optional
-  - Enable/Disable toggle
-
-- Click **Save**.
-
-The rule will appear in the rules list.
-
-### 3. Wait for next sync or trigger manual sync
-
-HCP will apply rules on the next registry sync cycle.
-
-If your UI supports manual sync:
-
-- Click **Sync Now** after adding/updating a rule.
-
-### 4. Verify Model Library content
-
-- Go to **AI Factory > Model Library**.
-- Use filters to browse images from your registry.
-- Confirm only desired repositories/tags appear.
-
-## Example Use Cases
-
-### Basic: Restrict to a single repo
-
-| Repository Name | Tag Filter |
-|-----------------|------------|
-| `my-org/nim-models` | (blank — all tags) |
-
-→ All tags in `my-org/nim-models` will appear.
-
-### Advanced: Filter to production tags
-
-| Repository Name | Tag Filter |
-|-----------------|------------|
-| `models/gpt4` | `^prod-.*` |
-
-→ Only tags starting with `prod-` will appear.
-
-### Multi-repo governance
-
-- Add multiple Repository Rules per registry as needed.
-- Use Tag Filters to align with your model promotion pipelines (dev, staging, prod).
-
-## Tips & Best Practices
-
-- **Be explicit** — add Repository Rules for only the repositories you want to expose.
-- Use **Tag Filters** to enforce version governance and CI/CD promotion stages.
-- Regularly **audit Repository Rules** to ensure compliance with internal image standards.
-- If your registry uses structured tagging (e.g., `prod-`, `test-`), leverage Tag Filters to separate experimental from production models.
-
-## Troubleshooting
-
-### Images not appearing
-
-- Check that Repository Rule exists for the desired repo.
-- If using Tag Filter — verify tags match the filter exactly.
-- Check when the last sync ran; trigger manual sync if needed.
-
-### Too many images appearing
-
-- Refine Repository Rules.
-- Add Tag Filters to narrow scope.
-
-### Repository Rule has no effect
-
-- Confirm repository name is exact (case sensitive).
-- Ensure Rule is enabled.
-- Trigger manual sync and check HCP logs if issue persists.
-
-## Summary
-
-- Repository Rules govern which image repositories and tags appear in Model Library.
-- You can scope exposure by repository name and tag filters.
-- This supports governance, version control, and security best practices.
-- Fine-grained control helps ensure only vetted model images are deployed.
-
-## Related Links
-
-- [Model Library Explained](../../explained/model-library-explained)
-- [Integrate Private Registry with Model Library](integrate-private-registry)
-- [Deploy AI Models from Model Library](deploy-ai-models)
-- [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/how-to-deploy-ai-models.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/how-to-deploy-ai-models.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/how-to-deploy-ai-models.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/how-to-deploy-ai-models.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,148 +0,0 @@
----
-title: How to Deploy AI Models from Model Library
-navTitle: Deploy AI Models
-description: Step-by-step guide to deploying AI models from the Model Library to Model Serving in Hybrid Control Plane.
----
-
-# How to Deploy AI Models from Model Library
-
-This guide explains how to deploy AI models from the AI Factory **Model Library** into Model Serving (powered by KServe) in your Hybrid Control Plane (HCP) environment.
-
-Once deployed, these models power key AI Factory features:
-
-- **Knowledge Bases** (via AIDB pipelines)
-- **Gen AI Builder Assistants and pipelines**
-- Other AI Factory and application integrations
-
-## Who should use this guide?
-
-- AI platform admins deploying validated model images
-- Data engineers configuring AI models for Knowledge Bases
-- AI application developers configuring models for Assistants
-
-## What this enables
-
-Once deployed:
-
-- Your AI models are available in Model Serving.
-- You can link them to Knowledge Bases or Gen AI Builder pipelines.
-- You can monitor and manage deployed models via the HCP Model Serving UI or Kubernetes.
-
-## Estimated time to complete
-
-10–20 minutes per model, depending on model size and cluster resources.
-
-## Prerequisites
-
-Before you begin:
-
-- An active HCP environment with **GPU worker nodes** configured.
-- NVIDIA NGC API key stored as `nvidia-nim-secrets` in Kubernetes.
-- The model image you want to deploy must be visible in the **Model Library**.
-- KServe must be configured and ready in your cluster.
-
-→ For a full setup guide, see: [Setup GPU Resources for Model Serving](../../../../hybrid-manager/ai-factory/learn-how/model-serving/setup-gpu)
-
-## Steps to deploy an AI model
-
-### 1. Browse and select model in Model Library
-
-- Go to **AI Factory > Model Library**.
-- Browse available model images.
-- Review versions and tags.
-- Select the version you want to deploy.
-
-### 2. Configure and deploy model
-
-- Click **Deploy** or **Deploy to Model Serving**.
-- Configure deployment parameters:
-
-  - Number of replicas
-  - Resource requests/limits (GPU, CPU, memory)
-  - Model runtime settings (if needed)
-
-- Confirm and deploy.
-
-This triggers creation of:
-
-- A `ClusterServingRuntime` for the model (if not already defined).
-- An `InferenceService` for the specific deployment.
-
-### 3. Verify deployed model
-
-You can verify your deployed models using:
-
-**Model Serving UI in HCP**
-
-- Go to **AI Factory > Model Serving** (or Hybrid Manager > AI Factory > Model Serving).
-- Confirm model appears with status **Ready**.
-
-Or use kubectl:
-kubectl get InferenceService -n default
-
-Example output:
-```NAME STATUS URL GPUs
-meta-nim-llama-3-3-nemotron-super-49b Ready http://meta-nim-llama-3-3-nemotron-super-49b-predictor... 4
-nim-snowflake-arctic-embed-l Ready http://nim-snowflake-arctic-embed-l-predictor... 1
-...
-```
-
-
-### 4. Connect model to AI Factory workloads
-
-Once the model is Ready:
-
-- You can select it in:
-
-  - Knowledge Base pipelines (for embedding or reranking)
-  - Gen AI Builder pipelines
-  - Assistant configurations
-
-→ The UI will show models available for each use case based on their **type** (Embedding, Completion, Reranking, etc.).
-
-## Supported model types
-
-| Model Type | Example Image |
-|------------|---------------|
-| Text Completion | llama-3.3-nemotron-super-49b |
-| Text Embedding | arctic-embed-l |
-| Image Embedding | nvclip |
-| OCR | paddleocr |
-| Text Reranker | llama-3.2-nv-rerankqa-1b-v2 |
-
-## Tips & Best Practices
-
-- **GPU placement**: Ensure your model matches your GPU capacity. Large models like **llama-3.3-49b** require multiple GPUs on a single node.
-- **Quota management**: Limit number of large models deployed simultaneously to avoid overloading GPU nodes.
-- **Version testing**: Test new model versions in isolated deployments before promoting to production pipelines or Assistants.
-
-## Troubleshooting
-
-### Model stuck in Pending
-
-- Check GPU node taints/labels.
-- Verify InferenceService tolerations and nodeSelectors match.
-
-### Model not appearing in Model Library
-
-- Confirm image is correctly tagged and synced via Image and Model Library.
-- Verify repository rules if using private registry.
-
-### Kubernetes errors on deploy
-
-- Check `kubectl describe InferenceService <model>` for detailed error logs.
-
-## Summary
-
-- You can deploy AI models from the AI Factory Model Library.
-- Deployed models run via KServe Model Serving.
-- Deployed models power Knowledge Bases and Gen AI Builder Assistants.
-- The deployment flow ensures consistent governance and visibility.
-
-## Related Links
-
-- [Model Library Explained](../../explained/model-library-explained)
-- [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
-- [Setup GPU Resources for Model Serving](../../../../hybrid-manager/ai-factory/learn-how/model-serving/setup-gpu)
-- [Deploy NVIDIA NIM Models - Advanced Guide](../../../../hybrid-manager/ai-factory/learn-how/model-serving/deploy-nim-container)
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,28 +0,0 @@
----
-title: Model Library How-To Guides
-navTitle: Model Library How-To
-description: Practical guides for configuring and managing the AI Factory Model Library and integrating your private container registries.
----
-
-Use these guides to configure and manage the Model Library in AI Factory.
-
-The Model Library helps you organize, discover, and deploy AI models within your AI Factory projects — powered by the underlying Image Library in Hybrid Manager (HCP).
-
----
-
-## Where to start
-
-- [Integrate a private container registry](./integrate-private-registry)
-- [Define repository rules for your models](./define-repository-rules)
-- [Manage repository and image tag metadata](./manage-repository-metadata)
-- [Deploy AI models from the Model Library](./how-to-deploy-ai-models)
-
----
-
-## Related concepts
-
-- [Model Library Concepts](../../explained/model-library-explained)
-- [Model Image Library in Hybrid Manager](../../../../hybrid-manager/learn/explained/model-image-library/model-image-library-explained)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/integrate-private-registry.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/integrate-private-registry.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/integrate-private-registry.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/integrate-private-registry.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,143 +0,0 @@
----
-title: Integrate Private Registry with Model Library
-navTitle: Integrate Private Registry
-description: How to connect a private container registry to the AI Factory Model Library to enable the use of your own model images.
----
-
-# Integrate Private Registry with Model Library
-
-This guide explains how to integrate your organization's **private container registry** with the AI Factory **Model Library**.
-
-Once integrated, your custom-built or internally-approved model images will appear in the Model Library UI, ready to deploy into Model Serving.
-
-## Who should use this guide?
-
-- AI platform admins responsible for container registry governance.
-- DevOps engineers managing private registries for AI images.
-- Developers needing to deploy private AI models through AI Factory.
-
-## What this enables
-
-- You can register private registries with Hybrid Control Plane (HCP).
-- HCP can discover AI model images in your registry.
-- These images will be available in the **Model Library** for deployment to Model Serving.
-- You can control visibility and usage of private model images across your AI Factory workloads.
-
-## Estimated time to complete
-
-10–15 minutes per registry configuration.
-
-## Prerequisites
-
-Before you begin:
-
-- You must have admin access to your private container registry (AWS ECR, GCP GCR, Azure ACR, Harbor, or similar).
-- You must have admin access to HCP (to configure registry integration).
-- You must know the required credentials (username/password or token) for your registry.
-- Your registry must expose a compatible Docker Registry API v2 endpoint.
-
-## Supported registry types
-
-- AWS Elastic Container Registry (ECR)
-- Google Container Registry (GCR) / Artifact Registry
-- Azure Container Registry (ACR)
-- Harbor
-- Generic registries supporting Docker Registry API v2
-
-## Integration Steps
-
-### 1. Prepare registry credentials
-
-- For public registries → no authentication required.
-- For private registries → prepare one of:
-
-  - Username/password pair
-  - Personal access token
-  - Robot account credentials (Harbor)
-
-Confirm you can perform a `docker pull` of your model images locally using these credentials.
-
-### 2. Register your registry in HCP
-
-- In the HCP UI, go to:
-
-`AI Factory > Model Library > Manage Repositories > Add Registry`
-
-You will see a dialog prompting for:
-
-| Field | Description |
-|-------|-------------|
-| Registry URL | The full registry hostname (e.g., `myregistry.company.com`) |
-| Registry Type | Select from supported registry types |
-| Username | (Optional) Username for authentication |
-| Password/Token | (Optional) Password or token for authentication |
-| Registry Name (Label) | Friendly name displayed in Model Library UI |
-
-- Fill in the required fields.
-- Click **Add Registry**.
-
-### 3. Verify registry integration
-
-After adding:
-
-- HCP will attempt to connect to the registry and validate credentials.
-- If successful, your registry will appear in the **Manage Repositories** list.
-- HCP will begin periodic sync of repository tags from this registry.
-
-### 4. Define Repository Rules
-
-To control which repositories/tags are discovered:
-
-- After adding the registry, configure **Repository Rules**.
-- See: [Define Repository Rules for Model Library](define-repository-rules)
-
-### 5. Validate image availability
-
-After sync completes:
-
-- Go to **AI Factory > Model Library**.
-- Select the Registry scope or Repository filter.
-- Confirm your private model images appear with correct tags.
-
-You can now deploy these images via the normal **Deploy to Model Serving** flow.
-
-## Tips & Best Practices
-
-- Use **robot accounts** or **token-based auth** when possible to avoid exposing personal credentials.
-- Limit discovery scope via **Repository Rules** — avoid syncing the entire registry.
-- Tag images clearly with version info to aid selection in Model Library.
-- For multi-tenant environments, segment registry visibility carefully.
-
-## Troubleshooting
-
-### Registry connection failed
-
-- Check Registry URL (must not include `https://`, just the hostname).
-- Validate credentials by testing `docker login` manually.
-- Ensure Registry API v2 is enabled.
-
-### Images not appearing
-
-- Check that Repository Rules allow the relevant repository.
-- Verify image has a valid tag.
-- Confirm periodic sync has completed.
-
-### Authentication errors
-
-- For AWS ECR → ensure IAM permissions allow `ecr:GetAuthorizationToken`.
-- For Azure ACR → ensure token has `acrPull` role.
-- For GCR → use a service account with Artifact Registry access.
-
-## Summary
-
-- You can integrate private registries with Model Library.
-- Your private model images become available to deploy via Model Serving.
-- You can govern visibility via Repository Rules.
-- Model Library unifies both public and private image sources for your AI workloads.
-
-## Related Links
-
-- [Model Library Explained](../../explained/model-library-explained)
-- [Define Repository Rules for Model Library](define-repository-rules)
-- [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
-- [Deploy AI Models from Model Library](deploy-ai-models)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/manage-repository-metadata.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/manage-repository-metadata.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/manage-repository-metadata.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-library/manage-repository-metadata.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,123 +0,0 @@
----
-title: Manage Repository and Image Tag Metadata
-navTitle: Manage Metadata
-description: How to manage metadata for repositories and image tags in the Image and Model Library.
----
-
-# Manage Repository and Image Tag Metadata
-
-This guide explains how to manage metadata for your repositories and image tags in the Image and Model Library. Proper metadata helps you track, organize, and govern images used for AI model serving and Postgres cluster provisioning.
-
-## Who should use this guide?
-
-- Platform admins curating approved image sets.
-- DevOps teams tagging images for specific environments (Dev, Test, Prod).
-- AI/ML teams managing model lifecycle in AI Factory.
-- Security & compliance owners applying ownership metadata.
-
-## What this enables
-
-- Apply descriptive tags to repositories and image tags.
-- Edit READMEs for repositories for team guidance.
-- Track image ownership and promotion state.
-- Provide clear context to end users in Image Library UI.
-
-## Estimated time to complete
-
-5–10 minutes per repository/image.
-
-## Prerequisites
-
-- You have access to Image and Model Library in Hybrid Manager or AI Factory.
-- Your repository integration is already configured and image sync completed.
-- You have appropriate permissions to edit metadata.
-
-## Supported Metadata Features
-
-| Metadata Type | Applies To |
-|---------------|------------|
-| HCP Tags | Repository, Image Tag |
-| README | Repository |
-
-## Managing Repository Metadata
-
-### Apply HCP Tags to a Repository
-
-1. Navigate to Image and Model Library.
-2. Click on desired Repository (e.g. `postgresql`, `meta/llama-3.3`, `your-private-model-repo`).
-3. Click **Manage Tags** or **Edit Tags**.
-4. Add HCP Tags:
- - Example: `approved-production`, `beta`, `team-ml`, `archived`.
-5. Save changes.
-
-### Edit Repository README
-
-1. On the Repository page, click **Edit README**.
-2. Enter or update README content (supports Markdown).
- - Example sections:
-  - Purpose of repository.
-  - Model or DB image types contained.
-  - Support contacts.
-  - Security notes.
-3. Save README. It will now display on Repository details page.
-
-## Managing Image Tag Metadata
-
-### Apply HCP Tags to an Image Tag
-
-1. Navigate to Repository.
-2. Click **Image Tags** tab.
-3. For desired Image Tag, click **Manage Tags** or **Edit Tags**.
-4. Add HCP Tags:
- - Example: `approved-production`, `testing`, `deprecated`.
-5. Save changes.
-
-### Common Tag Usage Patterns
-
-| Tag Example | Usage |
-|-------------|-------|
-| `approved-production` | Images approved for Production use. |
-| `beta` | Images under evaluation. |
-| `team-ml` | Models used by ML team. |
-| `archived` | Images no longer in active use. |
-| `test-only` | Images used for testing / PoC deployments. |
-
-## Best Practices
-
-- Always apply `approved-production` only after full validation.
-- Clearly tag deprecated or archived images.
-- Use team-based tags (`team-ml`, `team-dba`) to indicate ownership.
-- Add README to every custom Repository — improves transparency.
-- Use Image Tags + HCP Tags for precise version tracking (ex: `postgresql:17.4-xxxx` tagged with `approved-production`).
-
-## Limitations & Notes
-
-- Applying Tags / README does not affect image pull or runtime behavior — they are for UI and governance.
-- Tags must follow naming conventions established in your org.
-- Currently HCP Tags do not propagate to private registry — they are stored in HCP metadata.
-
-## Example Workflow
-
-Scenario: ML team is preparing to move a new Reranker model into production.
-
-1. Team syncs model image `nv-rerankqa-1b-v2:1.8.2` into `private-model-repo`.
-2. After test validation:
- - Apply `approved-production` tag to this Image Tag.
- - Apply `team-ml` to Repository and Image Tag.
- - Edit Repository README to include model version history and ownership.
-3. Now AI Factory users can easily identify `approved-production` Reranker model.
-
-## Summary
-
-- You can manage HCP Tags and READMEs for Repositories and Image Tags.
-- This improves governance, tracking, and transparency.
-- Always tag Production-approved images clearly.
-- Keep READMEs updated for team guidance.
-
-## Related Links
-
-- [Deploy AI Models from Model Library](deploy-ai-models-from-model-library)
-- [Integrate Private Registry](integrate-private-registry)
-- [Define Repository Rules](define-repository-rules)
-- [Model Library Explained](../../explained/model-library-explained)
-- [Image and Model Library Explained](../../../../hybrid-manager/learn/explained/model-image-library)
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/configure-servingruntime.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/configure-servingruntime.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/configure-servingruntime.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/configure-servingruntime.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,156 +0,0 @@
----
-title: Configure a ClusterServingRuntime
-navTitle: Configure ServingRuntime
-description: Learn how to configure a ClusterServingRuntime in KServe to define an AI model serving environment on Kubernetes.
----
-
-# Configure a ClusterServingRuntime
-
-This guide explains how to configure a **ClusterServingRuntime** in KServe.
-A ClusterServingRuntime defines the environment used to serve your AI models — specifying container image, resource settings, environment variables, and supported model formats.
-
-For Hybrid Manager users, configuring runtimes is a core step toward enabling Model Serving — see [Model Serving in Hybrid Manager](../../hybrid-manager/ai-factory/model-serving).
-
-## Goal
-
-Configure a ClusterServingRuntime so it can be used by InferenceServices to deploy models.
-
-## Estimated time
-
-5–10 minutes.
-
-## What you will accomplish
-
-- Define a ClusterServingRuntime YAML manifest.
-- Apply it to your Kubernetes cluster.
-- Enable reusable serving configuration for one or more models.
-
-## What this unlocks
-
-- Supports consistent deployment of models using a standard runtime definition.
-- Allows for centralized control over serving images and resource profiles.
-- Required step for deploying NVIDIA NIM containers with KServe.
-
-## Prerequisites
-
-- Kubernetes cluster with KServe installed.
-- Access to container image registry with the desired model server image.
-- NVIDIA GPU node pool configured (if using GPU-based models).
-- (If required) Kubernetes secret configured for API keys (e.g., NVIDIA NGC).
-
-For background concepts, see:
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#clusterservingruntime)
-
-## Steps
-
-### 1. Create ClusterServingRuntime YAML
-
-Create a file named `ClusterServingRuntime.yaml`.
-
-Example:
-
-apiVersion: serving.kserve.io/v1alpha1
-kind: ClusterServingRuntime
-metadata:
-name: nvidia-nim-llama-3.1-8b-instruct-1.3.3
-namespace: default
-spec:
-containers:
-- env:
-- name: NIM_CACHE_PATH
-value: /tmp
-- name: NGC_API_KEY
-valueFrom:
-secretKeyRef:
-name: nvidia-nim-secrets
-key: NGC_API_KEY
-image: upmdev.azurecr.io/nim/meta/llama-3.1-8b-instruct:1.3.3
-name: kserve-container
-ports:
-- containerPort: 8000
-protocol: TCP
-resources:
-limits:
-cpu: "12"
-memory: 64Gi
-requests:
-cpu: "12"
-memory: 64Gi
-volumeMounts:
-- mountPath: /dev/shm
-name: dshm
-imagePullSecrets:
-- name: edb-cred
-protocolVersions:
-- v2
-- grpc-v2
-supportedModelFormats:
-- autoSelect: true
-name: nvidia-nim-llama-3.1-8b-instruct
-priority: 1
-version: "1.3.3"
-volumes:
-- emptyDir:
-medium: Memory
-sizeLimit: 16Gi
-name: dshm
-
-**Key fields explained:**
-
-- `containers.image`: The model server container (e.g., NVIDIA NIM image).
-- `resources`: CPU, memory, and GPU requirements.
-- `NGC_API_KEY`: Secret reference for NVIDIA models.
-- `supportedModelFormats`: Logical name used by InferenceService to reference this runtime.
-
-### 2. Apply the ClusterServingRuntime
-
-Run:
-
-kubectl apply -f ClusterServingRuntime.yaml
-
-### 3. Verify deployed ClusterServingRuntime
-
-Run:
-
-kubectl get ClusterServingRuntime
-
-Example:
-
-NAME                                     AGE
-nvidia-nim-llama-3.1-8b-instruct-1.3.3   1m
-
-You can inspect full details with:
-
-kubectl get ClusterServingRuntime <name> -o yaml
-
-### 4. Reference runtime in InferenceService
-
-When you create your InferenceService, reference this runtime:
-
-runtime: nvidia-nim-llama-3.1-8b-instruct-1.3.3
-modelFormat:
-name: nvidia-nim-llama-3.1-8b-instruct
-
-See [Deploy an NVIDIA NIM container with KServe](./deploy-nim-container).
-
-## Notes
-
-- Runtimes are reusable — you can deploy multiple models referencing the same ClusterServingRuntime.
-- Use meaningful names and version fields in `supportedModelFormats` for traceability.
-- You can update a runtime by editing and re-applying the YAML.
-
-## Next steps
-
-- [Deploy an NVIDIA NIM container with KServe](./deploy-nim-container)
-- [Update GPU resources for a deployed model](./update-gpu-resources)
-- [Monitor deployed models with KServe](./monitor-model-serving)
-
-## Related reading
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#clusterservingruntime)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/create-inferenceservice.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/create-inferenceservice.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/create-inferenceservice.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/create-inferenceservice.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,127 +0,0 @@
----
-title: Create an InferenceService for NVIDIA NIM Container
-navTitle: Create InferenceService
-description: How to create an InferenceService to deploy an NVIDIA NIM container with KServe on Kubernetes.
----
-
-# Create an InferenceService for NVIDIA NIM Container
-
-This How-To explains how to deploy an NVIDIA NIM container as an InferenceService using KServe on Kubernetes.
-
-You will define and apply an `InferenceService.yaml` manifest that references an existing ClusterServingRuntime and runs a GPU-accelerated model instance.
-
-This is a core step to making your model available for inference through a network-accessible endpoint.
-
-## Goal
-
-Deploy an InferenceService for an NVIDIA NIM container using KServe.
-
-## Estimated time
-
-10–15 minutes.
-
-## What you will accomplish
-
-- Define an InferenceService manifest.
-- Deploy the InferenceService to your Kubernetes cluster.
-- Make the model available via a network endpoint.
-
-## What this unlocks
-
-Once deployed, your model can be consumed by applications (such as Gen AI Builder agents) via standard inference APIs.
-It will also appear in your KServe monitoring and can be scaled or updated dynamically.
-
-## Prerequisites
-
-- Kubernetes cluster with KServe installed and configured.
-- ClusterServingRuntime configured for your NIM container.
-See [Configure ClusterServingRuntime](./configure-serving-runtime).
-- GPU-enabled node pool provisioned and labeled appropriately.
-- NVIDIA NGC API key stored as Kubernetes secret, if required.
-- The desired NIM container image available in your registry.
-
-## Steps
-
-### 1. Create InferenceService manifest
-
-Create a file named `InferenceService.yaml` with the following content:
-
-apiVersion: serving.kserve.io/v1beta1
-kind: InferenceService
-metadata:
-annotations:
-serving.kserve.io/enable-prometheus-scraping: "true"
-prometheus.kserve.io/port: "8000"
-prometheus.kserve.io/path: "/v1/metrics"
-name: llama-3-1-8b-instruct-1xgpu-g5
-namespace: default
-spec:
-predictor:
-tolerations:
-- key: nvidia.com/gpu
-operator: Exists
-effect: NoSchedule
-nodeSelector:
-nvidia.com/gpu: "true"
-imagePullSecrets:
-- name: edb-cred
-model:
-modelFormat:
-name: nvidia-nim-llama-3.1-8b-instruct
-resources:
-limits:
-nvidia.com/gpu: "1"
-requests:
-nvidia.com/gpu: "1"
-runtime: nvidia-nim-llama-3.1-8b-instruct-1.3.3
-
-Key points:
-
-- **name** → provide a unique name for your InferenceService.
-- **tolerations** and **nodeSelector** → must match your GPU node pool configuration.
-- **runtime** and **modelFormat** → must match the values used in your ClusterServingRuntime.
-- **GPU resources** → specify the number of GPUs to allocate.
-
-### 2. Apply the InferenceService
-
-Run:
-
-kubectl apply -f InferenceService.yaml
-
-### 3. Verify the deployment
-
-Check the InferenceService status:
-
-kubectl get InferenceService -o custom-columns=NAME:.metadata.name,MODEL:.spec.predictor.model.modelFormat.name,URL:.status.address.url,RUNTIME:.spec.predictor.model.runtime,GPUs:.spec.predictor.model.resources.limits.nvidia\\.com/gpu --namespace=default
-
-You should see:
-
-- The name of your InferenceService.
-- The model format being served.
-- The endpoint URL.
-- The ClusterServingRuntime used.
-- The number of GPUs allocated.
-
-## What’s next
-
-Now that your model is deployed:
-
-- Your application (or Gen AI Builder agent) can send inference requests to the provided URL.
-- You can monitor model performance and resource usage.
-- You can update the GPU allocation or other runtime parameters if needed.
-
-## Related topics
-
-- [Configure ClusterServingRuntime](./configure-serving-runtime)
-- [Deploy NVIDIA NIM container with KServe](./deploy-nim-container) *(placeholder)*
-*(future page showing full deployment flow starting from scratch)*
-- [AI Factory Concepts](../explained-ai-factory-concepts)
-- [AI Factory Terminology](../terminology)
-
-## Next steps
-
-- [Update GPU resources for InferenceService](./update-gpu-resources)
-- [List deployed InferenceServices](./list-inferenceservices)
-- [Monitor KServe deployments](#)
-
-Explore more in the [Analytics & AI Factory learning guide](../learn/index).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/deploy-nim-container.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/deploy-nim-container.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/deploy-nim-container.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/deploy-nim-container.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,172 +0,0 @@
----
-title: Deploy an NVIDIA NIM container with KServe
-navTitle: Deploy NIM Container
-description: Learn how to deploy an NVIDIA NIM container using KServe on a Kubernetes cluster. Understand core concepts and prepare for using this capability in EDB Hybrid Manager AI Factory.
----
-
-# Deploy an NVIDIA NIM container with KServe
-
-This guide explains how to deploy an NVIDIA NIM container using KServe on a Kubernetes cluster.
-
-While the steps here apply to general Kubernetes environments, in Hybrid Manager AI Factory, we provide additional value such as lifecycle management, observability, and simplified integration. Learn more in the [Model Serving in Hybrid Manager](../../hybrid-manager/ai-factory/model-serving) section.
-
-## Goal
-
-Deploy an NVIDIA NIM container using KServe to create a network-accessible inference service that can be consumed by applications.
-
-## Estimated time
-
-15–30 minutes depending on cluster setup.
-
-## What you will accomplish
-
-- Define and deploy a ClusterServingRuntime for an NVIDIA NIM container.
-- Deploy an InferenceService that uses this runtime.
-- Validate your deployment and retrieve the model endpoint.
-
-## What this unlocks
-
-- Ability to serve NVIDIA NIM models via standard inference protocols (HTTP/gRPC).
-- Prepare to integrate these models with applications or tools such as Griptape (Gen AI Builder) or AIDB Knowledge Bases.
-- Foundation for using Hybrid Manager AI Factory model-serving capabilities.
-
-## Prerequisites
-
-- Kubernetes cluster with KServe installed.
-- GPU node pool configured (with NVIDIA device plugin).
-- NVIDIA NIM container image available in a private registry or NGC.
-- Kubernetes secret containing NGC API Key.
-- kubectl configured for your cluster.
-
-For background concepts, see:
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#model-serving)
-
-## Steps
-
-### 1. Create ClusterServingRuntime
-
-Define `ClusterServingRuntime.yaml`:
-
-apiVersion: serving.kserve.io/v1alpha1
-kind: ClusterServingRuntime
-metadata:
-name: nvidia-nim-llama-3.1-8b-instruct-1.3.3
-namespace: default
-spec:
-containers:
-- env:
-- name: NIM_CACHE_PATH
-value: /tmp
-- name: NGC_API_KEY
-valueFrom:
-secretKeyRef:
-name: nvidia-nim-secrets
-key: NGC_API_KEY
-image: your-registry/nim/meta/llama-3.1-8b-instruct:1.3.3
-name: kserve-container
-ports:
-- containerPort: 8000
-protocol: TCP
-resources:
-limits:
-cpu: "12"
-memory: 64Gi
-requests:
-cpu: "12"
-memory: 64Gi
-volumeMounts:
-- mountPath: /dev/shm
-name: dshm
-imagePullSecrets:
-- name: edb-cred
-protocolVersions:
-- v2
-- grpc-v2
-supportedModelFormats:
-- autoSelect: true
-name: nvidia-nim-llama-3.1-8b-instruct
-priority: 1
-version: "1.3.3"
-volumes:
-- emptyDir:
-medium: Memory
-sizeLimit: 16Gi
-name: dshm
-
-Apply the runtime:
-
-kubectl apply -f ClusterServingRuntime.yaml
-
-### 2. Create InferenceService
-
-Define `InferenceService.yaml`:
-
-apiVersion: serving.kserve.io/v1beta1
-kind: InferenceService
-metadata:
-annotations:
-serving.kserve.io/enable-prometheus-scraping: "true"
-prometheus.kserve.io/port: "8000"
-prometheus.kserve.io/path: "/v1/metrics"
-name: llama-3-1-8b-instruct-1xgpu
-namespace: default
-spec:
-predictor:
-tolerations:
-- key: nvidia.com/gpu
-operator: Exists
-effect: NoSchedule
-nodeSelector:
-nvidia.com/gpu: "true"
-imagePullSecrets:
-- name: edb-cred
-model:
-modelFormat:
-name: nvidia-nim-llama-3.1-8b-instruct
-resources:
-limits:
-nvidia.com/gpu: "1"
-requests:
-nvidia.com/gpu: "1"
-runtime: nvidia-nim-llama-3.1-8b-instruct-1.3.3
-
-Deploy the InferenceService:
-
-kubectl apply -f InferenceService.yaml
-
-### 3. Verify deployed models
-
-List active InferenceServices:
-
-kubectl get InferenceService \
--o custom-columns=NAME:.metadata.name,MODEL:.spec.predictor.model.modelFormat.name,URL:.status.address.url,RUNTIME:.spec.predictor.model.runtime,GPUs:.spec.predictor.model.resources.limits.nvidia\\.com/gpu \
---namespace=default
-
-Example output:
-
-NAME                           MODEL                              URL                                                                         RUNTIME                                  GPUs
-llama-3-1-8b-instruct-1xgpu    nvidia-nim-llama-3.1-8b-instruct   http://llama-3-1-8b-instruct-1xgpu.default.svc.cluster.local                 nvidia-nim-llama-3.1-8b-instruct-1.3.3   1
-
-### 4. Retrieve runtime details
-
-Check port and resources:
-
-kubectl get ClusterServingRuntimes \
--o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image,PORT:.spec.containers[0].ports[0].containerPort,CPUs:.spec.containers[0].resources.limits.cpu,MEMORY:.spec.containers[0].resources.limits.memory \
---namespace=all-namespaces
-
-## Next steps
-
-- [Monitor deployed models](./monitor-model-serving) *(Coming soon)*
-- [Update GPU resources for a deployed model](./update-gpu-resources-for-nim) *(Coming soon)*
-
-## Related reading
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#model-serving)
-- [Model Serving in Hybrid Manager](../../hybrid-manager/ai-factory/model-serving) *(Coming soon)*
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/faq.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/faq.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/faq.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/faq.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,56 +0,0 @@
----
-title: Model Serving FAQ
-navTitle: FAQ
-description: Frequently Asked Questions about using Model Serving in AI Factory with KServe.
----
-
-# Model Serving FAQ
-
-Here are answers to common questions about using Model Serving with AI Factory.
-
-## What models are currently supported?
-
-AI Factory Model Serving (version 1.2) supports deploying models packaged as **NVIDIA NIM containers** using KServe.
-
-Support for deploying additional model types and formats will be expanded in future releases. See the [Supported Models Index](../../models/supported-models/index) for the latest information.
-
-## What Kubernetes resources are used for serving models?
-
-Models are served using [KServe InferenceServices](https://kserve.github.io/website/), which manage the lifecycle of model-serving pods.
-
-Supporting KServe resources such as ServingRuntimes and related GPU-configured Kubernetes nodes are also used.
-
-## What hardware is required?
-
-NIM-based model serving requires **GPU-enabled Kubernetes nodes**.
-
-Refer to the [GPU setup guide](../model-serving/update-gpu-resources) for supported configurations and node recommendations by cloud provider.
-
-## Can I deploy custom models?
-
-At this time, AI Factory Model Serving is optimized for **NIM-based containers**. Support for deploying arbitrary models via KServe is planned, but requires manual configuration outside of the current AI Factory streamlined flow.
-
-## How is scaling handled?
-
-KServe InferenceServices use **Kubernetes-native scaling**, with auto-scaling supported for compatible runtimes.
-
-Scaling behavior can be configured in the InferenceService specification (replicas, resources).
-
-## How do I monitor running models?
-
-You can monitor deployed models using:
-
-- **KServe InferenceService status**
-- Kubernetes monitoring tools (Prometheus, Grafana, etc.)
-- [Model Serving Monitoring Guide](./monitor-inferenceservice)
-
-## Where can I learn more about the model serving architecture?
-
-See:
-
-- [Model Serving Explained](../../learn/explained/model-serving-explained)
-- [AI Factory Concepts](../../learn/explained/ai-factory-concepts#model-serving-kserve)
-
----
-
-Have more questions? Please contact your AI Factory administrator or consult [AI Factory Support Resources](../../index).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,40 +0,0 @@
----
-title: Model Serving How-To Guides
-navTitle: Model Serving How-To Guides
-description: Practical guides for deploying, configuring, and managing model serving in AI Factory using KServe.
----
-
-# Model Serving How-To Guides
-
-This section provides step-by-step guides to help you deploy, configure, and manage models with the AI Factory Model Serving capability.
-
-These guides focus on real-world tasks you will perform when using KServe-based model serving within Hybrid Manager Kubernetes infrastructure.
-
-## Getting started
-
-- [Model Serving Quickstart](../../learn/model-serving/quickstart)
-
-## Deployment
-
-- [Configure KServe ServingRuntime](./configure-servingruntime)
-- [Create an InferenceService](./create-inferenceservice)
-- [Deploy NVIDIA NIM Containers](./deploy-nim-container)
-
-## GPU configuration and tuning
-
-- [Update GPU resources](./update-gpu-resources)
-
-## Monitoring and observability
-
-- [Monitor InferenceService](./monitor-inferenceservice)
-
----
-
-## Related Concepts
-
-- [Model Serving Explained](../../learn/explained/model-serving-explained)
-- [Supported Models](../../models/supported-models/index)
-
----
-
-For broader architecture context, see [AI Factory Concepts](../../learn/explained/ai-factory-concepts#model-serving-kserve).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/monitor-inferenceservice.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/monitor-inferenceservice.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/monitor-inferenceservice.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/monitor-inferenceservice.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,127 +0,0 @@
----
-title: Monitor deployed models with KServe
-navTitle: Monitor model serving
-description: Learn how to monitor deployed AI models using KServe, check status and resource utilization, and prepare for integration with Hybrid Manager AI Factory observability.
----
-
-# Monitor deployed models with KServe
-
-This guide explains how to monitor deployed AI models using KServe on Kubernetes.
-
-Monitoring your models helps ensure reliability, performance, and optimal use of resources — whether you are working in general Kubernetes or using Hybrid Manager AI Factory.
-
-For AI Factory users, Hybrid Manager will provide additional value-add monitoring and observability features — see [Model Serving in Hybrid Manager](../../hybrid-manager/ai-factory/model-serving).
-
-## Goal
-
-Monitor deployed models, check model status and serving endpoints, and retrieve resource usage information.
-
-## Estimated time
-
-5–10 minutes.
-
-## What you will accomplish
-
-- List deployed InferenceServices (models).
-- Retrieve model endpoint and runtime details.
-- Understand how to observe model performance.
-- Prepare to integrate model metrics into observability pipelines.
-
-## What this unlocks
-
-- Confidence that models are correctly deployed and serving.
-- Ability to troubleshoot or scale model deployments.
-- Foundation for using Hybrid Manager AI Factory observability for model serving.
-
-## Prerequisites
-
-- Deployed InferenceService on KServe.
-- ClusterServingRuntime defined.
-- kubectl configured for your Kubernetes cluster.
-
-For background concepts, see:
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#model-serving)
-
-## Steps
-
-### 1. List deployed InferenceServices
-
-To list deployed models and see key details:
-
-kubectl get InferenceService \
--o custom-columns=NAME:.metadata.name,MODEL:.spec.predictor.model.modelFormat.name,URL:.status.address.url,RUNTIME:.spec.predictor.model.runtime,GPUs:.spec.predictor.model.resources.limits.nvidia\\.com/gpu \
---namespace=default
-
-Key columns:
-
-- **NAME**: Name of the InferenceService.
-- **MODEL**: Model format name (from ClusterServingRuntime).
-- **URL**: Service endpoint for inference requests.
-- **RUNTIME**: ClusterServingRuntime used.
-- **GPUs**: Number of GPUs allocated.
-
-### 2. Retrieve runtime details
-
-To view ClusterServingRuntime details, including serving port and resource allocations:
-
-kubectl get ClusterServingRuntimes \
--o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image,PORT:.spec.containers[0].ports[0].containerPort,CPUs:.spec.containers[0].resources.limits.cpu,MEMORY:.spec.containers[0].resources.limits.memory \
---namespace=all-namespaces
-
-Key columns:
-
-- **NAME**: Name of the runtime.
-- **IMAGE**: Model server image used.
-- **PORT**: Inference port (commonly 8000 for NIM).
-- **CPUs**: CPU resources allocated.
-- **MEMORY**: Memory allocated.
-
-### 3. Observe model metrics
-
-If you enabled Prometheus scraping via InferenceService annotations:
-
-- serving.kserve.io/enable-prometheus-scraping: "true"
-
-Then Prometheus can scrape metrics at:
-
-/v1/metrics on port 8000 of the model service.
-
-Metrics typically include:
-
-- **Request latency**
-- **Throughput (requests per second)**
-- **Error rates**
-- **GPU utilization (if GPUs used)**
-
-You can visualize these metrics in tools such as **Grafana**.
-
-### 4. Check pod status
-
-For debugging, you can also view model pods directly:
-
-kubectl get pods --namespace=default
-
-Look for pods with names matching:
-
-`<inference-service-name>-predictor-*`
-
-Check pod status and logs if needed:
-
-kubectl logs <pod-name> --namespace=default
-
-## Next steps
-
-- [Update GPU resources for a deployed model](./update-gpu-resources-for-nim) *(Coming soon)*
-- [Deploy additional NVIDIA NIM models](./deploy-nim-container)
-- [Model Serving in Hybrid Manager](../../hybrid-manager/ai-factory/model-serving) *(Coming soon)*
-
-## Related reading
-
-- [Model Serving Explained](../../learn/explained-ai-factory-concepts#model-serving)
-- [AI Factory Terminology](../../learn/terminology#model-serving)
-- [Deploy an NVIDIA NIM container with KServe](./deploy-nim-container)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/observability.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/observability.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/observability.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/observability.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,113 +0,0 @@
----
-title: Observability for Model Serving
-navTitle: Observability
-description: Learn how to monitor and observe your Model Serving workloads in AI Factory.
----
-
-# Observability for Model Serving
-
-Observability helps you ensure that your deployed AI models are running efficiently and reliably within AI Factory.
-
-Model Serving in AI Factory uses KServe on Kubernetes to serve models. This provides built-in options to monitor:
-
-- Model serving status and availability
-- Resource usage (CPU, Memory, GPU)
-- Inference performance and throughput
-
----
-
-## Key monitoring capabilities
-
-### KServe InferenceService status
-
-You can inspect model serving status directly via Kubernetes:
-
-kubectl get inferenceservice -n <namespace>
-
-Common status fields include:
-
-- Ready / NotReady
-- URL endpoint
-- Current replicas
-- Allocated resources (GPU, CPU, Memory)
-
-For detailed inspection:
-
-kubectl describe inferenceservice <name> -n <namespace>
-
----
-
-### GPU utilization monitoring
-
-If your models are deployed on GPU nodes, monitor GPU usage to optimize resource allocation.
-
-Example:
-
-kubectl top node
-
-For deeper GPU-specific metrics (if supported):
-
-nvidia-smi
-
----
-
-### Prometheus and Grafana integration
-
-If Prometheus is configured, AI Factory model serving exposes metrics through KServe:
-
-Prometheus annotations to enable scraping:
-
-serving.kserve.io/enable-prometheus-scraping: "true"
-prometheus.kserve.io/port: "8000"
-prometheus.kserve.io/path: "/v1/metrics"
-
-You can build Grafana dashboards to monitor:
-
-- Inference requests per second
-- Latency and error rates
-- GPU utilization trends
-- Pod restarts and health
-
----
-
-### Logs and debugging
-
-You can access detailed logs from model serving pods:
-
-kubectl logs -f <pod-name> -n <namespace>
-
-Use logs to:
-
-- Debug model loading or initialization issues
-- Review inference request behavior
-- Monitor response handling and error conditions
-
----
-
-## Hybrid Manager observability integration
-
-When running AI Factory in conjunction with Hybrid Manager (HCP), you can integrate model serving observability into the broader HCP observability stack.
-
-See [Hybrid Manager Model Serving Observability](../../../../hybrid-manager/ai-factory/learn/how-to/model-serving/verify-models) for additional guidance on using HCP observability tools with Model Serving.
-
----
-
-## Best practices
-
-- Enable Prometheus scraping on InferenceServices for production.
-- Regularly monitor GPU usage to optimize capacity.
-- Set proper resource requests/limits in InferenceService specs.
-- Leverage Grafana dashboards for real-time observability.
-- Use log inspection to support debugging and tuning.
-
----
-
-## Related links
-
-- [Deploying NIM Containers](./deploy-nim-container)
-- [KServe in AI Factory Concepts](../explained-ai-factory-concepts#model-serving-kserve)
-- [Hybrid Manager Model Serving Observability](../../../../hybrid-manager/ai-factory/learn/how-to/model-serving/verify-models)
-
----
-
-By applying these observability practices, you can operate your AI Factory Model Serving workloads with greater confidence, proactively identifying and addressing issues before they impact users.
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/update-gpu-resources.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/update-gpu-resources.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/update-gpu-resources.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/how-to/model-serving/update-gpu-resources.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,102 +0,0 @@
----
-title: Update GPU Resources for an InferenceService
-navTitle: Update GPU Resources
-description: How to update GPU resource allocation for an NVIDIA NIM InferenceService deployed with KServe.
----
-
-# Update GPU Resources for an InferenceService
-
-This How-To explains how to adjust the number of GPUs allocated to an existing InferenceService deployed with KServe.
-This allows you to scale your model deployment dynamically without redeploying from scratch.
-
-## Goal
-
-Change the GPU resource allocation (number of GPUs) for a deployed InferenceService.
-
-## Estimated time
-
-5 minutes.
-
-## What you will accomplish
-
-- Edit an existing InferenceService resource.
-- Apply updated GPU resource limits and requests.
-- Trigger KServe to redeploy the model container with new GPU settings.
-
-## What this unlocks
-
-Enables dynamic scaling of GPU resources to optimize performance and cost:
-
-- Increase GPUs for faster response time and higher throughput.
-- Reduce GPUs when demand is lower to save resources.
-
-## Prerequisites
-
-- An InferenceService already deployed using KServe.
-See [Create InferenceService for NVIDIA NIM Container](./create-inferenceservice).
-- GPU-enabled node pool available in your Kubernetes cluster.
-
-## Steps
-
-### 1. Edit the InferenceService
-
-Run:
-
-kubectl edit InferenceService <your-inferenceservice-name>
-
-Example:
-
-kubectl edit InferenceService llama-3-1-8b-instruct-1xgpu-g5
-
-### 2. Update GPU resource settings
-
-Locate this section:
-
-spec:
-predictor:
-model:
-resources:
-limits:
-nvidia.com/gpu: "1"
-requests:
-nvidia.com/gpu: "1"
-
-Change both **limits** and **requests** to the desired number of GPUs.
-Example to scale to 4 GPUs:
-
-spec:
-predictor:
-model:
-resources:
-limits:
-nvidia.com/gpu: "4"
-requests:
-nvidia.com/gpu: "4"
-
-Save and close the editor.
-
-### 3. Verify the updated GPU allocation
-
-Run:
-
-kubectl get InferenceService -o custom-columns=NAME:.metadata.name,MODEL:.spec.predictor.model.modelFormat.name,URL:.status.address.url,RUNTIME:.spec.predictor.model.runtime,GPUs:.spec.predictor.model.resources.limits.nvidia\\.com/gpu --namespace=default
-
-Confirm that the **GPUs** column reflects your updated setting.
-
-KServe will automatically redeploy the InferenceService with the new configuration.
-
-## Related topics
-
-- [Create InferenceService for NVIDIA NIM Container](./create-inferenceservice)
-- [Configure ClusterServingRuntime](./configure-serving-runtime)
-- [Deploy NVIDIA NIM container with KServe](./deploy-nim-container) *(placeholder)*
-- [AI Factory Concepts](../explained-ai-factory-concepts)
-- [AI Factory Terminology](../terminology)
-
-## Next steps
-
-- [List deployed InferenceServices](./list-inferenceservices)
-- [Monitor KServe deployments](#)
-- [Tune model performance](#)
-
-Explore more in the [Analytics & AI Factory learning guide](../learn/index).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/101.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/101.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/101.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/101.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,48 +0,0 @@
----
-title: 101 Path — Getting Started with AI Factory
-navTitle: 101 Path
-description: Start your AI Factory journey — learn the basics of building AI-powered applications with Assistants, Knowledge Bases, and Tools.
----
-
-## Who this is for
-
-- New AI Factory users
-- Developers and product teams getting started with building AI-powered features
-- Data teams enabling their organization to use AI Factory
-
----
-
-## Goals
-
-- Understand core AI Factory concepts
-- Create your first AI Assistant
-- Connect data sources and build a Knowledge Base
-- Test a simple Gen AI use case
-
----
-
-## Topics covered
-
-- AI Factory Concepts
-- Assistants and Structures
-- Rulesets and Governance
-- Knowledge Bases
-- Data Sources and Retrievers
-- Tools and simple API calls
-- Viewing Threads and interaction history
-
----
-
-## Suggested Learning Path
-
-1. [AI Factory Concepts](../explained/ai-factory-concepts)
-2. [Create an Assistant](../explained/create-assistant)
-3. [Create a Knowledge Base](../how-to/gen-ai/create-knowledge-base)
-4. [Connect a Data Source](../how-to/gen-ai/data-source-data-lake)
-5. [Create a Retriever](../how-to/gen-ai/create-retriever)
-6. [Create a Ruleset](../how-to/gen-ai/create-ruleset)
-7. [Create a Tool](../how-to/gen-ai/create-tool)
-8. [View Threads](../how-to/gen-ai/view-threads)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/201.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/201.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/201.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/201.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,46 +0,0 @@
----
-title: 201 Path — Building Production-Ready AI Features
-navTitle: 201 Path
-description: Deepen your AI Factory skills — learn to manage complex Assistants, advanced data pipelines, and model serving.
----
-
-## Who this is for
-
-- Developers integrating Gen AI features into production apps
-- Data teams building pipelines for AI-driven knowledge bases
-- MLOps and platform teams configuring model serving
-
----
-
-## Goals
-
-- Build multi-step AI Assistants and Structures
-- Use advanced data sources and processing pipelines
-- Configure and deploy GPU-powered model serving
-- Implement observability and monitoring for AI features
-
----
-
-## Topics covered
-
-- Advanced Assistant design
-- Complex Structures
-- Multi-source Knowledge Bases
-- Hybrid Knowledge Base best practices
-- Model Serving with KServe
-- Observability for AI features
-
----
-
-## Suggested Learning Path
-
-1. [Hybrid Knowledge Base best practices](../how-to/gen-ai/hybrid-kb-best-practices)
-2. [Manage a Knowledge Base](../how-to/gen-ai/manage-knowledge-base)
-3. [Configure ServingRuntime](../how-to/model-serving/configure-servingruntime)
-4. [Deploy a NIM container](../how-to/model-serving/deploy-nim-container)
-5. [Monitor InferenceService](../how-to/model-serving/monitor-inferenceservice)
-6. [Update GPU Resources](../how-to/model-serving/update-gpu-resources)
-7. [Observability for Model Serving](../how-to/model-serving/observability)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/301.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/301.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/301.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/301.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,46 +0,0 @@
----
-title: 301 Path — Advanced AI Factory Usage and Extensibility
-navTitle: 301 Path
-description: Master advanced AI Factory topics — build complex, agentic AI Assistants, extend model serving, and design governance and observability at scale.
----
-
-## Who this is for
-
-- Advanced developers building AI-first applications
-- MLOps teams managing large-scale model serving
-- Platform engineers building extensions and integrations
-
----
-
-## Goals
-
-- Build complex, agentic Assistants with tool calling
-- Extend model serving with custom runtimes and explainability
-- Automate AI Factory pipelines
-- Design advanced monitoring and governance
-- Integrate AI Factory with enterprise systems
-
----
-
-## Topics covered
-
-- Agentic Assistants
-- Tool-chaining and advanced Tools
-- Custom ServingRuntime definitions
-- Model explainability with KServe
-- AI Factory observability pipelines
-- API-driven automation of AI Factory
-
----
-
-## Suggested Learning Path
-
-1. [Tools Explained](../explained/tools-explained)
-2. [Create a Tool](../how-to/gen-ai/create-tool)
-3. [Configure ServingRuntime](../how-to/model-serving/configure-servingruntime)
-4. [Advanced ServingRuntime configuration] *(future — custom transformers/explainers)*
-5. [Observability for Model Serving](../how-to/model-serving/observability)
-6. [Monitor and automate with API](../../using-the-api/index)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/paths/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,107 +0,0 @@
----
-title: Learning Paths
-navTitle: Learning Paths
-description: Explore curated learning paths to master AI Factory — from getting started to advanced production deployments.
----
-
-AI Factory Learning Paths provide structured guidance to help you build AI-powered features using AI Factory.
-
-Whether you're just starting or scaling advanced AI workloads in production, these paths guide you through concepts, tools, and practical implementation — building on your experience as you progress.
-
----
-
-## How to use the Learning Paths
-
-Each path is designed to be self-paced. You can complete paths sequentially or dive into topics based on your role and project needs.
-
-For each path, we recommend:
-- Reviewing the **prerequisites** to ensure readiness.
-- Following the learning flow of **Concepts → How-To Guides → Tutorials → Practice**.
-- Using your existing AI Factory environment or a sandbox project to try the examples hands-on.
-
----
-
-## Learning Paths
-
-### [101 Path — Getting Started with AI Factory](./101)
-
-**Audience:** New users, developers, data engineers, architects.
-**Estimated time:** 1–2 hours.
-**Prerequisites:** Familiarity with basic AI concepts and using web applications.
-
-**What you'll learn:**
-- Core AI Factory concepts
-- Navigating AI Factory and Hybrid Manager
-- Creating your first AI Assistants
-- Connecting Knowledge Bases and Retrievers
-- Running and viewing interaction Threads
-- Understanding key terminology
-
-[Start the 101 Path →](./101)
-
----
-
-### [201 Path — Building Production-Ready AI Features](./201)
-
-**Audience:** Developers, data engineers, solution architects building production AI features.
-**Estimated time:** 2–4 hours.
-**Prerequisites:** Completion of 101 Path, basic knowledge of Kubernetes concepts.
-
-**What you'll learn:**
-- Architecting hybrid Knowledge Bases
-- Using and managing Rulesets
-- Building more advanced Assistants and Tools
-- GPU-powered Model Serving with KServe
-- Monitoring and observability best practices
-- Implementing production-readiness and scaling strategies
-
-[Start the 201 Path →](./201)
-
----
-
-### [301 Path — Advanced AI Factory Usage and Extensibility](./301)
-
-**Audience:** AI platform owners, advanced developers, ML engineers, architects.
-**Estimated time:** 4–6 hours (self-paced, advanced topics).
-**Prerequisites:** Completion of 201 Path, experience with Kubernetes and container-based AI workloads.
-
-**What you'll learn:**
-- Designing Agentic Assistants and complex Structures
-- Custom Tools development and deployment
-- Advanced Model Serving and custom ServingRuntimes
-- Model explainability and responsible AI patterns
-- Automation of AI Factory via API-driven workflows
-- Advanced observability and performance tuning
-
-[Start the 301 Path →](./301)
-
----
-
-## Recommended Training Courses
-
-To complement these self-paced Learning Paths, we also offer:
-
-### Instructor-Led Training
-- Advanced AI Factory Architectures
-- AI Factory Administration & Operations
-- Custom Model Development & Deployment with AI Factory
-
-### Self-Paced Training
-- Introduction to AI Factory
-- Building AI Assistants with AI Factory
-- Managing AI Models and Hybrid Knowledge Bases
-- Scaling AI Workloads with AI Factory
-
-*For more details, see the [Training Overview](../../training/index) and [Instructor-Led Training Catalog](../../training/instructor-led) (coming soon).*
-
----
-
-## Where to next?
-
-- [AI Factory Concepts](../explained/ai-factory-concepts)
-- [How-To Guides](../how-to/gen-ai/create-assistant)
-- [Model Serving Quickstart](../how-to/model-serving/index)
-- [Tutorials](../tutorials)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/solutions.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/solutions.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/solutions.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/solutions.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,56 +0,0 @@
----
-title: Industry Solutions with AI Factory
-navTitle: Solutions
-description: See how AI Factory supports common industry scenarios and example solutions.
----
-
-AI Factory provides flexible capabilities that can be used to build AI-powered solutions across industries.
-
-This page highlights example solutions and patterns for selected industries — and links to relevant [how-to guides](./how-to/gen-ai/index) and [learning paths](./paths/index) to help you implement them.
-
-→ For technical patterns, see [Use Cases](./use-cases).
-
----
-
-## Financial Services
-
-Example solutions:
-- **Client Support Chatbots** → [Create Assistant](./how-to/gen-ai/create-assistant), [Create Knowledge Base](./how-to/gen-ai/create-knowledge-base), [Rulesets](./how-to/gen-ai/create-ruleset)
-- **Internal Knowledge Search** → [Create Retriever](./how-to/gen-ai/create-retriever), [Pipeline](../pipeline/gettingstarted/index)
-- **Automated Document Processing (KYC, onboarding)** → [Pipelines](../pipeline/gettingstarted/index), [OCR](../pipeline/preparers/examples/perform_ocr)
-
-## Healthcare and Life Sciences
-
-Example solutions:
-- **Medical Literature Search** → [Knowledge Base](./how-to/gen-ai/create-knowledge-base), [Retriever](./how-to/gen-ai/create-retriever), [Assistant](./how-to/gen-ai/create-assistant)
-- **Clinical Document Summarization** → [Pipeline](../pipeline/gettingstarted/index), [Model Serving](./how-to/model-serving/index)
-- **Patient Support Assistants** → [Create Assistant](./how-to/gen-ai/create-assistant), [Rulesets](./how-to/gen-ai/create-ruleset), [Tools](./how-to/gen-ai/create-tool)
-
-## Retail and E-Commerce
-
-Example solutions:
-- **Semantic Product Search** → [Model Serving](./how-to/model-serving/index), [Retriever](./how-to/gen-ai/create-retriever)
-- **Personalized Recommendations** → [Create Assistant](./how-to/gen-ai/create-assistant), [Retriever](./how-to/gen-ai/create-retriever), Tools for API integration
-- **Customer Service Automation** → [Assistant](./how-to/gen-ai/create-assistant), [Knowledge Base](./how-to/gen-ai/create-knowledge-base), [Rulesets](./how-to/gen-ai/create-ruleset)
-
-## Manufacturing and Industrial
-
-Example solutions:
-- **Knowledge-driven Maintenance Assistants** → [Assistant](./how-to/gen-ai/create-assistant), [Knowledge Base](./how-to/gen-ai/create-knowledge-base), [Tools](./how-to/gen-ai/create-tool)
-- **Document Search across manuals and SOPs** → [Pipeline](../pipeline/gettingstarted/index), [Retriever](./how-to/gen-ai/create-retriever)
-
-## Public Sector and Government
-
-Example solutions:
-- **Internal Policy Search and Compliance QA** → [Knowledge Base](./how-to/gen-ai/create-knowledge-base), [Retriever](./how-to/gen-ai/create-retriever), [Assistant](./how-to/gen-ai/create-assistant)
-- **Citizen Support Portals** → [Assistant](./how-to/gen-ai/create-assistant), [Tools](./how-to/gen-ai/create-tool), [Rulesets](./how-to/gen-ai/create-ruleset)
-
----
-
-## Build Your Solution
-
-- [How-To Guides](./how-to/gen-ai/index) — step-by-step instructions
-- [Learning Paths](./paths/index) — structured journeys
-- [Use Cases](./use-cases) — common technical patterns
-
-→ Ready to get started? Pick a [learning path](./paths/index) or jump to the relevant [how-to guide](./how-to/gen-ai/index).
Index: advocacy_docs/edb-postgres-ai/ai-factory/learn/use-cases.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/use-cases.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/use-cases.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/learn/use-cases.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,107 +0,0 @@
----
-title: Learning Paths
-navTitle: Learning Paths
-description: Explore curated learning paths to master AI Factory — from getting started to advanced production deployments.
----
-
-AI Factory Learning Paths provide structured guidance to help you build AI-powered features using AI Factory.
-
-Whether you're just starting or scaling advanced AI workloads in production, these paths guide you through concepts, tools, and practical implementation — building on your experience as you progress.
-
----
-
-## How to use the Learning Paths
-
-Each path is designed to be self-paced. You can complete paths sequentially or dive into topics based on your role and project needs.
-
-For each path, we recommend:
-- Reviewing the **prerequisites** to ensure readiness.
-- Following the learning flow of **Concepts → How-To Guides → Tutorials → Practice**.
-- Using your existing AI Factory environment or a sandbox project to try the examples hands-on.
-
----
-
-## Learning Paths
-
-### [101 Path — Getting Started with AI Factory](./101)
-
-**Audience:** New users, developers, data engineers, architects.
-**Estimated time:** 1–2 hours.
-**Prerequisites:** Familiarity with basic AI concepts and using web applications.
-
-**What you'll learn:**
-- Core AI Factory concepts
-- Navigating AI Factory and Hybrid Manager
-- Creating your first AI Assistants
-- Connecting Knowledge Bases and Retrievers
-- Running and viewing interaction Threads
-- Understanding key terminology
-
-[Start the 101 Path →](./101)
-
----
-
-### [201 Path — Building Production-Ready AI Features](./201)
-
-**Audience:** Developers, data engineers, solution architects building production AI features.
-**Estimated time:** 2–4 hours.
-**Prerequisites:** Completion of 101 Path, basic knowledge of Kubernetes concepts.
-
-**What you'll learn:**
-- Architecting hybrid Knowledge Bases
-- Using and managing Rulesets
-- Building more advanced Assistants and Tools
-- GPU-powered Model Serving with KServe
-- Monitoring and observability best practices
-- Implementing production-readiness and scaling strategies
-
-[Start the 201 Path →](./201)
-
----
-
-### [301 Path — Advanced AI Factory Usage and Extensibility](./301)
-
-**Audience:** AI platform owners, advanced developers, ML engineers, architects.
-**Estimated time:** 4–6 hours (self-paced, advanced topics).
-**Prerequisites:** Completion of 201 Path, experience with Kubernetes and container-based AI workloads.
-
-**What you'll learn:**
-- Designing Agentic Assistants and complex Structures
-- Custom Tools development and deployment
-- Advanced Model Serving and custom ServingRuntimes
-- Model explainability and responsible AI patterns
-- Automation of AI Factory via API-driven workflows
-- Advanced observability and performance tuning
-
-[Start the 301 Path →](./301)
-
----
-
-## Recommended Training Courses
-
-To complement these self-paced Learning Paths, we also offer:
-
-### Instructor-Led Training
-- Advanced AI Factory Architectures
-- AI Factory Administration & Operations
-- Custom Model Development & Deployment with AI Factory
-
-### Self-Paced Training
-- Introduction to AI Factory
-- Building AI Assistants with AI Factory
-- Managing AI Models and Hybrid Knowledge Bases
-- Scaling AI Workloads with AI Factory
-
-*For more details, see the [Training Overview](../../training/index) and [Instructor-Led Training Catalog](../../training/instructor-led) (coming soon).*
-
----
-
-## Where to next?
-
-- [AI Factory Concepts](../explained/ai-factory-concepts)
-- [How-To Guides](../how-to/gen-ai/create-assistant)
-- [Model Serving Quickstart](../how-to/model-serving/index)
-- [Tutorials](../tutorials)
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/model/library/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/model/library/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/model/library/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/model/library/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,107 +0,0 @@
----
-title: Model Library (AI Factory)
-navTitle: Model Library
-description: Discover, manage, and deploy AI models in AI Factory using the Model Library, powered by the Image and Model Library in Hybrid Control Plane.
----
-
-# Model Library
-
-The **Model Library** is your central resource in AI Factory for discovering, managing, and deploying AI models.
-It provides the foundation for Model Serving across the AI Factory ecosystem.
-
-The Model Library is **powered by the Image and Model Library** in Hybrid Control Plane (HCP), which offers unified governance of all container images — both database images and AI model images — used across your HCP environment.
-
-→ [Learn more about the Image and Model Library](/image-and-model-library).
-
----
-
-## What does the Model Library provide?
-
-- **Discover containerized AI models** ready for deployment via Model Serving.
-- **Select optimized GPU models** to match your use case and hardware.
-- **Integrate with Knowledge Bases** to drive vector search and RAG applications.
-- **Support Gen AI Builder** → Assistants and Retrievers use these models for inference.
-- **Leverage private registries** → Bring your own models and manage their lifecycle.
-
----
-
-## Where does the Model Library fit in AI Factory?
-
-The Model Library powers Model Serving for AI Factory, supporting:
-
-- **AIDB Knowledge Bases** → embedding models, rerankers, OCR.
-- **Gen AI Builder** → chat completion models with tool calling, retrievers.
-- **Custom AI Factory workflows** → any Griptape pipeline or agent using Model Serving.
-
-Typical usage flow:
-
-1. **Discover model images** → via Model Library (powered by Image Library).
-2. **Deploy models** → to Model Serving via KServe.
-3. **Consume models** → in Knowledge Bases, Assistants, and AI Factory pipelines.
-
----
-
-## Core Concepts
-
-- **Repository** → AI model family (ex: `meta/llama-3.3-49b-nemotron-super`).
-- **Image Tag** → Model version (ex: `1.8.5`, `latest`).
-- **GPU Requirements** → Defined per model; informs node sizing.
-- **Serving Runtime** → Managed via KServe integration.
-
-→ [KServe Concepts](../../../learn/explained/kserve-explained) (placeholder)
-
----
-
-## Guides and How-Tos
-
-### Deploying AI models
-
-- [Deploy AI models using Library images](../../../how-to/gen-ai/image-library/deploy-ai-models)
-- [Setup GPU resources for Model Serving](../../../how-to/gen-ai/model-serving/setup-gpu)
-
-### Managing registries and repositories
-
-- [Integrate private registry for custom models](../../../how-to/gen-ai/image-library/integrate-private-registry)
-- [Define repository rules for Model Library](../../../how-to/gen-ai/image-library/define-repository-rules)
-
-### Running advanced AI workloads
-
-- [GPU Model Serving Runbook](../../../tutorials/gen-ai/gpu-serving-runbook)
-- [Provision GPU nodes for Model Serving](../../../how-to/gen-ai/model-serving/setup-gpu)
-
----
-
-## Key Models and Usage Patterns
-
-In HCP 1.2, these model types are validated and recommended:
-
-| Model Type        | Example Model Image                                   | Used by |
-|-------------------|-----------------------------------------------------|---------|
-| **Chat Completion** | `llama-3.3-nemotron-super-49b`                       | Gen AI Builder |
-| **Text Embedding**  | `arctic-embed-l`                                     | AIDB Knowledge Base, Gen AI Builder |
-| **Text Reranking**  | `llama-3.2-nv-rerankqa-1b-v2`                        | AIDB Knowledge Base, Gen AI Builder |
-| **Image Embedding** | `nvclip`                                            | AIDB Knowledge Base |
-| **OCR**            | `paddleocr`                                         | AIDB Knowledge Base |
-
-→ See full details in [Deploy AI models using Library images](../../../how-to/gen-ai/image-library/deploy-ai-models).
-→ Recommended node sizing in [GPU Model Serving Runbook](../../../tutorials/gen-ai/gpu-serving-runbook).
-
----
-
-## Related Reading
-
-- [AI Factory Overview](../../../learn/explained/ai-factory-concepts)
-- [GPU Configuration and Runbook](../../../tutorials/gen-ai/gpu-serving-runbook)
-- [Image and Model Library (HCP Hub)](/image-and-model-library)
-
----
-
-## Future Topics (Coming Soon)
-
-- Advanced serving workflows with multiple runtimes.
-- Using vLLM for model files (non-container serving).
-- Supporting hybrid deployment patterns (vLLM + container serving).
-- Automating model promotion pipelines via CI/CD.
-
----
-
Index: advocacy_docs/edb-postgres-ai/ai-factory/model/serving/deployment.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/deployment.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/deployment.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/deployment.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,79 +0,0 @@
----
-title: How Model Serving Deployment Works
-navTitle: Deployment
-description: Understand how model deployment works in AI Factory and how to deploy your models as scalable inference services.
----
-
-# How Model Serving Deployment Works
-
-AI Factory makes it easy to deploy your AI models as scalable, production-ready inference services. The platform uses **KServe** as the model serving engine, operating within the Hybrid Manager (HCP) Kubernetes infrastructure.
-
-This page explains the general flow of model deployment and links to key how-to guides for hands-on instructions.
-
----
-
-## Deployment flow overview
-
-- You deploy models by creating **KServe InferenceServices** in your HCP project.
-- AI Factory provides **GPU-enabled Kubernetes infrastructure** to run these services.
-- You can deploy supported **NVIDIA NIM containers** or other compatible models.
-- The **Model Library** helps you discover and manage model images.
-- Applications access model endpoints over **HTTP** or **gRPC** APIs.
-
----
-
-## Deployment components
-
-### KServe InferenceService
-
-Each model is deployed via a Kubernetes-native `InferenceService` object:
-
-- Manages lifecycle of the model server pods.
-- Handles scaling, health checks, and routing.
-- Exposes a network endpoint for model consumption.
-
-### ClusterServingRuntime
-
-Advanced users can also configure `ClusterServingRuntime` resources to customize runtime environments for their models.
-
----
-
-## Where to start
-
-If you're ready to deploy models, follow these guides:
-
-- [Deploy NIM Containers](../../../learn/how-to/model-serving/deploy-nim-container)
-- [Create InferenceService](../../../learn/how-to/model-serving/create-inferenceservice)
-- [Configure ServingRuntime](../../../learn/how-to/model-serving/configure-servingruntime)
-- [Monitor InferenceService](../../../learn/how-to/model-serving/monitor-inferenceservice)
-- [Update GPU Resources](../../../learn/how-to/model-serving/update-gpu-resources)
-
----
-
-## Hybrid Manager integration
-
-Model Serving runs on **Hybrid Manager (HCP)** Kubernetes clusters. For more on Hybrid Manager and GPU setup:
-
-- [Setting Up GPU Resources in Hybrid Manager](../../../hybrid-manager/ai-factory/learn/how-to/model-serving/setup-gpu)
-- [Verifying Model Deployment in Hybrid Manager](../../../hybrid-manager/ai-factory/learn/how-to/model-serving/verify-models)
-
----
-
-## Best practices
-
-- Use the Model Library to select supported models.
-- Verify that your cluster has sufficient GPU resources.
-- Monitor deployed models to ensure performance and availability.
-- Use `ClusterServingRuntime` where advanced customization is needed.
-
----
-
-## Next steps
-
-- Explore our [Model Serving How-To Guides](../../../learn/how-to/model-serving/index)
-- Review supported models in the [Supported Models Index](../../../models/supported-models/index)
-- Learn about [Observability for Model Serving](./observability)
-
----
-
-By following this deployment flow, you can run AI models in production with full observability and scale — directly integrated with the broader AI Factory and Hybrid Manager ecosystem.
Index: advocacy_docs/edb-postgres-ai/ai-factory/model/serving/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/model/serving/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,60 +0,0 @@
----
-title: Model Serving
-navTitle: Model Serving
-description: Explore the Model Serving capability in AI Factory — enabling production deployment of supported models as scalable inference services.
----
-
-Use the Model Serving capability of AI Factory to deploy models as network-accessible inference services using a standardized Kubernetes-native serving engine.
-
-AI Factory uses KServe as its model serving engine. In AI Factory version 1.2, Model Serving is used for deploying NVIDIA NIM containers. While KServe supports a broad range of model formats, current AI Factory support focuses on providing a streamlined and integrated deployment experience for NIM-based models.
-
-KServe operates within the managed Hybrid Manager (HCP) Kubernetes infrastructure provided by AI Factory, giving you access to scalable, GPU-accelerated model serving.
-
-For a deeper understanding of the model serving architecture, see [KServe in AI Factory concepts](../learn/explained-ai-factory-concepts#model-serving-kserve).
-
----
-
-## How Model Serving works
-
-- Models are deployed as KServe InferenceServices within your HCP project.
-- AI Factory provides GPU-enabled Kubernetes infrastructure to run these services.
-- The HCP Model Library helps you discover and manage supported models.
-- Your applications can consume model endpoints using standard HTTP or gRPC APIs.
-
----
-
-## Where to start
-
-- [Model Serving Quickstart](../learn/model-serving/quickstart) — get started fast
-- [Model Serving Learning Paths](../learn/paths/index) — structured learning journeys
-- [KServe in AI Factory concepts](../learn/explained-ai-factory-concepts#model-serving-kserve)
-
----
-
-## Key topics
-
-### Learning and implementation
-
-- [Model Serving Learning Paths](../learn/paths/index)
-- [Model Serving How-To Guides](../learn/model-serving/index)
-
-### Deployment
-
-- [How Model Serving deployment works](./deployment)
-- [Deploy NIM Containers](../../../learn/how-to/model-serving/deploy-nim-container)
-
-### Model list
-
-- [Supported models index](../../models/supported-models/index)
-
-### Monitoring and observability
-
-- [Observability for model serving](./observability)
-
-### FAQ
-
-- [Model Serving FAQ](./faq)
-
----
-
-For product-specific guidance on using Model Serving within Hybrid Manager (HCP), see the [Hybrid Manager Model Serving spoke](../hybrid-manager/ai-factory/model-serving/index).
Index: advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/getting-started.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/getting-started.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/getting-started.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/getting-started.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,108 +0,0 @@
----
-title: Model Serving Quickstart
-navTitle: Quickstart
-description: Get started with Model Serving in AI Factory. This quickstart provides a guided path for new and experienced users with links to essential resources.
----
-
-# Model Serving Quickstart
-
-This page helps you quickly understand how to start using **Model Serving** within the AI Factory and where to find supporting documentation.
-
-Model Serving in AI Factory enables you to deploy AI models (such as NVIDIA NIM containers) as scalable, production-grade inference services. It is powered by **KServe**, a Kubernetes-native model serving engine.
-
-## Where to start
-
-### 1. Learn the concepts
-
-Before deploying models, it's useful to understand how Model Serving works and how it fits into the AI Factory ecosystem:
-
-- [Model Serving Concepts](../learn/explained/ai-factory-concepts#model-serving)
-- [Model Serving Terminology](../learn/explained/terminology)
-
-### 2. Understand how AI Factory integrates Model Serving
-
-Model Serving interacts with:
-
-- **Model Library**: Browse and manage model images for deployment (coming soon)
-- **Knowledge Bases (AIDB)**: Vector stores that may use embedding models served by Model Serving
-- **Gen AI Builder**: Applications may call into Model Serving endpoints for inferencing
-
-### 3. Follow the How-To Guides
-
-If you're ready to deploy or manage models:
-
-- [How-To Guides: Model Serving](../learn/model-serving/)
-
----
-
-## Getting started checklist
-
-Use this checklist to guide your progress depending on your experience level.
-
-### For new users (101 level)
-
-* Read the [Model Serving Concepts](../learn/explained/ai-factory-concepts#model-serving)
-* Review key [Model Serving Terminology](../learn/explained/terminology)
-* Understand [What KServe is](../learn/explained/ai-factory-concepts#model-serving) and how it powers Model Serving
-* Understand [How Model Library relates to Model Serving](../model-library) (coming soon)
-
-[Follow Learning Path 101 for Model Serving](../learn/paths/model-serving/101)
-
----
-
-### For existing users familiar with Kubernetes (101 level)
-
-* Verify your Kubernetes access in your HCP project
-* Review the [Concepts](../learn/explained/ai-factory-concepts#model-serving) and [Terminology](../learn/explained/terminology)
-* Prepare your cluster prerequisites:
-- GPU node pools (if needed)
-- NVIDIA device plugin (if needed)
-- Access to your container registry for model images
-* Configure basic KServe resources:
-- [Deploy NVIDIA NIM Container](../learn/model-serving/deploy-nim-container)
-- [Configure ClusterServingRuntime](../learn/model-serving/configure-clusterservingruntime)
-- [Create InferenceService](../learn/model-serving/create-inferenceservice)
-
-[Follow Learning Path 101 for Model Serving](../learn/paths/model-serving/101)
-
----
-
-### For advanced users (201 level)
-
-* Tune deployed InferenceService resource usage:
-- [Update GPU Resources](../learn/model-serving/update-gpu-resources)
-
-* Monitor deployed models:
-- [List deployed InferenceServices](../learn/model-serving/list-inferenceservices)
-- [Monitor KServe deployments](../learn/model-serving/monitor-kserve-deployments) (coming soon)
-
-* Understand traffic routing, canary rollouts, and scaling:
-- [Model serving scaling patterns](../learn/explained/ai-factory-concepts#model-serving)
-- Future: Advanced How-Tos
-
-[Follow Learning Path 201 for Model Serving](../learn/paths/model-serving/201)
-
----
-
-### For expert users (301 level)
-
-* Manage your own custom model images
-* Build and configure custom ServingRuntime definitions
-* Use Transformers and Explainers in KServe (coming soon)
-* Build CI/CD pipelines for deploying models in KServe
-* Instrument InferenceServices for advanced observability
-
-[Follow Learning Path 301 for Model Serving](../learn/paths/model-serving/301)
-
----
-
-## Next steps
-
-- [Model Serving Concepts](../learn/explained/ai-factory-concepts#model-serving)
-- [Model Serving How-To Guides](../learn/model-serving/)
-- [Model Library (future hub)](../model-library)
-
----
-
-Use this quickstart as your launch point into Model Serving within AI Factory.
-
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/ai-factory/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/ai-factory/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/ai-factory/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/ai-factory/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,49 +0,0 @@
----
-title: AI Factory in Hybrid Manager
-navTitle: AI Factory
-description: How AI Factory features are implemented and used within Hybrid Manager, with links to deep conceptual content and Hybrid Manager-specific guidance.
----
-
-This section explains how key **AI Factory** capabilities work in the context of **Hybrid Manager**.
-
-Hybrid Manager delivers AI Factory as a managed **AI workload** within your Hybrid Manager (HCP) project.
-This allows you to run **agentic AI applications**, deploy and serve **AI models**, and integrate **vector and knowledge-based search** — all with Kubernetes-native control and security.
-
----
-
-## What this section covers
-
-This section provides **Hybrid Manager-specific explanations** of AI Factory capabilities:
-
-- How they run within HCP project Kubernetes clusters
-- How they interact with other AI Factory features (Model Serving, Knowledge Bases, Agent Studio)
-- How they interact with **Hybrid Manager features** (image management, GPU management, observability, user and project scopes)
-- Where to find deep conceptual content in the **AI Factory Hub**
-
----
-
-## Topics
-
-- [Gen AI in Hybrid Manager](../../../../gen-ai/index)
-- [Agent Studio in Hybrid Manager](../../../../gen-ai/agent-studio)
-- [Gen AI Builder in Hybrid Manager](../../../../gen-ai/builder)
-- [Model Serving in Hybrid Manager](../../../../model/serving)
-- [Model Library in Hybrid Manager](../../../../model/library)
-- [Vector Engine & Knowledge Bases in Hybrid Manager](../../pipeline/knowledge-base)
-
----
-
-## Learn more
-
-For full conceptual explanations of each AI Factory capability, see the [AI Factory Hub](/ai-factory/learn/explained/index).
-
-For feature-specific **How-To Guides** within Hybrid Manager:
-
-- [Gen AI How-To Guides](../../how-to/gen-ai/)
-- [Model Serving How-To Guides](../../how-to/model-serving/)
-- [Model Library How-To Guides](../../how-to/model-library/)
-
----
-
-By combining **AI Factory** with **Hybrid Manager’s Kubernetes-native controls and GPU resources**, you can deliver highly integrated and production-ready AI applications.
-
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/container-registry-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/container-registry-explained.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/container-registry-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/container-registry-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,129 +0,0 @@
----
-title: Container Registry Integration
-navTitle: Container Registry Integration
-description: Understand how the Hybrid Control Plane Image and Model Library integrates with container registries to provide database and AI model images.
----
-
-# Container Registry Integration
-
-The Image and Model Library in Hybrid Control Plane (HCP) is powered by container registries.
-Registries provide the actual image content for:
-
-- **Database images** (PostgreSQL, EPAS, PGE)
-- **AI model images** (NVIDIA NIM, future custom models)
-
-Understanding how container registry integration works is key to managing and extending your Image Library.
-
----
-
-## How does it work?
-
-HCP relies on the **Beacon Agent** to pull image metadata from connected registries.
-
-The Beacon Agent uses **Repository Rules** to know:
-
-- Which registries to connect to
-- Which repositories to scan
-- Which image tags to import
-
-→ The Image Library UI in HCP is a reflection of what the agent has pulled.
-
----
-
-## Key Concepts
-
-| Concept | Meaning |
-|---------|---------|
-| **Registry** | A container registry (ex: Azure ACR, AWS ECR, GCP GCR, private registry) |
-| **Repository** | A collection of images for a given software or model (ex: `postgresql`, `meta/llama-3.3-49b`) |
-| **Tag** | A version of an image (ex: `17.4-2503310734`, `1.8.5`, `latest`) |
-| **Repository Rule** | A rule that tells Beacon Agent which repositories/tags to discover |
-| **HCP Tags** | Organizational tags within HCP → these are separate from registry tags |
-
----
-
-## Default registries
-
-In ephemeral HCP environments:
-
-- Default registry is `upmdev.azurecr.io` (Azure ACR).
-- Repositories:
-- `postgresql` → PostgreSQL community images.
-- `edb-postgres-advanced` → EPAS images.
-- `edb-postgres-extended` → PGE images.
-
-In customer environments:
-
-- Customer provides a **private registry**.
-- Image Library is configured to pull from that registry.
-
----
-
-## Repository Rules
-
-Repository Rules drive what Beacon Agent discovers:
-
-- You can add **custom repositories** via Image Library UI → "Manage Repositories."
-- You can edit the Beacon Agent config for advanced control.
-
-Example agent config section:
-
-```yaml
-dispatcher:
-  capabilities:
-    provisioning:
-      images:
-        discovery:
-          registry:
-            postgres_repositories:
-              - name: postgresql
-                description: Community PostgreSQL
-              - name: edb-postgres-advanced
-                description: EPAS
-              - name: edb-postgres-extended
-                description: PGE
-              - name: postgresql-torsten
-                description: Custom PG Build with AIDB
-```
-This tells agent which repositories to pull and display.
-
-→ You can edit this config to add more repositories (advanced).
-
-Private Registry Integration Flow
-Add repository via Image Library UI → simplest flow for most users.
-
-Agent pulls image list periodically.
-
-Image tags and metadata appear in Image Library.
-
-You can select these images for:
-
-Database provisioning
-
-AI model serving (via Model Library in AI Factory).
-
-Relationship to AI Factory Model Library
-Model Library in AI Factory is powered by Image Library.
-
-All AI model images flow through this same registry → agent pull → Image Library → available for Model Serving.
-
-This ensures consistent governance of all model images.
-
-→ See: AI Factory Model Serving Explained
-Summary
-* Registries provide the source of truth for images.
-* Repository Rules tell HCP which repos/tags to scan.
-* Beacon Agent pulls the images and updates Image Library UI.
-* Private registries can be fully integrated and governed.
-* AI Factory Model Library leverages this same system.
-
-
-Related How-Tos
-Integrate Private Registry
-
-Define Repository Rules
-
-Related Explained Pages
-Image and Model Library Explained
-
-AI Factory Model Serving Explained
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/model-image-library-explained.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/model-image-library-explained.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/model-image-library-explained.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/explained/model-image-library/model-image-library-explained.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,126 +0,0 @@
----
-title: Image and Model Library Explained
-navTitle: Image and Model Library Explained
-description: Understand the Image and Model Library in HCP, powering both database images and AI model images through a unified architecture.
----
-
-# Image and Model Library Explained
-
-The **Image and Model Library** is a core capability of the Hybrid Control Plane (HCP).
-It provides a unified, governed repository for managing container images across your entire HCP environment.
-
-This includes:
-
-- **Database images** → for PostgreSQL cluster provisioning.
-- **AI model images** → for Model Serving (via AI Factory Model Library).
-
-## Why a single library?
-
-Using a unified Image and Model Library ensures:
-
-- **Governance** → all images (DB or AI model) pass through the same approval flow.
-- **Consistency** → the same agent-driven sync pulls images from container registries.
-- **Security** → private registry integration applies equally to all image types.
-- **Flexibility** → both database and AI teams can manage images within a common framework.
-
----
-
-## What is the difference between Image Library and Model Library?
-
-| Capability | Image and Model Library (Hub) | Model Library (AI Factory) |
-|------------|-------------------------------|----------------------------|
-| Scope | All container images used in HCP | Subset: AI model images used for Model Serving |
-| Primary Users | DB admins, platform teams, AI teams | AI Factory users, AI engineers |
-| Backed by Container Registry? | Yes | Yes (via Image Library) |
-| Supports Private Registries? | Yes | Yes (via Image Library) |
-| Source of Truth | Container Registry → Image Library | Filtered view of Image Library |
-| Usage | Postgres cluster provisioning, AI model serving | AI model serving only |
-| UI Location | Image Library (HCP Portal) | Model Library (AI Factory UI) |
-
-**Summary:**
-→ The Model Library is powered by Image Library.
-→ All images pass through Image Library first → Model Library presents a user-friendly view of AI model images.
-
----
-
-## Architecture flow
-
-```
-Container Registry → Image and Model Library →
-→ Postgres Provisioning / AI Model Serving
-→ Model Library UI → Model Serving
-```
-
-* Postgres provisioning consumes database images from Image Library.
-
-* AI Factory Model Library consumes model images → curated from Image Library.
-
-
-
-
-Key Concepts
-Concept	Meaning
-Repository	A collection of images (ex: postgresql, meta/llama-3.3-49b)
-Tag	A version of an image (ex: 17.4-2503310734, 1.8.5, latest)
-HCP Tags	Organizational tags applied to image repos in HCP
-Repository Rules	Define which repos/tags to sync from connected registries
-Private Registry	Any customer-provided container registry integrated into HCP
-Beacon Agent	Agent that pulls image metadata into Image Library
-
-Use Cases
-Database images
-Provision new Postgres clusters using curated database images.
-
-Support for:
-
-PostgreSQL Community.
-
-EDB Postgres Advanced Server (EPAS).
-
-EDB Postgres Extended Server (PGE).
-
-Manage extensions via OCI annotations.
-
-AI model images
-Deploy AI models using AI Factory → Model Serving (powered by KServe).
-
-Support for:
-
-NVIDIA NIM model images.
-
-Future: customer-provided model images.
-
-Model Library surfaces a curated view of AI model images from Image Library.
-
-Governance flow
-Connect container registries → private or public.
-
-Define Repository Rules → which repos/tags to sync.
-
-Beacon Agent pulls images → Image Library populated.
-
-Image Library available to:
-
-Postgres Provisioning flows.
-
-Model Library (for AI model serving).
-
-Model Library filters Image Library → only AI models shown.
-
-Benefits
-* Single governance model for all images.
-* No bypass of security or compliance policies.
-* Private registry integration → works equally for database and AI model images.
-* Teams can manage their own repositories but operate within platform-wide visibility.
-
-Related Explained Pages
-Container Registry Integration Explained
-
-AI Factory Model Serving Explained (Model Library)
-
-Related How-Tos
-- Integrate Private Registry
-- Define Repository Rules
-- Find and Use Images
-- Deploy AI Models
-- Provision a Postgres Cluster with Custom Image
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,51 +0,0 @@
----
-title: AI Factory How-To Guides (Hybrid Manager)
-navTitle: AI Factory How-To
-description: How-To Guides for using AI Factory features within Hybrid Manager projects — focused on deployment, configuration, and operations.
----
-
-Use these **How-To Guides** to deploy and operate **AI Factory** capabilities inside your **Hybrid Manager (HCP)** environment.
-
-Hybrid Manager runs AI Factory as an integrated AI workload inside your HCP Kubernetes project.
-While many core concepts are shared with the [AI Factory Hub](/ai-factory/learn/explained/index), there are important **Hybrid Manager-specific patterns and steps** to follow when:
-
-- Setting up Model Serving infrastructure
-- Deploying and verifying AI models (InferenceServices)
-- Managing GPU resources for model serving
-- Using AI models in Gen AI applications and database extensions
-- Managing project- and user-scope permissions for AI Factory
-
----
-
-## Topics
-
-### Gen AI
-
-- [Set up and manage Hybrid Manager GPU resources](../model-serving/setup-gpu)
-- [Verify deployed models and GPU usage](../model-serving/verify-models)
-
-### Model Serving
-
-- [Deploy NVIDIA NIM containers](../model-serving/deploy-nim-container)
-- [Monitor model serving deployments](../model-serving/monitor-inferenceservice)
-- [Update GPU resources](../model-serving/update-gpu-resources)
-
-### Model Library
-
-- [Integrate private container registry](../model-library/integrate-private-registry)
-- [Define repository rules](../model-library/define-repository-rules)
-- [Manage repository metadata](../model-library/manage-repository-metadata)
-- [Deploy AI models from Model Library](../model-library/how-to-deploy-ai-models)
-
----
-
-## Learn more
-
-For conceptual background, see:
-
-- [AI Factory in Hybrid Manager](../../explained/ai-factory/index)
-- [AI Factory Hub — Full Concepts](../../../../ai-factory/learn/explained/index)
-
----
-
-For broader **Hybrid Manager How-To Guides**, visit the [Hybrid Manager Learn How-To](../../how-to/index).
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/setup-gpu.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/setup-gpu.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/setup-gpu.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/setup-gpu.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,113 +0,0 @@
-
----
-title: Setup GPU resources in Hybrid Manager
-navTitle: Setup GPU resources
-description: How to provision and configure GPU resources in Hybrid Manager to support Model Serving.
----
-
-# Setup GPU resources in Hybrid Manager
-
-Use this guide to prepare GPU resources in your Hybrid Manager (HCP) Kubernetes cluster to support Model Serving with KServe.
-
-You provision GPU-enabled nodes, configure them for KServe, and store required secrets for deploying NVIDIA NIM models.
-
-## Goal
-
-Prepare your HCP cluster to run GPU-based Model Serving workloads using KServe.
-
-## Estimated time
-
-20–40 minutes (provisioning might take longer depending on your cloud provider).
-
-## What you accomplish
-
-- Provision GPU node groups or node pools in your HCP cluster.
-- Label and taint GPU nodes correctly.
-- Deploy the NVIDIA device plugin DaemonSet.
-- Store your NVIDIA API key as a Kubernetes secret.
-- Enable your cluster to run NIM model containers in KServe.
-
-## What this unlocks
-
-After you complete this procedure, you can deploy supported GPU-accelerated models through Model Serving:
-
-- For AIDB Knowledge Bases.
-- For GenAI Builder assistants.
-- For custom model-based applications.
-
-## Prerequisites
-
-- Access to an HCP Kubernetes cluster with appropriate permissions.
-- Administrative access to provision node groups (AWS EKS / GCP GKE / RHOS).
-- NVIDIA API key for accessing NIM models.
-- Familiarity with basic `kubectl` usage.
-
-## Provision GPU nodes
-
-Provision GPU node groups (EKS) or node pools (GKE / RHOS) in your HCP cluster:
-
-- Use instances with L40S or A100 GPUs (for example, `g6e.12xlarge` on AWS or `a2-highgpu-4g` on GCP).
-- Recommended: Provision at least one node with four GPUs to support large models such as Llama 70B.
-
-See [HCP validated GPU configurations](../ai-factory/model-serving/gpu).
-
-## Label and taint GPU nodes
-
-Apply the following Kubernetes label and taint to GPU nodes.
-
-Label:
-
-```shell
-kubectl label node <gpu-node-name> nvidia.com/gpu=true
-````
-
-Taint:
-
-```shell
-kubectl taint nodes <gpu-node-name> nvidia.com/gpu=true:NoSchedule
-```
-
-This ensures that KServe model pods are scheduled correctly and that Postgres clusters do not land on GPU nodes.
-
-## Deploy the NVIDIA device plugin
-
-Deploy the NVIDIA device plugin DaemonSet.
-
-```shell
-kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml
-```
-
-Verify that the plugin is running.
-
-```shell
-kubectl get ds -n kube-system nvidia-device-plugin-daemonset
-```
-
-The plugin exposes GPU resources to Kubernetes and KServe.
-
-## Store NVIDIA API key as Kubernetes secret
-
-Generate an NVIDIA API key from the NGC Catalog portal.
-
-Create a Kubernetes secret.
-
-```shell
-kubectl create secret generic nvidia-nim-secrets --from-literal=NGC_API_KEY=<your_NVIDIA_API_KEY>
-```
-
-This secret is required when deploying `ClusterServingRuntime` resources for NIM models.
-
-## Related concepts
-
-* [GPUs in Model Serving](../ai-factory/model-serving/gpu)
-* [KServe in AI Factory concepts](../ai-factory/model-serving/index)
-* [AI Factory Learning Paths](../ai-factory/index)
-
-## Next steps
-
-* [Deploy NIM containers](../ai-factory/learn/how-to/model-serving/deploying-nim.mdx)
-* [Verify InferenceServices and GPU usage in Hybrid Manager](./verifying-models.mdx)
-* [Learn about GPU sizing and model requirements](../ai-factory/model-serving/supported-models)
-
-```
-```
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/verify-models.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/verify-models.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/verify-models.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/how-to/ai-factory/verify-models.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,87 +0,0 @@
----
-title: Verify InferenceServices and GPU Usage in Hybrid Manager
-navTitle: Verify InferenceServices and GPU Usage
-description: How to verify InferenceServices deployments and GPU resource usage in Hybrid Manager.
----
-
-# Verify InferenceServices and GPU Usage in Hybrid Manager
-
-Use this guide to confirm the correct deployment and operational status of InferenceServices and GPU resource usage within your Hybrid Manager (HCP) Kubernetes cluster.
-
-## Goal
-
-Ensure your deployed InferenceServices are correctly utilizing GPU resources.
-
-## Estimated time
-
-15–20 minutes.
-
-## What you accomplish
-
-- Verify the status of deployed InferenceServices.
-- Confirm GPU resource allocation and utilization.
-- Troubleshoot common deployment and GPU-related issues.
-
-## Prerequisites
-
-- Completed setup and deployment of GPU resources and NIM containers in Hybrid Manager.
-- Access to HCP Kubernetes cluster with appropriate permissions.
-- Familiarity with basic `kubectl` usage.
-
-## Verify InferenceServices Status
-
-Check the status of your deployed InferenceServices to confirm they are operational.
-
-```shell
-kubectl get inferenceservice -n <namespace>
-````
-
-Look for the `READY` status to ensure your service is successfully running.
-
-## Confirm GPU Resource Usage
-
-Check GPU resource allocation and usage on nodes.
-
-```shell
-kubectl describe nodes | grep nvidia.com/gpu
-```
-
-Review the output to verify GPU availability and allocation.
-
-Use `nvidia-smi` from within your GPU-enabled pods to check real-time GPU utilization.
-
-```shell
-kubectl exec -n <namespace> -it <pod-name> -- nvidia-smi
-```
-
-## Troubleshoot Common Issues
-
-If the InferenceService is not ready or GPU resources are not properly allocated:
-
-* Confirm the NVIDIA device plugin DaemonSet is running:
-
-```shell
-kubectl get ds -n kube-system nvidia-device-plugin-daemonset
-```
-
-* Check for resource constraints or scheduling issues using:
-
-```shell
-kubectl describe pods -n <namespace>
-```
-
-Address any errors or issues reported by these commands.
-
-## Related concepts
-
-* [Setup GPU resources in Hybrid Manager](../setup-gpu.mdx)
-* [Deploy NIM containers](./deploying-nim.mdx)
-* [KServe in AI Factory concepts](../ai-factory/model-serving/index)
-
-## Next steps
-
-* Optimize GPU resource utilization and scaling.
-* Monitor model performance and health.
-
-```
-```
Index: advocacy_docs/edb-postgres-ai/hybrid-manager/learn/paths/ai-factory/index.mdx
===================================================================
diff --git a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/paths/ai-factory/index.mdx b/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/paths/ai-factory/index.mdx
deleted file mode 100644
--- a/advocacy_docs/edb-postgres-ai/hybrid-manager/learn/paths/ai-factory/index.mdx	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
+++ /dev/null	(revision bfb5e7e771eb9cfd2db55b5a8d5f364ab05f32ed)
@@ -1,73 +0,0 @@
----
-title: AI Factory Learning Paths (Hybrid Manager)
-navTitle: AI Factory Learning Paths
-description: Structured learning paths to help you deploy and operate AI Factory features within Hybrid Manager.
----
-
-Follow these **Learning Paths** to gain practical skills for running **AI Factory** capabilities inside **Hybrid Manager** projects.
-
----
-
-## Getting Started
-
-### AI Factory 101 (Hybrid Manager)
-
-- Understand AI Factory architecture in HCP
-- Understand how AI Factory capabilities are deployed in Kubernetes
-- Explore Model Library and Model Serving in Hybrid Manager
-- Explore Gen AI Builder in Hybrid Manager
-
-Start with:
-
-- [AI Factory in Hybrid Manager Explained](../../explained/ai-factory/index)
-- [Model Library How-To Guides](../model-library/index)
-- [Model Serving How-To Guides](../model-serving/index)
-
----
-
-## Intermediate
-
-### AI Factory 201 (Hybrid Manager)
-
-- Deploy custom AI models via Model Library
-- Manage GPU resources for AI model serving
-- Deploy InferenceServices with advanced configuration
-- Monitor and troubleshoot model serving
-- Connect Gen AI applications to served models
-
-Suggested guides:
-
-- [Integrate Private Container Registry](../model-library/integrate-private-registry)
-- [Deploy NVIDIA NIM Containers](../model-serving/deploy-nim-container)
-- [Monitor InferenceServices](../model-serving/monitor-inferenceservice)
-
----
-
-## Advanced
-
-### AI Factory 301 (Hybrid Manager)
-
-- Optimize GPU usage across AI workloads
-- Implement multi-model serving patterns
-- Use model explainability tools (future)
-- Integrate model endpoints with Hybrid Manager Gen AI pipelines
-- Manage project- and user-scope for AI Factory features
-
-Continue with:
-
-- [Update GPU Resources](../model-serving/update-gpu-resources)
-- [Verify Models and GPU Usage](../model-serving/verify-models)
-- [Advanced Model Deployment Patterns](../../explained/ai-factory/index) *(coming soon)*
-
----
-
-## Learn more
-
-For full conceptual background:
-
-- [AI Factory in Hybrid Manager](../../explained/ai-factory/index)
-- [AI Factory Hub — Concepts](../../../../ai-factory/learn/explained/index)
-
-For **Training** options:
-
-- Instructor-led and self-paced AI Factory courses *(see EDB Training Catalog)*.
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/capabilities/auto-processing.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/capabilities/auto-processing.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/capabilities/auto-processing.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/capabilities/auto-processing.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/capabilities/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/capabilities/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/capabilities/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/capabilities/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/gettingstarted/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/gettingstarted/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/gettingstarted/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/gettingstarted/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/complete.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/installing/complete.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/complete.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/installing/complete.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/installing/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/installing/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/packages.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/installing/packages.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/packages.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/installing/packages.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/upgrading.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/installing/upgrading.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/installing/upgrading.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/installing/upgrading.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/concepts.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/concepts.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/concepts.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/concepts.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/example.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/example.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/example.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/example.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/performance_tuning.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/performance_tuning.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/performance_tuning.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/performance_tuning.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/usage.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/usage.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/usage.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/usage.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/usage_auto_processing.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/usage_auto_processing.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/knowledge_base/usage_auto_processing.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/knowledge_base/usage_auto_processing.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/bert.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/bert.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/bert.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/bert.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/clip.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/clip.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/clip.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/clip.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/completions.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/completions.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/completions.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/completions.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/embeddings.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/embeddings.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/embeddings.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/embeddings.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_clip.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_clip.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_clip.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_clip.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_paddle_ocr.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_paddle_ocr.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_paddle_ocr.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_paddle_ocr.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_reranking.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_reranking.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/nim_reranking.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/nim_reranking.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/t5.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/t5.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/supported-models/t5.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/supported-models/t5.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/images/get-api-key.svg b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/images/get-api-key.svg
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/images/get-api-key.svg
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/images/get-api-key.svg
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/using-nim-in-nvidia-cloud.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/using-nim-in-nvidia-cloud.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/using-nim-in-nvidia-cloud.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/using-nim-in-nvidia-cloud.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/using-nim-in-your-environment.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/using-nim-in-your-environment.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/using-nvidia-nim/using-nim-in-your-environment.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/using-nvidia-nim/using-nim-in-your-environment.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/openai-api-compatibility.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/openai-api-compatibility.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-with/openai-api-compatibility.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-with/openai-api-compatibility.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/primitives.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/primitives.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/primitives.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/primitives.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-models.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-models.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/models/using-models.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/models/using-models.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/gcs.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/gcs.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/gcs.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/gcs.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/local.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/local.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/local.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/local.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/s3.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/s3.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/functions/s3.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/functions/s3.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/settings.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/settings.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pgfs/settings.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgfs/settings.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/vector-engine/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pgvector/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/vector-engine/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pgvector/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chained_preparers.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chained_preparers.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chained_preparers.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chained_preparers.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chunk_text.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chunk_text.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chunk_text.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chunk_text.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chunk_text_auto_processing.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chunk_text_auto_processing.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/chunk_text_auto_processing.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/chunk_text_auto_processing.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/parse_html.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/parse_html.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/parse_html.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/parse_html.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/parse_pdf.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/parse_pdf.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/parse_pdf.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/parse_pdf.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/perform_ocr.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/perform_ocr.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/perform_ocr.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/perform_ocr.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/summarize_text.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/summarize_text.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/examples/summarize_text.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/examples/summarize_text.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/concepts.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/concepts.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/concepts.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/concepts.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/primitives.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/primitives.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/primitives.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/primitives.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/usage.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/usage.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/usage.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/usage.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/volume_destination.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/volume_destination.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/preparers/volume_destination.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/preparers/volume_destination.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/knowledge_bases.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/knowledge_bases.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/knowledge_bases.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/knowledge_bases.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/models.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/models.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/models.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/models.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/pgfs.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/pgfs.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/pgfs.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/pgfs.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/preparers.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/preparers.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/preparers.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/preparers.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/retrievers.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/reference/retrievers.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/reference/retrievers.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/reference/retrievers.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/meta.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/meta.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/meta.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/meta.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_1.0.7.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_1.0.7.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_1.0.7.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_1.0.7.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.0.0.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.0.0.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.0.0.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.0.0.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.1.1.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.1.1.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.1.1.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.1.1.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.1.2.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.1.2.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.1.2.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.1.2.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.2.1.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.2.1.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_2.2.1.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_2.2.1.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_3.0.1.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_3.0.1.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_3.0.1.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_3.0.1.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.0.0.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.0.0.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.0.0.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.0.0.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.0.1.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.0.1.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.0.1.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.0.1.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.1.0.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.1.0.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.1.0.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.1.0.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.1.1.yml b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.1.1.yml
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/src/rel_notes_4.1.1.yml
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/src/rel_notes_4.1.1.yml
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_1.0.7_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_1.0.7_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_1.0.7_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_1.0.7_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.0.0_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.0.0_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.0.0_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.0.0_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.1.1_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.1.1_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.1.1_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.1.1_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.1.2_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.1.2_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.1.2_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.1.2_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.2.1_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.2.1_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_2.2.1_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_2.2.1_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_3.0.1_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_3.0.1_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_3.0.1_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_3.0.1_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.0.0_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.0.0_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.0.0_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.0.0_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.0.1_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.0.1_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.0.1_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.0.1_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.1.0_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.1.0_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.1.0_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.1.0_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.1.1_rel_notes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.1.1_rel_notes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/ai-accelerator_4.1.1_rel_notes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/ai-accelerator_4.1.1_rel_notes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/rel_notes/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/rel_notes/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/compatibility.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/compatibility.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/compatibility.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/compatibility.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/index.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/index.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/index.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/index.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/licenses.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/licenses.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/licenses.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/licenses.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/limitations.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/limitations.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/limitations.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/limitations.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/overview.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/overview.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/overview.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/overview.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pipelines-overview.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/pipelines-overview.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/pipelines-overview.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/pipelines-overview.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/volumes.mdx b/advocacy_docs/edb-postgres-ai/ai-accelerator/volumes.mdx
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/volumes.mdx
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/volumes.mdx
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/explained.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/explained.mdx
deleted file mode 100644
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/index.mdx b/advocacy_docs/edb-postgres-ai/ai-factory/learn/explained/index.mdx
deleted file mode 100644
diff --git a/advocacy_docs/edb-postgres-ai/ai-factory/pipeline/images/aidb-overview-withbackground.png b/advocacy_docs/edb-postgres-ai/ai-accelerator/images/aidb-overview-withbackground.png
rename from advocacy_docs/edb-postgres-ai/ai-factory/pipeline/images/aidb-overview-withbackground.png
rename to advocacy_docs/edb-postgres-ai/ai-accelerator/images/aidb-overview-withbackground.png
index afe4e5b5c439c616e731bc6b11fa9923e71a4440..afe4e5b5c439c616e731bc6b11fa9923e71a4440
GIT binary patch
literal 130
zc$^KzK@!3s3;@7;U%>|~C71yH4NZbDqqZZo2Vbvy*-JmY?Qc`ZIQnYsqmA3M^!a~1
zx0&~s?T50x+_X0_y=FjK9Vg<LF?khr%cg)*1TnKni4K$@dW?=pVh9i@WxSV#y(8Yh
Nr>VEE8iE-7^aK5nCwl+@

