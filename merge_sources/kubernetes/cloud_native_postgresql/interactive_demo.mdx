---
title: "Installation, Configuration and Deployment Demo"
description: "Walk through the process of installing, configuring and deploying the Cloud Native PostgreSQL Operator via a browser-hosted Kubernetes environment"
navTitle: Install, Configure, Deploy
product: 'Cloud Native Operator'
platform: ubuntu
tags:
    - postgresql
    - cloud-native-postgresql-operator
    - kubernetes
    - k3d
    - live-demo
katacodaPanel:
  scenario: ubuntu:2004
  initializeCommand: clear; echo -e \\\\033[1mPreparing k3d and kubectl...\\\\n\\\\033[0m; snap install kubectl --classic; wget -q -O - https://raw.githubusercontent.com/rancher/k3d/main/install.sh | bash; clear; echo -e \\\\033[2mk3d is ready\\ - enjoy Kubernetes\\!\\\\033[0m;
  codelanguages: shell, yaml
showInteractiveBadge: true
---

Want to see what it takes to get the Cloud Native PostgreSQL Operator up and running? This section will demonstrate the following:

1. Installing the Cloud Native PostgreSQL Operator
2. Deploying a three-node PostgreSQL cluster
3. Installing and using the kubectl-cnp plugin
4. Testing failover to verify the resilience of the cluster

It will take roughly 5-10 minutes to work through.

!!!interactive This demo is interactive
    You can follow along right in your browser by clicking the button below. Once the environment initializes, you'll see a terminal open at the bottom of the screen.

<KatacodaPanel />

Once [k3d](https://k3d.io/) is ready, we need to start a cluster:

```shell
k3d cluster create
__OUTPUT__
INFO[0000] Prep: Network                                
INFO[0000] Created network 'k3d-k3s-default'            
INFO[0000] Created volume 'k3d-k3s-default-images'      
INFO[0000] Starting new tools node...                   
INFO[0000] Pulling image 'docker.io/rancher/k3d-tools:5.1.0' 
INFO[0001] Creating node 'k3d-k3s-default-server-0'     
INFO[0001] Pulling image 'docker.io/rancher/k3s:v1.21.5-k3s2' 
INFO[0002] Starting Node 'k3d-k3s-default-tools'        
INFO[0006] Creating LoadBalancer 'k3d-k3s-default-serverlb' 
INFO[0007] Pulling image 'docker.io/rancher/k3d-proxy:5.1.0' 
INFO[0011] Using the k3d-tools node to gather environment information 
INFO[0011] HostIP: using network gateway...             
INFO[0011] Starting cluster 'k3s-default'               
INFO[0011] Starting servers...                          
INFO[0011] Starting Node 'k3d-k3s-default-server-0'     
INFO[0018] Starting agents...                           
INFO[0018] Starting helpers...                          
INFO[0018] Starting Node 'k3d-k3s-default-serverlb'     
INFO[0024] Injecting '172.19.0.1 host.k3d.internal' into /etc/hosts of all nodes... 
INFO[0024] Injecting records for host.k3d.internal and for 2 network members into CoreDNS configmap... 
INFO[0025] Cluster 'k3s-default' created successfully!  
INFO[0025] You can now use it like this:                
kubectl cluster-info
```

This will create the Kubernetes cluster, and you will be ready to use it.
Verify that it works with the following command:

```shell
kubectl get nodes
__OUTPUT__
NAME                       STATUS   ROLES                  AGE   VERSION
k3d-k3s-default-server-0   Ready    control-plane,master   16s   v1.21.5+k3s2
```

You will see one node called `k3d-k3s-default-server-0`. If the status isn't yet "Ready", wait for a few seconds and run the command above again.

## Install Cloud Native PostgreSQL

Now that the Kubernetes cluster is running, you can proceed with Cloud Native PostgreSQL installation as described in the ["Installation and upgrades"](installation_upgrade.md) section:

```shell
kubectl apply -f https://get.enterprisedb.io/cnp/postgresql-operator-1.10.0.yaml
__OUTPUT__
namespace/postgresql-operator-system created
customresourcedefinition.apiextensions.k8s.io/backups.postgresql.k8s.enterprisedb.io created
customresourcedefinition.apiextensions.k8s.io/clusters.postgresql.k8s.enterprisedb.io created
customresourcedefinition.apiextensions.k8s.io/poolers.postgresql.k8s.enterprisedb.io created
customresourcedefinition.apiextensions.k8s.io/scheduledbackups.postgresql.k8s.enterprisedb.io created
serviceaccount/postgresql-operator-manager created
clusterrole.rbac.authorization.k8s.io/postgresql-operator-manager created
clusterrolebinding.rbac.authorization.k8s.io/postgresql-operator-manager-rolebinding created
service/postgresql-operator-webhook-service created
deployment.apps/postgresql-operator-controller-manager created
mutatingwebhookconfiguration.admissionregistration.k8s.io/postgresql-operator-mutating-webhook-configuration created
validatingwebhookconfiguration.admissionregistration.k8s.io/postgresql-operator-validating-webhook-configuration created
```

And then verify that it was successfully installed:

```shell
kubectl get deploy -n postgresql-operator-system postgresql-operator-controller-manager
__OUTPUT__
NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
postgresql-operator-controller-manager   1/1     1            1           52s
```

## Deploy a PostgreSQL cluster

As with any other deployment in Kubernetes, to deploy a PostgreSQL cluster
you need to apply a configuration file that defines your desired `Cluster`.

The [`cluster-example.yaml`](../samples/cluster-example.yaml) sample file
defines a simple `Cluster` using the default storage class to allocate
disk space:

```yaml
cat <<EOF > cluster-example.yaml
# Example of PostgreSQL cluster
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Cluster
metadata:
  name: cluster-example
spec:
  instances: 3

  # Example of rolling update strategy:
  # - unsupervised: automated update of the primary once all
  #                 replicas have been upgraded (default)
  # - supervised: requires manual supervision to perform
  #               the switchover of the primary
  primaryUpdateStrategy: unsupervised

  # Require 1Gi of space
  storage:
    size: 1Gi
EOF
```

!!! Note "There's more"
    For more detailed information about the available options, please refer
    to the ["API Reference" section](api_reference.md).

In order to create the 3-node PostgreSQL cluster, you need to run the following command:

```shell
kubectl apply -f cluster-example.yaml
__OUTPUT__
cluster.postgresql.k8s.enterprisedb.io/cluster-example created
```

You can check that the pods are being created with the `get pods` command. It'll take a bit to initialize, so if you run that
immediately after applying the cluster configuration you'll see the status as `Init:` or `PodInitializing`:

```shell
kubectl get pods
__OUTPUT__
NAME                             READY   STATUS            RESTARTS   AGE
cluster-example-1-initdb-kq2vw   0/1     PodInitializing   0          18s
```

...give it a minute, and then check on it again:

```shell
kubectl get pods
__OUTPUT__
NAME                READY   STATUS    RESTARTS   AGE
cluster-example-1   1/1     Running   0          56s
cluster-example-2   1/1     Running   0          35s
cluster-example-3   1/1     Running   0          19s
```

Now we can check the status of the cluster:


```shell
kubectl get cluster cluster-example -o yaml
__OUTPUT__
apiVersion: postgresql.k8s.enterprisedb.io/v1
kind: Cluster
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"postgresql.k8s.enterprisedb.io/v1","kind":"Cluster","metadata":{"annotations":{},"name":"cluster-example","namespace":"default"},"spec":{"instances":3,"primaryUpdateStrategy":"unsupervised","storage":{"size":"1Gi"}}}
  creationTimestamp: "2021-11-12T05:56:37Z"
  generation: 1
  name: cluster-example
  namespace: default
  resourceVersion: "2005"
  uid: 621d46bc-8a3b-4039-a9f3-6f21ab4ef68d
spec:
  affinity:
    podAntiAffinityType: preferred
    topologyKey: ""
  bootstrap:
    initdb:
      database: app
      encoding: UTF8
      localeCType: C
      localeCollate: C
      owner: app
  enableSuperuserAccess: true
  imageName: quay.io/enterprisedb/postgresql:14.1
  imagePullPolicy: IfNotPresent
  instances: 3
  logLevel: info
  maxSyncReplicas: 0
  minSyncReplicas: 0
  postgresGID: 26
  postgresUID: 26
  postgresql:
    parameters:
      log_destination: csvlog
      log_directory: /controller/log
      log_filename: postgres
      log_rotation_age: "0"
      log_rotation_size: "0"
      log_truncate_on_rotation: "false"
      logging_collector: "on"
      max_parallel_workers: "32"
      max_replication_slots: "32"
      max_worker_processes: "32"
      shared_preload_libraries: ""
      wal_keep_size: 512MB
  primaryUpdateStrategy: unsupervised
  resources: {}
  startDelay: 30
  stopDelay: 30
  storage:
    resizeInUseVolumes: true
    size: 1Gi
status:
  certificates:
    clientCASecret: cluster-example-ca
    expirations:
      cluster-example-ca: 2022-02-10 05:51:37 +0000 UTC
      cluster-example-replication: 2022-02-10 05:51:37 +0000 UTC
      cluster-example-server: 2022-02-10 05:51:37 +0000 UTC
    replicationTLSSecret: cluster-example-replication
    serverAltDNSNames:
    - cluster-example-rw
    - cluster-example-rw.default
    - cluster-example-rw.default.svc
    - cluster-example-r
    - cluster-example-r.default
    - cluster-example-r.default.svc
    - cluster-example-ro
    - cluster-example-ro.default
    - cluster-example-ro.default.svc
    serverCASecret: cluster-example-ca
    serverTLSSecret: cluster-example-server
  cloudNativePostgresqlCommitHash: f616a0d
  cloudNativePostgresqlOperatorHash: 02abbad9215f5118906c0c91d61bfbdb33278939861d2e8ea21978ce48f37421
  configMapResourceVersion: {}
  currentPrimary: cluster-example-1
  currentPrimaryTimestamp: "2021-11-12T05:57:15Z"
  healthyPVC:
  - cluster-example-1
  - cluster-example-2
  - cluster-example-3
  instances: 3
  instancesStatus:
    healthy:
    - cluster-example-1
    - cluster-example-2
    - cluster-example-3
  latestGeneratedNode: 3
  licenseStatus:
    isImplicit: true
    isTrial: true
    licenseExpiration: "2021-12-12T05:56:37Z"
    licenseStatus: Implicit trial license
    repositoryAccess: false
    valid: true
  phase: Cluster in healthy state
  poolerIntegrations:
    pgBouncerIntegration: {}
  pvcCount: 3
  readService: cluster-example-r
  readyInstances: 3
  secretsResourceVersion:
    applicationSecretVersion: "934"
    clientCaSecretVersion: "930"
    replicationSecretVersion: "932"
    serverCaSecretVersion: "930"
    serverSecretVersion: "931"
    superuserSecretVersion: "933"
  targetPrimary: cluster-example-1
  targetPrimaryTimestamp: "2021-11-12T05:56:38Z"
  writeService: cluster-example-rw
```

!!! Note
    By default, the operator will install the latest available minor version
    of the latest major version of PostgreSQL when the operator was released.
    You can override this by setting [the `imageName` key in the `spec` section of
    the `Cluster` definition](api_reference/#clusterspec).

!!! Important
    The immutable infrastructure paradigm requires that you always
    point to a specific version of the container image.
    Never use tags like `latest` or `13` in a production environment
    as it might lead to unpredictable scenarios in terms of update
    policies and version consistency in the cluster.

## Install the kubectl-cnp plugin

Cloud Native PostgreSQL provides [a plugin for kubectl](cnp-plugin) to manage a cluster in Kubernetes, along with a script to install it:

```shell
curl -sSfL \
  https://github.com/EnterpriseDB/kubectl-cnp/raw/main/install.sh | \
  sudo sh -s -- -b /usr/local/bin
__OUTPUT__
EnterpriseDB/kubectl-cnp info checking GitHub for latest tag
EnterpriseDB/kubectl-cnp info found version: 1.10.0 for v1.10.0/linux/x86_64
EnterpriseDB/kubectl-cnp info installed /usr/local/bin/kubectl-cnp
```

The `cnp` command is now available in kubectl:

```shell
kubectl cnp status cluster-example
__OUTPUT__
Cluster in healthy state   
Name:              cluster-example
Namespace:         default
PostgreSQL Image:  quay.io/enterprisedb/postgresql:14.1
Primary instance:  cluster-example-1
Instances:         3
Ready instances:   3
Current Timeline:  1
Current WAL file:  000000010000000000000005

Continuous Backup status
Not configured

Instances status
Manager Version  Pod name           Current LSN  Received LSN  Replay LSN  System ID            Primary  Replicating  Replay paused  Pending restart  Status
---------------  --------           -----------  ------------  ----------  ---------            -------  -----------  -------------  ---------------  ------
1.10.0           cluster-example-1  0/5000060                              7029558504442904594  ✓        ✗            ✗              ✗                OK
1.10.0           cluster-example-2               0/5000060     0/5000060   7029558504442904594  ✗        ✓            ✗              ✗                OK
1.10.0           cluster-example-3               0/5000060     0/5000060   7029558504442904594  ✗        ✓            ✗              ✗                OK
```

!!! Note "There's more"
    See [the Cloud Native PostgreSQL Plugin page](cnp-plugin/) for more commands and options.

## Testing failover

As our status checks show, we're running two replicas - if something happens to the primary instance of PostgreSQL, the cluster will fail over to one of them. Let's demonstrate this by killing the primary pod:

```shell
kubectl delete pod --wait=false cluster-example-1
__OUTPUT__
pod "cluster-example-1" deleted
```

This simulates a hard shutdown of the server - a scenario where something has gone wrong. 

Now if we check the status...
```shell
kubectl cnp status cluster-example
__OUTPUT__
Failing over   Failing over to cluster-example-2
Name:              cluster-example
Namespace:         default
PostgreSQL Image:  quay.io/enterprisedb/postgresql:14.1
Primary instance:  cluster-example-2
Instances:         3
Ready instances:   2
Current Timeline:  2
Current WAL file:  000000020000000000000006

Continuous Backup status
Not configured

Instances status
Manager Version  Pod name           Current LSN  Received LSN  Replay LSN  System ID            Primary  Replicating  Replay paused  Pending restart  Status
---------------  --------           -----------  ------------  ----------  ---------            -------  -----------  -------------  ---------------  ------
1.10.0           cluster-example-3               0/60000A0     0/60000A0   7029558504442904594  ✗        ✗            ✗              ✗                OK
45               cluster-example-1  -            -             -           -                    -        -            -              -                pod not available
1.10.0           cluster-example-2  0/6000F58                              7029558504442904594  ✓        ✗            ✗              ✗                OK
```

...the failover process has begun, with the second pod promoted to primary. Once the failed pod has restarted, it will become a replica of the new primary:

```shell
kubectl cnp status cluster-example
__OUTPUT__
Cluster in healthy state   
Name:              cluster-example
Namespace:         default
PostgreSQL Image:  quay.io/enterprisedb/postgresql:14.1
Primary instance:  cluster-example-2
Instances:         3
Ready instances:   3
Current Timeline:  2
Current WAL file:  000000020000000000000006

Continuous Backup status
Not configured

Instances status
Manager Version  Pod name           Current LSN  Received LSN  Replay LSN  System ID            Primary  Replicating  Replay paused  Pending restart  Status
---------------  --------           -----------  ------------  ----------  ---------            -------  -----------  -------------  ---------------  ------
1.10.0           cluster-example-3               0/60000A0     0/60000A0   7029558504442904594  ✗        ✗            ✗              ✗                OK
1.10.0           cluster-example-2  0/6004CA0                              7029558504442904594  ✓        ✗            ✗              ✗                OK
1.10.0           cluster-example-1               0/6004CA0     0/6004CA0   7029558504442904594  ✗        ✓            ✗              ✗                OK
```


### Further reading

This is all it takes to get a PostgreSQL cluster up and running, but of course there's a lot more possible - and certainly much more that is prudent before you should ever deploy in a production environment!

- Design goals and possibilities offered by the Cloud Native PostgreSQL Operator: check out the [Architecture](architecture/) and [Use cases](use_cases/) sections.

- Configuring a secure and reliable system: read through the [Security](security/), [Failure Modes](failure_modes/) and [Backup and Recovery](backup_recovery/) sections.

- Webinar: [Watch Gabriele Bartolini discuss and demonstrate Cloud Native PostgreSQL lifecycle management](https://www.youtube.com/watch?v=S-I9y-HnAnI) 

- Development: [Leonardo Cecchi writes about setting up a local environment using Cloud Native PostgreSQL for application development](https://www.enterprisedb.com/blog/cloud-native-postgresql-application-developers)

